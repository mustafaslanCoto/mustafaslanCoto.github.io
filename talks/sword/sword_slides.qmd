---
format:
  revealjs:
    theme: default
    slide-number: c/t
    width: 2100
    height: 1200
    margin: 0.06
    auto-stretch: true
    css: style.css
---

{{< include title-slide.html >}}

## Outline

-  What problem(s) does our research focus on?

-  Research Questions and How We Address Them

-  Methodology & Approach

-  Progress so far?

## Outline

-  <span style="color: #944454;">**What problem(s) does our research focus on?**</span>

-  Research Questions and How We Address Them

-  Methodology & Approach

-  Progress so far?

## What problem(s) does our research focus on?

::::: columns
::: {.column width="50%"}
![](images/disc_news2.png){fig-align="left" width="85%"}
:::

::: {.column width="50%"}
**Problem**

-   The challenge of capacity management
    -   Inefficient use of healthcare and social care resources
-   Inadequate discharge care coordination
    -   Difficulties with discharging medically fit patients in a timely manner

**Why it matters**

Poorly managed discharge processes negatively affect both individuals and patient flow through hospitals, creating bottlenecks that increase pressure on all healthcare services.

**Who it impacts**

Patients, healthcare professionals, and the overall healthcare system.
:::
:::::

## Outline

-  What problem(s) does our research focus on?

-  <span style="color: #944454;">**Research Questions and How We Address Them**</span>

-  Methodology & Approach

-  Progress so far?

## ‚ùìResearch Questions and How We Address Them

![](plan0.svg){fig-align="center" width="100%"}

## ‚ùìResearch Questions and How We Address Them

![](plan1.svg){fig-align="center" width="100%"}

## ‚ùìResearch Questions and How We Address Them

![](plan2.svg){fig-align="center" width="100%"}

## ‚ùìResearch Questions and How We Address Them

![](plan3.svg){fig-align="center" width="100%"}

## Outline

-  What problem(s) does our research focus on?

-  Research Questions and How We Address Them

-  <span style="color: #944454;">**Methodology & Approach**</span>

-  Progress so far?

## üõ†Ô∏è Methodology & Approach

::::: {columns}
::: {.column width="50%" style="font-size: 100%;"}
### Data

-   SAIL Databank
-   Patient-level data sources

### Model Development

-   Mathematical modelling
-   Stocastic optimization and reinforcement learning methods
-   Machine learning
-   Probabilistic modelling

### Validation and Testing

-   Cross-Validation to test predictive models
-   Test models in a real-world setting, possibly in collaboration with a hospital
:::

::: {.column width="50%"}
![](images/machine_learning.jpg){fig-align="center" width="100%"}
:::
:::::

## Outline

-  What problem(s) does our research focus on?

-  Research Questions and How We Address Them

-  Methodology & Approach

-  <span style="color: #944454;">**Progress so far?**</span>

## Progress so far?

![](plan_so_far.svg){fig-align="center" width="100%"}

## Regime-Switching AutoRegressive Hidden Markov Model (RS-ARHMM) {.smaller}

Let $y_t$ be the observed value at time $t$, modeled as a function of its $p$ lagged values, the regime-specific parameters associated with the hidden state $s_t$, and exogenous variables $\mathbf{X}_t = (X_{t1}, \ldots, X_{tM})$.

The **RS-ARHMM** can be expressed as follows:

$$
y_t^{(s)} = \beta_{0}^{(s)} + \sum_{i=1}^{p} \beta_{i}^{(s)} \, y_{t-i} + \sum_{j=1}^{M} \beta_{p+j}^{(s)} \, X_{tj} + \epsilon_t^{(s)},
$$

where:

- $\beta_{i}^{(s)}$ are the coefficients for the lagged values for regime $s$,
- $\beta_{p+j}^{(s)}$ are the coefficients for the exogenous variables for regime $s$,
- $M$ is the number of exogenous variables,
- $\epsilon_t^{(s)}$ is the error term for regime $s$.

The **Markov property**:
$$
P(s_t = k \mid s_{1:t-1}) = P(s_t = k \mid s_{t-1}), \quad \forall t \geq 2.
$$

A HMM has the following components:

- $S$: The set of regimes, $\mathbb{S} = \{S_1, S_2, \ldots, S_K\}$.
- $P$: The transition probability matrix
- $p_{ij} = P(s_t = S_j \mid s_{t-1} = S_i)$ is the probability of transitioning from regime $S_i$ to regime $S_j$, s.t. $\sum_{j=1}^{K} p_{ij} = 1$.

**Transition matrix $P$** is defined as:

$$
P = \begin{pmatrix}
p_{11} & \cdots & p_{1K} \\
\vdots & \ddots & \vdots \\
p_{K1} & \cdots & p_{KK}
\end{pmatrix}
$$


## Forecasting with RS-ARHMM


- **Forecast Computation**: The final forecast for each time step is a weighted average of the forecasts from each regime, `weighted by the predicted regime probabilities`:

$$
\hat{y}_{T+h} = \sum_{j=1}^{K} P(s_{T+h} = S_j) \, \hat{y}_{T+h}^{(S_j)}
$$

- **State Prediction**: We predict the most probable state sequence for the next $h$ time steps using the transition matrix $\mathbf{P}$. The probability of being in state $S_j$ at time $T+h$ is computed recursively as:

$$
P(s_{T+h} = S_j) = \sum_{i=1}^{K} P(s_{T+h-1} = S_i) \, p_{ij}
$$


- **Regime-specific forecasts**: For each predicted regime, we utilize the corresponding regression model to forecast the observation at that time step. The predicted observation $\hat{y}_{T+h}$ for state $s_{T+h}$ is computed as:

$$
\hat{y}_{T+h}^{s_{T+h}} = \beta_{0}^{(s_{T+h})} + \sum_{i=1}^{p} \beta_{i}^{(s_{T+h})} \, y_{T+h-i} + \sum_{j=1}^{M} \beta_{p+j}^{(s_{T+h})} \, X_{T+h,j}
$$

---

## RS-ARHMM Parameter Estimation
:::: {.columns}
::: {.column width="30%"}


::: {.callout-note icon="false"}
## `EM (Expectation-Maximization) Algorithm`
- Iteratively estimates parameters $\Theta = \{\beta^{(s)}, \sigma^{2(s)}, P, \pi\}$ to maximize likelihood.

- Alternates between two steps:
  - **E-Step:** Estimate regime probabilities given current $\Theta$.
  - **M-Step:** Update $\Theta$ using these probabilities.
:::
:::

::: {.column width="35%"}
::: {.callout-note icon="false"}
## 1. E-Step

- **State probability:**  
$$
\gamma_t(i) = P(s_t = S_i \mid y_{1:T}, \Theta^{(k)})
$$

- **Transition probability:**  
$$
\xi_t(i,j) = P(s_t = S_i, s_{t+1} = S_j \mid y_{1:T}, \Theta^{(k)})
$$

- Computed via **Forward-Backward algorithm**:
  
  - Forward:
  
  $\alpha_t(i) = P(y_{1:t}, s_t = S_i \mid \Theta) = \sum_{j=1}^{K} \alpha_{t-1}(j) p_{ji} o_i(y_t)$
  
  - Backward:
  
  $b_t(i) = P(y_{t+1:T} \mid s_t = S_i, \Theta) = \sum_{j=1}^{K} p_{ij} o_j(y_{t+1}) b_{t+1}(j)$


**Emission (observation) likelihood:**  
$$
o_i(y_t) = \frac{1}{\sqrt{2\pi \sigma_i^2}} \exp\left( -\frac{(y_t - \mu_{i,t})^2}{2\sigma_i^2} \right)
$$

where $\mu_{i,t}$ is AR + exogenous mean for state $i$.
:::
:::

::: {.column width="35%"}
::: {.callout-note icon="false"}
## 2. M-Step


Update parameters using weighted averages with $\gamma_t(i)$:

- **Transition matrix:**  
$$
p_{ij}^{(k+1)} = \frac{\sum_{t=1}^{T-1} \xi_t(i,j)}{\sum_{t=1}^{T-1} \gamma_t(i)}
$$

- **Initial state:**  
$$
\pi_i^{(k+1)} = \gamma_1(i)
$$

- **Regression (AR/exog) coefficients:**  
$$
\beta^{(i,k+1)} = (\mathbf{X}^T \mathbf{W}_i \mathbf{X})^{-1} \mathbf{X}^T \mathbf{W}_i \mathbf{y}
$$

Where $\mathbf{W}_i = \text{diag}(\gamma_1(i),\dots,\gamma_T(i))$

- **Variance:**  
$$
\sigma_i^{2(k+1)} = \frac{\sum_{t=1}^T \gamma_t(i)(y_t-\mu_{i,t})^2}{\sum_{t=1}^T \gamma_t(i)}
$$

:::

::: {.callout-note icon="false"}
## Repeat E-M Steps until convergence:
$$|\ell(\Theta^{(k+1)}) - \ell(\Theta^{(k)})| < \epsilon$$

:::
:::
::::

---

## Benchmark models

**Statistical models:**

- `Exponential Smoothing (ETS)`: A state space time series model capturing level, trend, and seasonality.
- `Linear Regression`: A statistical model that estimates the linear relationship between predictors and a response variable.
- `Lasso Regression`: A regression method with *L1* regularization, useful for variable selection and preventing overfitting.

**Machine Learning models:**

- `XGBoost`: An optimized gradient boosting library designed to be highly efficient and flexible. It uses level-wise tree growth, building trees level by level horizontally.
- `LightGBM`: A gradient boosting framework that uses tree-based learning algorithms, known for its speed and efficiency. It uses leaf-wise tree growth.
- `Random Forest`: An ensemble learning method that builds multiple decision trees and merges them together to get a more accurate and stable prediction.

---

## Probabilistic Forecasting using Conformal Prediction

### Conformal Prediction for Time Series

- A distribution-free method for constructing prediction intervals
- A way to quantify the uncertainty of point forecasts by generating prediction intervals

::: {.callout-note icon="false"}
## Steps to Generate Prediction Intervals

:::: {.columns}
::: {.column width="50%"}
1. **Point Forecasting**

- Generate point forecasts $\hat{y}_{t+h}$ for the desired forecast horizon $h$ using rolling-origin cross-validation on a `calibration set` of 500 observations per horizon.

2. **Residual Calculation**

$$
r_{t+h} = y_{t+h} - \hat{y}_{t+h}
$$

where $y_{t+h}$ are the actual observed values and $\hat{y}_{t+h}$ are the point forecasts for $t+h$.

3. **Nonconformity Scores**
$$
A_{t+h} = |r_{t+h}|
$$

:::

::: {.column width="50%"}

4. **Quantile Calculation**

$$
q_{1-\alpha}^{(h)} = \text{Quantile}_{1-\alpha}(\{A_{t-n+h}\}_{t=t-n}^{n})
$$

where $n$ is the size of the calibration set and $1 - \alpha$ is the desired coverage level

5. **Prediction Intervals**

$$
\left[\hat{y}_{t+h} - q_{1-\alpha}^{(h)}, \hat{y}_{t+h} + q_{1-\alpha}^{(h)}\right]
$$

6. **Distribution Approximation**

$$
y_{t+h}^{(i)} = \hat{y}_{t+h} + r_{t+h}^{(i)}, \quad r_{t+h}^{(i)} \in \{r_{t+h}^{(1)}, r_{t+h}^{(2)}, \ldots, r_{t+h}^{(n)}\}
$$

:::
::::

:::

---

## Forecasting metrics

:::: {.columns}
::: {.column width="50%"}
### Point Forecasting Metrics

- Root Mean Squared Error (RMSE)
$$
RMSE = \sqrt{\frac{1}{n} \sum_{t=1}^{n} (y_t - \hat{y}_t)^2}
$$

- Mean Absolute Error (MAE)
$$
MAE = \frac{1}{n} \sum_{t=1}^{n} |y_t - \hat{y}_t|
$$

:::
::: {.column width="50%"}
### Probabilistic Forecasting Metrics
- Qunatile Loss (QL)
$$
QL_{\alpha} = 2 \sum_{t=1}^{h} \left[\alpha (y_t - \hat{y}_t^{(\alpha)}) \mathbb{1}_{\{y_t > \hat{y}_t^{(\alpha)}\}} + (1 - \alpha) (\hat{y}_t^{(\alpha)} - y_t) \mathbb{1}_{\{y_t \leq \hat{y}_t^{(\alpha)}\}}\right]
$$

:::
::::

---

## Forecast Distributions (Quantile scores)

```{python .fragment}
#| fig-width: 100%
#| fig-align: center
#| fig-height: 100%

import sys
import pickle
sys.path.append("../..") 
from project_utils import table_styler
import glob
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import pandas as pd
from sktime.transformations.series.boxcox import BoxCoxTransformer
sns.set_context("talk")
from statsmodels.tsa.seasonal import STL, MSTL
from statsmodels.nonparametric.smoothers_lowess import lowess
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

import pickle

plt.rcParams['figure.facecolor'] = "#FBFAF4"
plt.rcParams['axes.facecolor'] = "#FBFAF4"
occup_raw = pd.read_excel("occupancy.xlsx", index_col="Date")
# train-test split for both dataset and test size is 30 days for both datasets occupancy_hrs and occupancy_fourier
train_size = len(occup_raw) - 360
occup_train = occup_raw[:train_size]
occup_test = occup_raw[train_size:]
# filter the columns that contains "ward"
ward_cols_ = [col for col in occup_raw.columns if "ward" in col]


import pickle
# Load from file
with open('results/hmm_conforms.pkl', 'rb') as f:
    hmm_conforms = pickle.load(f)


# Load from file
with open('results/lr_conforms.pkl', 'rb') as f:
    lr_conforms = pickle.load(f)

# Load from file
with open('results/ml_conforms.pkl', 'rb') as f:
    ml_conforms = pickle.load(f)


# Load from file
with open('results/ets_conforms.pkl', 'rb') as f:
    ets_conforms = pickle.load(f)

# Load from file
with open('model_params/hmm_best_forward_lags.pkl', 'rb') as f:
    hmm_best_forward_lags = pickle.load(f)

# Load from file
with open('model_params/best_ward_ets_tk.pkl', 'rb') as f:
    best_ward_ets_tk = pickle.load(f)
# Load from file
with open('model_params/best_ward_ets_forecastk.pkl', 'rb') as f:
    best_ward_ets_forecastk = pickle.load(f)

# Load from file
with open('model_params/best_hmm_ets_tks.pkl', 'rb') as f:
    best_hmm_ets_tks = pickle.load(f)

with open('model_params/best_ward_ets_tk.pkl', 'rb') as f:
    best_ward_ets_tk = pickle.load(f)

# Load from file
with open('results/avg_of_qswards.pkl', 'rb') as f:
    qs = pickle.load(f)

plt.figure(figsize=(24, 11))
markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', 'X', 'd']
my_qs = np.arange(0.05, 1.00, 0.05)
for i, model in enumerate(qs.index.tolist()):
    # if model != "LR":
        plt.plot(my_qs, qs.loc[model], label=model, linewidth=1.4, alpha=1, marker=markers[i % len(markers)], markersize=5)
    # plt.plot(my_qs, avg_of_wards.loc[model], label=model, linewidth=3)
    # plt.plot(my_qs, avg_ward.loc[model], label=model)
plt.xlabel("Quantiles", fontdict={'fontsize': 16})
plt.ylabel("Pinball Loss", fontdict={'fontsize': 16})
plt.title("Pinball Loss across different quantiles (Average across all wards)", fontsize=24)
plt.xticks(my_qs, [f"{q:.2f}" for q in my_qs], fontdict={'fontsize': 16}, rotation=90)
plt.yticks(fontsize=16)
plt.legend(
    loc='upper center', 
    bbox_to_anchor=(0.5, 0.1),  # (x, y) position, y<0 puts below the axis
    ncol=len(qs.index),      # one column per model (horizontal)
    frameon=False,  # no box around the legend
    fontsize=20  # legend font size
)
plt.grid()
# Highlight area between 0.45 and 0.85
plt.axvspan(0.45, 0.85, color='green', alpha=0.08)
plt.show()
```

---

## Next steps

- Develop an optimization framework that utilizes the probabilistic forecasts from the AR-HMM and VAR-HMM models to optimize bed allocation and resource management.

- Test the models in real-world scenarios, collaborating with hospital staff to refine the models based on practical feedback and operational needs.

# Any questions or thoughts? üí¨

![](images/follow_us.png){fig-align="center"}