{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "format:\n",
        "  revealjs:\n",
        "    theme: default\n",
        "    slide-number: c/t\n",
        "    width: 2100\n",
        "    height: 1200\n",
        "    margin: 0.06\n",
        "    auto-stretch: true\n",
        "    css: style.css\n",
        "---\n",
        "\n",
        "<!-- logo style -->\n",
        "\n",
        "<style>\n",
        "  .logo-left {\n",
        "    position: absolute;\n",
        "    top: 10px;\n",
        "    left: 20px;\n",
        "  }\n",
        "\n",
        "  .logo-right {\n",
        "    position: absolute;\n",
        "    top: 10px;\n",
        "    right: 20px;\n",
        "    display: flex;\n",
        "    gap: 10px;\n",
        "    align-items: flex-start;\n",
        "  }\n",
        "\n",
        "  .logo-left img,\n",
        "  .logo-right img {\n",
        "    height: 160px;\n",
        "    width: auto;\n",
        "    \n",
        "  }\n",
        "  \n",
        "/* title sytle */\n",
        "  h1 {\n",
        "    text-align: center;\n",
        "  }\n",
        "\n",
        "</style>\n",
        "\n",
        "<div class=\"logo-left\">\n",
        "  <img src=\"images/cu.png\" alt=\"Left Logo\">\n",
        "  <img src=\"images/wgsss.png\" alt=\"Left Logo 2\">\n",
        "  <img src=\"images/hcrw_back.png\" alt=\"Left Logo 3\">\n",
        "</div>\n",
        "\n",
        "<div class=\"logo-right\">\n",
        "  <img src=\"images/dlsg.png\" alt=\"Right Logo\">\n",
        "  <img src=\"images/care.png\" alt=\"Right Logo 2\">\n",
        "</div>\n",
        "\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <h1>Hospital Bed Occupancy Forecasting: A Autoregressive Regression Hidden Markov Model\n",
        "</h1>\n",
        "\n",
        "<div class=\"title-block\">\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <h3>\n",
        "  Mustafa Aslan, Cardiff University, UK <br>\n",
        "  Lead supervisor: Prof. Bahman Rostami-Tabar<br>\n",
        "  Co-supervisor: Dr. Jeremy Dixon <br>\n",
        "  Data Lab for Social Good <br>\n",
        "  Cardiff University, UK <br>\n",
        "  <!-- <span style=\"color: rgba(179, 0, 0, 0.7); font-weight: bold;\">Slides:</span>\n",
        "  <a href=\"https://mustafaslancoto.github.io/talks/\" style=\"color: #000; font-weight: bold; text-decoration: underline dotted;text-underline-offset: 5px\">\n",
        "    https://mustafaslancoto.github.io/talks/\n",
        "  </a> -->\n",
        "  </h3>\n",
        "  <p>16 Sep 2025</p>\n",
        "</div>\n",
        "\n",
        "\n",
        "<!-- ::: {.slide background-image=\"images/machine_learning.jpg\" background-size=\"cover\" background-opacity=\"0.06\"}\n",
        "::: -->\n",
        "## Outline\n",
        "\n",
        "- The Problem\n",
        "- Why is this important?\n",
        "- Questions to address\n",
        "- Data\n",
        "- Modeling framework\n",
        "- Experimental design\n",
        "- Key findings\n",
        "- Next Steps\n",
        "\n",
        "# The Problem\n",
        "\n",
        "## Current Systemic Issues in Patient Flow\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/late_shift.png){width=\"100%\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "- Unexpected surges lead to overcrowding, delayed admissions, and compromised patient care quality\n",
        "- Inefficient bed management results in prolonged patient stays and increased healthcare costs\n",
        "- High variability in patient flow complicates resource allocation\n",
        "- Unexpected occupancy spikes lead to **staff burnout and reduced morale**\n",
        "- Patients waiting **12+** hours for mental health beds due to poor coordination\n",
        "<!-- - Current planning methods are reactive rather than predictive -->\n",
        "\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "# Why is this important?\n",
        "\n",
        "## The Critical Impact of Hospital Bed Management\n",
        "\n",
        "- **Patient Care**: Overcrowding and delays can compromise patient care quality and safety.\n",
        "\n",
        "- **Resource Optimization**: Efficient bed management can reduce costs and improve hospital operations.\n",
        "\n",
        "- **Staff Well-being**: Reducing unexpected workload can help prevent staff burnout and improve morale.\n",
        "\n",
        "- **System Efficiency**: Better coordination can reduce wait times and improve overall healthcare delivery.\n",
        "\n",
        "# Questions to address\n",
        "\n",
        "## Key Research Questions\n",
        "\n",
        "- How can we accurately forecast hospital bed occupancy to improve resource allocation and patient care?\n",
        "\n",
        "- How can probabilistic forecasting models be developed to account for the inherent uncertainty in patient flow and bed availability?\n",
        "\n",
        "- What are the hidden factors influencing bed occupancy, and how can they be incorporated into forecasting models?\n",
        "\n",
        "# Data\n",
        "\n",
        "## Dataset\n",
        "\n",
        " Daily hospital occupancy data from a UK hospital with 7 wards, spanning from July 2018 to April 2025. The dataset includes:\n",
        "\n",
        "- Daily occupancy counts for each ward (daily number of patients staying in each ward)\n",
        "- Date-related features: day of the week, month, year, day of the month, week of the year\n",
        "- Holiday indicators\n",
        "\n",
        "\n",
        "## Data characteristics\n",
        "\n",
        "- Strong upward trend in occupancy over time\n",
        "\n",
        "- Spike after COVID-19 pandemic"
      ],
      "id": "19eb782e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "# allow max rows to be displayed\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, root_mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline       \n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
        "import numpy as np\n",
        "from cubist import Cubist\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK, space_eval\n",
        "from hyperopt.pyll import scope\n",
        "pd.set_option('display.max_rows', 150)\n",
        "import pickle # for saving and loading models\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose, MSTL\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "from peshbeen.models import (ml_forecaster, ml_bidirect_forecaster, VARModel, MsHmmRegression, MsHmmVar)\n",
        "from peshbeen.model_selection import (cross_validate,  mv_cross_validate,\n",
        "                                      cv_tune, mv_cv_tune, prob_param_forecasts,\n",
        "                                      tune_ets, tune_sarima, ParametricTimeSeriesSplit,\n",
        "                                      forward_feature_selection, backward_feature_selection,\n",
        "                                      mv_forward_feature_selection, mv_backward_feature_selection,\n",
        "                                      hmm_forward_feature_selection, hmm_backward_feature_selection,\n",
        "                                      hmm_mv_forward_feature_selection, hmm_mv_backward_feature_selection,\n",
        "                                      hmm_cross_validate, hmm_mv_cross_validate, cv_lag_tune, \n",
        "                                      cv_hmm_lag_tune)\n",
        "from peshbeen.statplots import (plot_ccf, plot_PACF_ACF)\n",
        "from peshbeen.stattools import (unit_root_test, cross_autocorrelation,\n",
        "                                lr_trend_model, forecast_trend, pacf_strength, ccf_strength)\n",
        "from peshbeen.transformations import (fourier_terms, rolling_quantile,\n",
        "                        rolling_mean, rolling_std, expanding_mean, expanding_std,\n",
        "                        expanding_quantile, expanding_ets, box_cox_transform,\n",
        "                        back_box_cox_transform,undiff_ts, seasonal_diff, invert_seasonal_diff,\n",
        "                        nzInterval, zeroCumulative, kfold_target_encoder, target_encoder_for_test)\n",
        "from peshbeen.metrics import (MAPE, MASE, MSE, MAE, RMSE, SMAPE, CFE, CFE_ABS, WMAPE, SRMSE, RMSSE, SMAE)\n",
        "from peshbeen.prob_forecast import (ml_conformalizer, hmm_conformalizer, ets_conformalizer, bidirect_ts_conformalizer, var_conformalizer, bag_boost_aggr_conformalizer,\n",
        "                                       bidirect_aggr_conformalizer, ets_aggr_conformalizer, s_arima_aggr_conformalizer,\n",
        "                                       var_aggr_conformalizer, hmm_var_conformalizer)\n",
        "from sktime.transformations.series.boxcox import BoxCoxTransformer\n",
        "sns.set_context(\"talk\")\n",
        "from statsmodels.tsa.seasonal import STL, MSTL\n",
        "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "plt.rcParams['figure.facecolor'] = \"#FBFAF4\"\n",
        "plt.rcParams['axes.facecolor'] = \"#FBFAF4\"\n",
        "occup_raw = pd.read_excel(\"occupancy.xlsx\", index_col=\"Date\")\n",
        "# train-test split for both dataset and test size is 30 days for both datasets occupancy_hrs and occupancy_fourier\n",
        "train_size = len(occup_raw) - 360\n",
        "occup_train = occup_raw[:train_size]\n",
        "occup_test = occup_raw[train_size:]\n",
        "# filter the columns that contains \"ward\"\n",
        "ward_cols_ = [col for col in occup_raw.columns if \"ward\" in col]\n",
        "\n",
        "\n",
        "import pickle\n",
        "# Load from file\n",
        "with open('results/hmm_conforms.pkl', 'rb') as f:\n",
        "    hmm_conforms = pickle.load(f)\n",
        "\n",
        "\n",
        "# Load from file\n",
        "with open('results/lr_conforms.pkl', 'rb') as f:\n",
        "    lr_conforms = pickle.load(f)\n",
        "\n",
        "# Load from file\n",
        "with open('results/ml_conforms.pkl', 'rb') as f:\n",
        "    ml_conforms = pickle.load(f)\n",
        "\n",
        "\n",
        "# Load from file\n",
        "with open('results/ets_conforms.pkl', 'rb') as f:\n",
        "    ets_conforms = pickle.load(f)\n",
        "\n",
        "# Load from file\n",
        "with open('model_params/hmm_best_forward_lags.pkl', 'rb') as f:\n",
        "    hmm_best_forward_lags = pickle.load(f)\n",
        "\n",
        "# Load from file\n",
        "with open('model_params/best_ward_ets_tk.pkl', 'rb') as f:\n",
        "    best_ward_ets_tk = pickle.load(f)\n",
        "# Load from file\n",
        "with open('model_params/best_ward_ets_forecastk.pkl', 'rb') as f:\n",
        "    best_ward_ets_forecastk = pickle.load(f)\n",
        "\n",
        "# Load from file\n",
        "with open('model_params/best_hmm_ets_tks.pkl', 'rb') as f:\n",
        "    best_hmm_ets_tks = pickle.load(f)"
      ],
      "id": "5eea3aec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "n_plots = len(ward_cols_)\n",
        "# create a grid with enough slots (2 rows, 4 cols = 8 slots here) and a larger figure size\n",
        "covid_start = pd.to_datetime(\"2020-03-01\")\n",
        "covid_end = pd.to_datetime(\"2022-03-31\")\n",
        "fig, axes = plt.subplots(2, 4, figsize=(22, 11))\n",
        "axes = axes.flatten()  # flatten to index linearly\n",
        "for i, ward in enumerate(ward_cols_):\n",
        "   axes[i].plot(occup_raw.index, occup_raw[ward])\n",
        "   axes[i].set_title(ward, fontsize=20)\n",
        "   axes[i].set_xlabel(\"Date\", fontsize=14)\n",
        "   axes[i].set_ylabel(\"Number of patients\", fontsize=14)\n",
        "   axes[i].tick_params(axis='x', labelsize=12, rotation=45)\n",
        "   axes[i].tick_params(axis='y', labelsize=12)\n",
        "    # Highlight the COVID period\n",
        "   axes[i].axvspan(covid_start, covid_end, color='red', alpha=0.1, label='COVID period')\n",
        "    # Add legend only to the first subplot\n",
        "   if i == 0:\n",
        "     axes[i].legend(loc='upper left', fontsize=12)\n",
        "   # axes[i].legend(fontsize=12)\n",
        "# hide any unused subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "   fig.delaxes(axes[j])\n",
        "plt.tight_layout(pad=2.0)\n",
        "plt.show()"
      ],
      "id": "023f787f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "## Seasonal patterns\n",
        "\n",
        "### Day of the week seasonality\n"
      ],
      "id": "b3fd63a2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "wards = [f\"ward_{i}\" for i in range(7)]  # e.g. 7 wards (0..6)\n",
        "n_plots = len(wards)\n",
        "\n",
        "# create a grid with enough slots (2 rows, 4 cols = 8 slots here)\n",
        "fig, axes = plt.subplots(2, 4, figsize=(22, 11))\n",
        "axes = axes.flatten()  # flatten to index linearly\n",
        "\n",
        "for ax, ward_col in zip(axes[:n_plots], wards):\n",
        "        # Plot each week's data (less transparent)\n",
        "        ward = occup_train.pivot_table(index=\"day_of_week\", columns=\"week_of_year\", values=ward_col, aggfunc='mean').reset_index()\n",
        "        mean = occup_train.groupby('day_of_week')[ward_col].mean()\n",
        "        for week in ward.columns[1:]:\n",
        "            ax.plot(ward.index, ward[week], alpha=0.2)  # Less transparent\n",
        "\n",
        "        # Plot average of all weeks (more transparent, thicker line)\n",
        "        ax.plot(mean.index, mean, color='black', linewidth=2, alpha=0.9, label='Weekly Average')  # More transparent\n",
        "        ax.set_xlabel(\"Day of Week\", fontsize=12)\n",
        "        ax.tick_params(axis='x', labelsize=12)  # <-- Correct way\n",
        "        ax.tick_params(axis='y', labelsize=12)  # <-- Correct way\n",
        "        ax.set_ylabel(\"Occupancy\", fontsize=12)\n",
        "        ax.set_title(f\"Weekly Occupancy for ward {ward_col}\", fontsize=12)\n",
        "        ax.legend(fontsize=12)  # <-- Only 'Weekly Average' will show up\n",
        "\n",
        "# remove any unused axes (if grid has more slots than n_plots)\n",
        "for ax in axes[n_plots:]:\n",
        "    fig.delaxes(ax)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "680e3399",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Seasonal patterns\n",
        "\n",
        "### Monthly seasonality"
      ],
      "id": "b91025a8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "# year_df = occup_train[occup_train[\"year\"].isin([2019, 2020, 2021, 2022, 2023])]\n",
        "\n",
        "# choose how many wards to plot\n",
        "wards = [f\"ward_{i}\" for i in range(7)]  # e.g. 7 wards (0..6)\n",
        "n_plots = len(wards)\n",
        "\n",
        "# create a grid with enough slots (2 rows, 4 cols = 8 slots here)\n",
        "fig, axes = plt.subplots(2, 4, figsize=(22, 11))\n",
        "axes = axes.flatten()  # flatten to index linearly\n",
        "\n",
        "for ax, ward_col in zip(axes[:n_plots], wards):\n",
        "        # Plot each week's data (less transparent)\n",
        "        ward = occup_train.pivot_table(index=\"month\", columns=\"day_of_month\", values=ward_col, aggfunc='mean').reset_index()\n",
        "        mean = occup_train.groupby('month')[ward_col].mean()\n",
        "        for week in ward.columns[1:]:\n",
        "            ax.plot(ward.index, ward[week], alpha=0.2)  # Less transparent\n",
        "\n",
        "        # Plot average of all weeks (more transparent, thicker line)\n",
        "        ax.plot(mean.index, mean, color='black', linewidth=2, alpha=0.9, label='Monthly Average')  # More transparent\n",
        "        ax.set_xlabel(\"Month\", fontsize=9)\n",
        "        ax.tick_params(axis='both', labelsize=9)  # <-- Correct way\n",
        "        ax.tick_params(axis='x', labelsize=10)  # x-axis tick labels size\n",
        "        ax.tick_params(axis='y', labelsize=12) # y-axis tick labels size\n",
        "        ax.set_ylabel(\"Occupancy\", fontsize=10)\n",
        "        ax.set_title(f\"Monthly Occupancy for {ward_col}\", fontsize=10)\n",
        "        ax.legend(loc='upper right', fontsize=10)\n",
        "\n",
        "# remove any unused axes (if grid has more slots than n_plots)\n",
        "for ax in axes[n_plots:]:\n",
        "    fig.delaxes(ax)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "871b33ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Effect of holidays"
      ],
      "id": "27657835"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(15, 7))\n",
        "# Custom palette for highlighting holidays\n",
        "sns.scatterplot(data=occup_train[:\"2022-07-31\"], x=occup_train[:\"2022-07-31\"].index, y=\"ward_0\", hue=\"is_holiday\", ax=axes)\n",
        "\n",
        "    \n",
        "# Customize the legend to show only holidays\n",
        "handles, labels = axes.get_legend_handles_labels()\n",
        "\n",
        "# Assuming holidays are at index 1 (adjust if needed)\n",
        "holiday_handle = handles[1]  \n",
        "\n",
        "# Remove the default legend and add custom one\n",
        "axes.legend_.remove()\n",
        "axes.legend([holiday_handle], ['Public Holidays'], loc='upper left')\n",
        "\n",
        "# Get holiday data points\n",
        "holiday_data = occup_train[occup_train[\"holiday\"] != \"not_holiday\"][:\"2022-07-31\"]\n",
        "\n",
        "# Add date boxes for each holiday\n",
        "for idx, (date, row) in enumerate(holiday_data.iterrows()):\n",
        "    occupancy = row['ward_0']\n",
        "    \n",
        "    # Format the date string\n",
        "    # date_str = date.strftime('%Y-%m-%d')\n",
        "    date_str = row['holiday']\n",
        "    \n",
        "    # Calculate text position (slightly offset from the point)\n",
        "    x_offset = pd.Timedelta(days=30)  # Adjust this for horizontal spacing\n",
        "    y_offset = 0.5  # Adjust this for vertical spacing\n",
        "    \n",
        "    # Alternate positioning to avoid overlap\n",
        "    if idx % 2 == 0:\n",
        "        text_x = date + x_offset\n",
        "        text_y = occupancy + y_offset\n",
        "        ha = 'left'\n",
        "    else:\n",
        "        text_x = date - x_offset\n",
        "        text_y = occupancy - y_offset\n",
        "        ha = 'right'\n",
        "    \n",
        "    # Add the date box\n",
        "    bbox_props = dict(boxstyle=\"round,pad=0.3\", facecolor='white', \n",
        "                     edgecolor='C1', alpha=0.5, linewidth=0.5)\n",
        "    \n",
        "    axes.annotate(date_str, \n",
        "                 xy=(date, occupancy),           # Point to annotate\n",
        "                 xytext=(text_x, text_y),        # Text position\n",
        "                 bbox=bbox_props,\n",
        "                 fontsize=12,\n",
        "                 rotation=50,\n",
        "                 ha=ha,\n",
        "                 va='center',\n",
        "                 arrowprops=dict(arrowstyle='->', \n",
        "                               connectionstyle='arc3,rad=0.1',\n",
        "                               color='white', alpha=0.6))\n",
        "\n",
        "axes.set_title(\"Ward 0 Daily Occupancy\", fontsize=16)\n",
        "axes.set_xlabel(\"Date\", fontsize=14)\n",
        "axes.set_ylabel(\"Occupancy\", fontsize=14)\n",
        "plt.show()"
      ],
      "id": "353fcd2d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Modeling Framework\n",
        "\n",
        "## Autoregressive Regression Markov Switching Hidden Markov Model (AR-MSHMM) {.smaller}\n",
        "\n",
        "Let $y_t$ be the observed value at time $t$, modeled as a function of its $p$ lagged values, the regime-specific parameters associated with the hidden state $s_t$, and exogenous variables $\\mathbf{X}_t = (X_{t1}, \\ldots, X_{tM})$.\n",
        "\n",
        "The **AR-MSHMM** can be expressed as follows:\n",
        "\n",
        "$$\n",
        "y_t^{(s)} = \\beta_{0}^{(s)} + \\sum_{i=1}^{p} \\beta_{i}^{(s)} \\, y_{t-i} + \\sum_{j=1}^{M} \\beta_{p+j}^{(s)} \\, X_{tj} + \\epsilon_t^{(s)},\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $\\beta_{i}^{(s)}$ are the coefficients for the lagged values for regime $s$,\n",
        "- $\\beta_{p+j}^{(s)}$ are the coefficients for the exogenous variables for regime $s$,\n",
        "- $M$ is the number of exogenous variables,\n",
        "- $\\epsilon_t^{(s)}$ is the error term for regime $s$.\n",
        "\n",
        "The **Markov property**:\n",
        "$$\n",
        "P(s_t = k \\mid s_{1:t-1}) = P(s_t = k \\mid s_{t-1}), \\quad \\forall t \\geq 2.\n",
        "$$\n",
        "\n",
        "A HMM has the following components:\n",
        "\n",
        "- $S$: The set of regimes, $\\mathbb{S} = \\{S_1, S_2, \\ldots, S_K\\}$.\n",
        "- $P$: The transition probability matrix\n",
        "- $p_{ij} = P(s_t = S_j \\mid s_{t-1} = S_i)$ is the probability of transitioning from regime $S_i$ to regime $S_j$, s.t. $\\sum_{j=1}^{K} p_{ij} = 1$.\n",
        "\n",
        "**Transition matrix $P$** is defined as:\n",
        "\n",
        "$$\n",
        "P = \\begin{pmatrix}\n",
        "p_{11} & \\cdots & p_{1K} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "p_{K1} & \\cdots & p_{KK}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "## Modeling Framework\n",
        "\n",
        "<br>\n",
        "\n",
        "### Parameter Estimation of AR-MSHMM\n",
        "\n",
        "The model parameters $\\Theta = \\{\\beta^{(s)}, \\sigma^{2(s)}, P, \\pi\\}$ are estimated using the **Expectation-Maximization (EM)** algorithm.\n",
        "\n",
        "- The **Expectation-Maximization (EM)** algorithm is utilized to `estimate` the parameters,$\\Theta = \\{\\beta^{(s)}, \\sigma^{2(s)}, P, \\pi\\}$. \n",
        "\n",
        "- The **Baum-Welch algorithm** is used to `update` the parameters to maximize the likelihood of the observed data.\n",
        "\n",
        "<br>\n",
        "\n",
        "### EM Algorithm Framework\n",
        "\n",
        "The EM algorithm consists of two main steps that are iterated `until convergence`:\n",
        "\n",
        "- **E-step (Expectation)**: Calculate the expected value of the complete-data likelihood given the current parameter estimates\n",
        "- **M-step (Maximization)**: Update parameter estimates by maximizing the expected log-likelihood computed in the E-step\n",
        "\n",
        "## Modeling Framework\n",
        "\n",
        "### E-step: State Probability Computation\n",
        "\n",
        "The probability of being in regime $S_i$ at time $t$, given the `observed data` and `current parameter estimates`, is computed as:\n",
        "\n",
        "$$\n",
        "\\gamma_t(i) = P(s_t = S_i \\mid y_{1:T}, \\Theta^{(k)}) = \\frac{\\alpha_t(i) b_t(i)}{\\sum_{j=1}^{K} \\alpha_t(j) b_t(j)}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $\\Theta^{(k)}$ represents the parameter estimates at iteration $k$\n",
        "- $\\alpha_t(i)$ and $b_t(i)$ are the forward and backward variables, respectively\n",
        "\n",
        "Additionally, we compute the probability of transitioning from state $S_i$ at time $t$ to state $S_j$ at time $t+1$:\n",
        "\n",
        "$$\n",
        "\\xi_t(i,j) = P(s_t = S_i, s_{t+1} = S_j \\mid y_{1:T}, \\Theta^{(k)}) = \\frac{\\alpha_t(i) p_{ij} b_j(y_{t+1}) \\beta_{t+1}(j)}{\\sum_{i=1}^{K} \\sum_{j=1}^{K} \\alpha_t(i) p_{ij} b_j(y_{t+1}) \\beta_{t+1}(j)}\n",
        "$$ {#eq-xi}\n",
        "\n",
        "## Modeling Framework\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "\n",
        "::: {.callout-note icon=\"false\" .fragment}\n",
        "\n",
        "## Forward and Backward Variables\n",
        "\n",
        "**Forward Variable** $\\alpha_t(i)$: The joint probability of observing the sequence $y_{1:t}$ and being in state $S_i$ at time $t$:\n",
        "\n",
        "**Recursive computation for Forward variable:**\n",
        "$$\n",
        "\\alpha_t(i) = P(y_{1:t}, s_t = S_i \\mid \\Theta) = \\left[\\sum_{j=1}^{K} \\alpha_{t-1}(j) p_{ji}\\right] o_i(y_t)\n",
        "$$ {#eq-forward-recursive}\n",
        "\n",
        "with initialization: $\\alpha_1(i) = \\pi_i o_i(y_1)$\n",
        "\n",
        "**Backward Variable** $b_t(i)$: The conditional probability of observing the future sequence $y_{t+1:T}$ given that the system is in state $S_i$ at time $t$:\n",
        "\n",
        "**Recursive computation for Backward variable:**\n",
        "$$\n",
        "b_t(i) = P(y_{t+1:T} \\mid s_t = S_i, \\Theta) = \\sum_{j=1}^{K} p_{ij} o_j(y_{t+1}) b_{t+1}(j)\n",
        "$$ {#eq-backward-recursive}\n",
        "\n",
        "with initialization: $b_T(i) = 1$ for all $i$\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "\n",
        "::: {.callout-note icon=\"false\" .fragment}\n",
        "## Emission Probabilities (Observation Likelihoods)\n",
        "\n",
        "The emission probability, $o_i(y_t)$: the likelihood of observing $y_t$ given state $S_i$.\n",
        "\n",
        "For the AR-MSHMM, this is modeled as a Gaussian distribution:\n",
        "\n",
        "$$\n",
        "o_i(y_t) = \\frac{1}{\\sqrt{2\\pi \\sigma_i^2}} \\exp\\left(-\\frac{(y_t - \\mu_{i,t})^2}{2\\sigma_i^2}\\right)\n",
        "$$ {#eq-emission}\n",
        "\n",
        "where the state-specific conditional mean $\\mu_{i,t}$ is given by the autoregressive specification:\n",
        "\n",
        "$$\n",
        "\\mu_{i,t} = \\beta_0^{(i)} + \\sum_{k=1}^{p} \\beta_k^{(i)} y_{t-k} + \\sum_{j=1}^{M} \\beta_{p+j}^{(i)} X_{t,j}\n",
        "$$\n",
        "\n",
        ":::\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "## Modeling Framework\n",
        "\n",
        "### M-step: Parameter Updates\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "\n",
        "::: {.callout-note icon=\"false\" .fragment}\n",
        "## Transition Probabilities\n",
        "$$\n",
        "p_{ij}^{(k+1)} = \\frac{\\sum_{t=1}^{T-1} \\xi_t(i,j)}{\\sum_{t=1}^{T-1} \\gamma_t(i)}\n",
        "$$\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.callout-note icon=\"false\" .fragment}\n",
        "## Initial State Probabilities\n",
        "$$\n",
        "\\pi_i^{(k+1)} = \\gamma_1(i)\n",
        "$$\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.callout-note icon=\"false\" .fragment}\n",
        "\n",
        "## Regression Coefficients\n",
        "For each state $i$, the coefficients are updated using weighted least squares:\n",
        "$$\n",
        "\\theta^{(i,k+1)} = \\left(\\mathbf{X}^T \\mathbf{W}_i \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{W}_i \\mathbf{y}\n",
        "$$\n",
        "\n",
        "\n",
        "where $\\mathbf{X}$ is the design matrix containing lagged values and exogenous variables\n",
        "\n",
        "$\\mathbf{X}$ is defined as:\n",
        "$$\n",
        "\\mathbf{X} = \\begin{pmatrix}\n",
        "1 & y_0 & y_{-1} & \\cdots & y_{-p+1} & X_{1,1} & \\cdots & X_{1,M} \\\\\n",
        "1 & y_1 & y_0 & \\cdots & y_{-p+2} & X_{2,1} & \\cdots & X_{2,M} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & y_{T-1} & y_{T-2} & \\cdots & y_{T-p} & X_{T,1} & \\cdots & X_{T,M}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "and $\\mathbf{W}_i$ is a diagonal matrix with weights $\\gamma_t(i)$ for $t = 1, \\ldots, T$\n",
        "$$\n",
        "\\mathbf{W}_i = \\text{diag}(\\gamma_1(i), \\gamma_2(i), \\ldots, \\gamma_T(i))\n",
        "$$\n",
        "\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "\n",
        "::: {.callout-note icon=\"false\" .fragment}\n",
        "\n",
        "## Variance Parameters\n",
        "$$\n",
        "\\sigma_i^{2(k+1)} = \\frac{\\sum_{t=1}^{T} \\gamma_t(i) (y_t - \\mu_{i,t})^2}{\\sum_{t=1}^{T} \\gamma_t(i)}\n",
        "$$\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.callout-note icon=\"false\" .fragment}\n",
        "## Convergence Criterion\n",
        "The EM algorithm continues until the change in log-likelihood between successive iterations falls below a predetermined threshold:\n",
        "$$\n",
        "|\\ell(\\Theta^{(k+1)}) - \\ell(\\Theta^{(k)})| < \\epsilon\n",
        "$$\n",
        "where $\\epsilon$ is typically set to $10^{-3}$ or $10^{-4}$.\n",
        "\n",
        ":::\n",
        "\n",
        ":::\n",
        "::::\n",
        "---\n",
        "\n",
        "\n",
        "## Modeling Framework {.smaller}\n",
        "\n",
        "### Forecasting with AR-RHMM\n",
        "\n",
        "To forecast $h$ steps ahead, we employ a systematic three-stage approach that combines state prediction with regime-specific observation modeling.\n",
        "\n",
        "1. **State Prediction**: We predict the most probable state sequence for the next $h$ time steps using the transition matrix $\\mathbf{P}$. The probability of being in state $S_j$ at time $T+h$ is computed recursively as:\n",
        "\n",
        "$$\n",
        "P(s_{T+h} = S_j) = \\sum_{i=1}^{K} P(s_{T+h-1} = S_i) \\, p_{ij}\n",
        "$$\n",
        "\n",
        "The initial state distribution $P(s_{T+1} = S_j)$ for the first step is estimated from the last estimated state probabilities obtained from the forward algorithm at time $T$.\n",
        "\n",
        "2. **Regime-specific forecasts**: For each predicted regime, we utilize the corresponding regression model to forecast the observation at that time step, incorporating both lagged values and exogenous variables. The predicted observation $\\hat{y}_{T+h}$ for state $s_{T+h}$ is computed as:\n",
        "\n",
        "$$\n",
        "\\hat{y}_{T+h}^{s_{T+h}} = \\beta_{0}^{(s_{T+h})} + \\sum_{i=1}^{p} \\beta_{i}^{(s_{T+h})} \\, y_{T+h-i} + \\sum_{j=1}^{M} \\beta_{p+j}^{(s_{T+h})} \\, X_{T+h,j}\n",
        "$$\n",
        "\n",
        "\n",
        "For multi-step forecasting, we use the predicted observations as inputs for subsequent predictions. $y_{T+h-i}$ are the actual observed values for $i \\leq h$ and the previously predicted values for $i > h$.\n",
        "\n",
        "3. **Final Forecast Computation**: The final forecast for each time step is a weighted average of the forecasts from each regime, weighted by the predicted regime probabilities:\n",
        "\n",
        "$$\n",
        "\\hat{y}_{T+h} = \\sum_{j=1}^{K} P(s_{T+h} = S_j) \\, \\hat{y}_{T+h}^{(S_j)}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## Benchmark models\n",
        "\n",
        "**Statistical models:**\n",
        "\n",
        "- `Exponential Smoothing (ETS)`: A state space time series model capturing level, trend, and seasonality.\n",
        "- `Linear Regression`: A statistical model that estimates the linear relationship between predictors and a response variable.\n",
        "- `Lasso Regression`: A regression method with *L1* regularization, useful for variable selection and preventing overfitting.\n",
        "\n",
        "**Machine Learning models:**\n",
        "\n",
        "- `XGBoost`: An optimized gradient boosting library designed to be highly efficient and flexible. It uses level-wise tree growth, building trees level by level horizontally.\n",
        "- `LightGBM`: A gradient boosting framework that uses tree-based learning algorithms, known for its speed and efficiency. It uses leaf-wise tree growth.\n",
        "- `Random Forest`: An ensemble learning method that builds multiple decision trees and merges them together to get a more accurate and stable prediction.\n",
        "\n",
        "---\n",
        "\n",
        "## Probabilistic Forecasting using Conformal Prediction {.smaller}\n",
        "\n",
        "### Conformal Prediction for Time Series\n",
        "\n",
        "- Conformal prediction is a distribution-free, finite-sample valid method for constructing prediction intervals\n",
        "- It provides a way to quantify the uncertainty of point forecasts by generating prediction intervals that are guaranteed to contain the true future values with a specified probability.\n",
        "\n",
        "::: {.fragment}\n",
        "### Steps to Generate Prediction Intervals\n",
        ":::\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "::: {.fragment}\n",
        "1. **Point Forecasting**:\n",
        "\n",
        "- Generate point forecasts $\\hat{y}_{t+h}$ for the desired forecast horizon $h$ using rolling-origin cross-validation on a `calibration set` of 500 observations per horizon, derived from the last 500 days of training data.\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "2. **Residual Calculation**: Compute the residuals from the calibration data set:\n",
        "$$\n",
        "r_{t+h} = y_{t+h} - \\hat{y}_{t+h}\n",
        "$$\n",
        "\n",
        "where $y_{t+h}$ are the actual observed values and $\\hat{y}_{t+h}$ are the point forecasts for $t+h$.\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "3. **Nonconformity Scores**: Calculate the nonconformity scores associated with the residuals of each forecast horizon. Nonconformity scores are calculated as the absolute values of the residuals:\n",
        "$$\n",
        "A_{t+h} = |r_{t+h}|\n",
        "$$\n",
        "\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "::: {.fragment}\n",
        "4. **Quantile Calculation**: For a desired confidence level $1 - \\alpha$, compute the $(1 - \\alpha)$-quantile of the nonconformity scores:\n",
        "\n",
        "$$\n",
        "q_{1-\\alpha}^{(h)} = \\text{Quantile}_{1-\\alpha}(\\{A_{t-n+h}\\}_{t=t-n}^{n})\n",
        "$$\n",
        "\n",
        "where $n$ is the size of the calibration set.\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "5. **Prediction Intervals**: Construct the prediction intervals for the forecast horizon $h$ as:\n",
        "$$\n",
        "\\left[\\hat{y}_{t+h} - q_{1-\\alpha}^{(h)}, \\hat{y}_{t+h} + q_{1-\\alpha}^{(h)}\\right]\n",
        "$$\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "6. **Distribution Approximation**: For each forecast horizon, generate a distribution of possible future values by adding residuals calculated from calibration set to the point forecasts.\n",
        "\n",
        "$$\n",
        "y_{t+h}^{(i)} = \\hat{y}_{t+h} + r_, \\quad r_i \\in \\{r_1, r_2, \\ldots, r_n\\}\n",
        "$$\n",
        "\n",
        "where $n$ is the number of residuals in the calibration set and $i = 1, 2, \\ldots, n$.\n",
        ":::\n",
        "\n",
        ":::\n",
        "::::\n",
        "\n",
        "---\n",
        "\n",
        "## Forecasting metrics\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "### Point Forecasting Metrics\n",
        "\n",
        "- Scaled Root Mean Squared Error (SRMSE)\n",
        "$$\n",
        "SRMSE = \\frac{\\sqrt{\\frac{1}{h} \\sum_{t\n",
        "=n+1}^{n+h} (y_t - \\hat{y}_t)^2}}{\\frac{1}{n-1} \\sum_{t=2}^{n} y_t}\n",
        "$$\n",
        "\n",
        "- Root Mean Squared Error (RMSE)\n",
        "$$\n",
        "RMSE = \\sqrt{\\frac{1}{n} \\sum_{t=1}^{n} (y_t - \\hat{y}_t)^2}\n",
        "$$\n",
        "\n",
        "- Mean Absolute Error (MAE)\n",
        "$$\n",
        "MAE = \\frac{1}{n} \\sum_{t=1}^{n} |y_t - \\hat{y}_t|\n",
        "$$\n",
        "\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "### Probabilistic Forecasting Metrics\n",
        "- Qunatile Loss (QL)\n",
        "$$\n",
        "QL_{\\alpha} = 2 \\sum_{t=1}^{h} \\left[\\alpha (y_t - \\hat{y}_t^{(\\alpha)}) \\mathbb{1}_{\\{y_t > \\hat{y}_t^{(\\alpha)}\\}} + (1 - \\alpha) (\\hat{y}_t^{(\\alpha)} - y_t) \\mathbb{1}_{\\{y_t \\leq \\hat{y}_t^{(\\alpha)}\\}}\\right]\n",
        "$$\n",
        "\n",
        ":::\n",
        "::::\n",
        "---\n",
        "\n",
        "## Experimental design {.smaller}\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "### Preliminary data analysis and data preparation\n",
        "\n",
        "::: {.column .fragment}\n",
        "\n",
        "- Missing values imputed using linear interpolation for wards with 0 occupancy level to run models\n",
        "- Data visualization: Plotted time series to identify trends, seasonality, and anomalies.\n",
        "- Stationarity tests: Augmented Dickey-Fuller (ADF) and Kwiatkowski-Phillips-Schmidt-Shin (KPSS) tests to assess stationarity of the time series data.\n",
        "- Seasonal decomposition: Decomposed the time series into trend, seasonal, and residual components to understand underlying patterns.\n",
        "- Correlation analysis, ACF and PACF plots: Analyzed autocorrelation and partial autocorrelation functions to determine appropriate lag orders for autoregressive models.\n",
        ":::\n",
        "\n",
        "::: {.column .fragment}\n",
        "### Feature engineering\n",
        "\n",
        "- Time-based features: Extracted day and month from timestamps to capture temporal patterns.\n",
        "- Public holidays: Created binary indicators for public holidays to account for their impact on hospital occupancy.\n",
        "- Lagged features: Created lagged versions of key variables to incorporate past information into the models.\n",
        "- Fourier terms: Added Fourier series terms for linear models to capture seasonality in the data.\n",
        "- Rolling window features: Computed rolling means and standard deviations to see if improving the model's performance.\n",
        ":::\n",
        "\n",
        "::: {.column .fragment}\n",
        "### Split the data into train and test data\n",
        "\n",
        "- The last 360 days of data are reserved as the test set to evaluate model performance\n",
        "- The remaining data is used for:\n",
        "    -   training and validation\n",
        "    -   hyperparameter tuning through rolling-origin cross-validation with a fixed window size of 30 days and step size of 13 days to capture yearly seasonality.\n",
        "    -   Generate probabilistic forecasts using conformal prediction using optimized models on the training data.\n",
        "- `The train-test split ensures that the models are evaluated on unseen data, providing a realistic assessment of their forecasting capabilities.`\n",
        ":::\n",
        "\n",
        "::: {.column .fragment}\n",
        "### Model evaluation\n",
        "\n",
        "- Cross-validation: Employed rolling-origin cross-validation on test set to assess model performance over multiple forecast origins and horizons. (30-day window, 13-day step size)\n",
        "- Point forecasting metrics: Mean Absolute Error (MAE) and Scaled Root Mean Squared Error (SRMSE) to evaluate the accuracy of point forecasts.\n",
        "- Probabilistic forecasting metrics: Quantile Loss (QL) to assess the quality of probabilistic forecasts.\n",
        "- Model comparison: Compared the performance of the AR-RHMM with benchmark statistical and machine learning models to determine its effectiveness in forecasting hospital bed occupancy.\n",
        ":::\n",
        "::::\n",
        "\n",
        "---\n",
        "\n",
        "## Key insights from data analysis\n",
        "\n",
        "### An important assumption to model: Stationarity\n",
        "\n",
        "One approach to make the series stationary is to take the first `difference` of the series.\n",
        "\n",
        "Original series: $y_t, y_{t-1}, y_{t-2}, y_{t-3}, \\ldots$ \\\n",
        "Differenced series: $y_t - y_{t-1}, y_{t-1} - y_{t-2}, y_{t-2} - y_{t-3}, \\ldots$"
      ],
      "id": "274ef3bc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "# Load from file\n",
        "\n",
        "import pickle\n",
        "with open('model_params/best_ward_ets_tk.pkl', 'rb') as f:\n",
        "    best_ward_ets_tk = pickle.load(f)\n",
        "\n",
        "\n",
        "import ruptures as rpt\n",
        "\n",
        "def stl_data_input(series, period=7, seasonal=2999):\n",
        "    # Apply STL decomposition\n",
        "    res = STL(series.interpolate(method=\"linear\").squeeze(), # Convert dataframe to a Series\n",
        "                                                        # to avoid error in Statsmodels\n",
        "            robust=True, period=period, seasonal=seasonal).fit()\n",
        "    \n",
        "    seasonal_component = res.seasonal\n",
        "    # residual_component = res.resid\n",
        "    # De-seasonlise original data\n",
        "    df_deseasonalised = series - seasonal_component\n",
        "\n",
        "    # # Perform linear interpolation on de-seasonalised data\n",
        "    df_deseasonalised_imputed = df_deseasonalised.interpolate(method=\"linear\")\n",
        "\n",
        "    # # Randomly sample residuals (with replacement) and add back\n",
        "    # sampled_residuals = np.random.choice(residual_component, size=len(residual_component), replace=True)\n",
        "    # # Match DataFrame index\n",
        "    # sampled_residuals = pd.Series(sampled_residuals, index=series.index)\n",
        "\n",
        "    # # Add seasonal component back to get the final imputed time series\n",
        "    df_imputed = df_deseasonalised_imputed + seasonal_component\n",
        "\n",
        "    return df_imputed\n",
        "\n",
        "def data_prep_f(ward, fourier_k):\n",
        "    ward_train = occup_train_clean[[ward]+cat_col_f]\n",
        "    ward_test = occup_test[[ward]+cat_col_f]\n",
        "    ward_test[ward] = ward_test[ward].interpolate(method=\"linear\")\n",
        "    ward_test[\"day_of_week\"] = ward_test.index.day_name()\n",
        "    df_all = pd.concat([ward_train, ward_test], axis=0)\n",
        "    ft = fourier_terms(start_end_index=(df_all.index.min(), df_all.index.max()),\n",
        "                period=365.25, num_terms=fourier_k)\n",
        "    return df_all.merge(ft, left_index=True, right_index=True, how=\"left\")\n",
        "\n",
        "def data_prep(ward):\n",
        "    ward_train = occup_train_clean[[ward]+cat_cols]\n",
        "    ward_test = occup_test[[ward]+cat_cols]\n",
        "    ward_test[ward] = ward_test[ward].interpolate(method=\"linear\")\n",
        "    ward_test[\"day_of_week\"] = ward_test.index.day_name()\n",
        "    ward_test[\"month\"] = ward_test.index.month_name()\n",
        "    return pd.concat([ward_train, ward_test], axis=0)\n",
        "\n",
        "## first input nan values using STL\n",
        "for ward in ward_cols_:\n",
        "    occup_train[ward] = stl_data_input(occup_train[ward], period=None, seasonal=2999)\n",
        "\n",
        "## replace the outlier values between 965 and 1040 by interpolation\n",
        "occup_train_clean = occup_train.copy()\n",
        "occup_train_nan = occup_train.copy()\n",
        "occup_train_nan.iloc[965-1:1040+1, occup_train_nan.columns.get_loc('ward_0')] = np.nan\n",
        "occup_train_clean['ward_0'] = stl_data_input(occup_train_nan['ward_0'], period=None, seasonal=2999)\n",
        "\n",
        "occup_train_nan.iloc[815-1:945+1, occup_train_nan.columns.get_loc('ward_4')] = np.nan\n",
        "occup_train_clean['ward_4'] = stl_data_input(occup_train_nan['ward_4'], period=None, seasonal=2999)\n",
        "\n",
        "## replace the outlier values between 1215 and 1395 by interpolation\n",
        "occup_train_nan.iloc[915-1:970+1, occup_train_nan.columns.get_loc('ward_5')] = np.nan\n",
        "occup_train_nan.iloc[1215-1:1395+1, occup_train_nan.columns.get_loc('ward_5')] = np.nan\n",
        "occup_train_clean['ward_5'] = stl_data_input(occup_train_nan['ward_5'], period=None, seasonal=2999)\n",
        "\n",
        "occup_train_clean[\"day_of_week\"] = occup_train_clean.index.day_name()\n",
        "occup_train_clean[\"month\"] = occup_train_clean.index.month_name()\n",
        "occup_train_clean[\"is_holiday\"] = np.where(occup_train_clean[\"is_holiday\"] == 1, \"holiday\", \"not holiday\")\n",
        "cat_cols = [\"day_of_week\", \"month\", \"is_holiday\"]\n",
        "cat_col_f = [\"day_of_week\", \"is_holiday\"]\n",
        "\n",
        "df_diff = pd.DataFrame()\n",
        "for w in ward_cols_:\n",
        "    df_diff[f'{w}'] = occup_train_clean[w]\n",
        "    df_diff[f'{w}_diff'] = occup_train_clean[w].diff()\n",
        "    df_diff[f'{w}_fitted_ets_trend'] = ExponentialSmoothing(occup_train_clean[w], **best_ward_ets_tk[w][0][0]).fit(**best_ward_ets_tk[w][0][1]).fittedvalues\n",
        "    df_diff[f'{w}_detrended'] = df_diff[f'{w}'] - df_diff[f'{w}_fitted_ets_trend']\n",
        "df_diff.dropna(inplace=True)\n",
        "\n",
        "# plot original differenced series in two plots in one graph\n",
        "fig, axs = plt.subplots(2, 1, figsize=(22, 11))\n",
        "axs[0].plot(df_diff.index, df_diff['ward_0'], label='Original Differenced Series', color='C0', alpha=0.9)\n",
        "axs[0].set_title('Original Differenced Series for ward_0')\n",
        "axs[0].legend(loc = 'lower right')\n",
        "axs[1].plot(df_diff.index, df_diff['ward_0_diff'], label='Differenced Series', color='C1', alpha=0.9)\n",
        "axs[1].set_title('Differenced Series for ward_0 occupancy')\n",
        "axs[1].legend(loc = 'lower right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "d3070a56",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key insights from data analysis\n",
        "\n",
        "### Information from ACF and PACF plots after differencing"
      ],
      "id": "6bba506b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "fig, axes = plt.subplots(2, 1, figsize=(22, 11))\n",
        "plot_acf(df_diff['ward_0_diff'], lags=40, ax=axes[0])\n",
        "plot_pacf(df_diff['ward_0_diff'], lags=40, ax=axes[1])\n",
        "axes[0].set_title('ACF of Differenced Series for ward_0 occupancy')\n",
        "axes[1].set_title('PACF of Differenced Series for ward_0 occupancy')\n",
        "plt.tight_layout(pad=2)\n",
        "plt.show()"
      ],
      "id": "35db5122",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key insights from data analysis\n",
        "\n",
        "### Alternative approach to make the series stationary and keep the dependence structure"
      ],
      "id": "b41bfa11"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "# plot original differenced series in two plots in one graph\n",
        "plt.figure(figsize=(22, 11))\n",
        "plt.plot(df_diff.index[-500:], df_diff['ward_0'][-500:], label='Original Differenced Series', color='C0', alpha=0.9)\n",
        "plt.plot(df_diff.index[-500:], df_diff['ward_0_fitted_ets_trend'][-500:], label='Fitted ETS Trend', color='C1', alpha=0.9)\n",
        "plt.plot(df_diff.index[-500:], df_diff['ward_0_detrended'][-500:], label='Detrended Series', color='C2', alpha=0.9)\n",
        "plt.title('Original Differenced Series vs Detrended Series for ward_0')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "id": "cbb95d72",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## An important assumption to model: Stationarity\n",
        "### Information from ACF and PACF plots after detrending"
      ],
      "id": "278cc13f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "fig, axes = plt.subplots(2, 1, figsize=(22, 11))\n",
        "plot_acf(df_diff['ward_0_detrended'], lags=40, ax=axes[0])\n",
        "plot_pacf(df_diff['ward_0_detrended'], lags=40, ax=axes[1])\n",
        "axes[0].set_title('ACF of Detrended Series for ward_0 occupancy')\n",
        "axes[1].set_title('PACF of Detrended Series for ward_0 occupancy')\n",
        "plt.tight_layout(pad=2)\n",
        "plt.show()"
      ],
      "id": "e7e010cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Validation setup\n",
        "\n",
        "Rolling-origin cross-validation is used to evaluate model performance over time"
      ],
      "id": "6a4639c0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from matplotlib.patches import Patch\n",
        "\n",
        "def plot_cv_indices(cv, X, y, ax=None,\n",
        "                    train_tail=50, step_size=13,\n",
        "                    annotate=True, first_n=5, last_n=1):\n",
        "    \"\"\"\n",
        "    Visualize cross-validation indices for any CV splitter.\n",
        "    Shows the first_n splits, ellipsis, and last_n splits.\n",
        "    \"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    splits = list(cv.split(X))\n",
        "    total_splits = len(splits)\n",
        "\n",
        "    # Indices for splits to show: first_n at bottom, last_n at top\n",
        "    show_indices = list(range(first_n)) + list(range(total_splits - last_n, total_splits))\n",
        "    yticks = []\n",
        "    yticklabels = []\n",
        "    plot_ys = []\n",
        "\n",
        "    # Gather all x-values for shown splits\n",
        "    all_xs = []\n",
        "    for plot_idx, split_idx in enumerate(show_indices):\n",
        "        train, test = splits[split_idx]\n",
        "        tail_len = train_tail + split_idx * step_size\n",
        "        train_tail_idx = train[-tail_len:] if len(train) > tail_len else train\n",
        "        all_xs.extend(train_tail_idx)\n",
        "        all_xs.extend(test)\n",
        "\n",
        "    # Compute y-locations: first_n at bottom, last at top, ellipsis in between\n",
        "    for i in range(first_n):\n",
        "        yticks.append(i + 0.5)\n",
        "        yticklabels.append(f\"Split {show_indices[i]+1}\")\n",
        "        plot_ys.append(i + 0.5)\n",
        "    ellipsis_y = first_n + 0.5\n",
        "    yticks.append(ellipsis_y)\n",
        "    yticklabels.append(\"\")\n",
        "    plot_ys.append(None)\n",
        "    yticks.append(first_n + 1.5)\n",
        "    yticklabels.append(f\"Split {show_indices[-1]+1}\")\n",
        "    plot_ys.append(first_n + 1.5)\n",
        "\n",
        "    # Plot the splits\n",
        "    for plot_idx, split_idx in enumerate(show_indices):\n",
        "        if plot_idx < first_n:\n",
        "            y_pos = plot_idx + 0.5\n",
        "        else:\n",
        "            y_pos = first_n + 1.5  # last split at top\n",
        "        train, test = splits[split_idx]\n",
        "        tail_len = train_tail + split_idx * step_size\n",
        "        train_tail_idx = train[-tail_len:] if len(train) > tail_len else train\n",
        "\n",
        "        ax.scatter(train_tail_idx, [y_pos] * len(train_tail_idx),\n",
        "                   c=\"C0\", marker=\"s\", s=50,\n",
        "                   label=\"Train (tail)\" if plot_idx == 0 else \"\")\n",
        "        ax.scatter(test, [y_pos] * len(test),\n",
        "                   c=\"C1\", marker=\"s\", s=50,\n",
        "                   label=\"Test\" if plot_idx == 0 else \"\")\n",
        "\n",
        "        if annotate:\n",
        "            ax.text(train_tail_idx[0], y_pos + 0.15,\n",
        "                    f\"Train size ={len(train)}\", color=\"C0\", fontsize=9, ha=\"left\")\n",
        "            ax.text(test[len(test)//2]+10, y_pos + 0.15,\n",
        "                    f\"Test size ={len(test)}\", color=\"C1\", fontsize=9, ha=\"center\")\n",
        "\n",
        "    # Add ellipsis at a representative x-position (center of shown data)\n",
        "    if all_xs:\n",
        "        ellipsis_x = 0.5 * (min(all_xs) + max(all_xs))\n",
        "    else:\n",
        "        ellipsis_x = 0\n",
        "    ax.text(ellipsis_x, ellipsis_y, \"......\", fontsize=32, ha=\"center\", va=\"center\", color=\"gray\")\n",
        "\n",
        "    ax.set(\n",
        "        yticks=yticks,\n",
        "        yticklabels=yticklabels,\n",
        "        xlabel=\"Data size\",\n",
        "        ylabel=\"CV iteration\",\n",
        "        ylim=(0, yticks[-1] + 1),\n",
        "    )\n",
        "    # Tighten xlim to shown data (add small margin)\n",
        "    if all_xs:\n",
        "        margin = max(1, int(0.01 * (max(all_xs) - min(all_xs))))\n",
        "        ax.set_xlim(min(all_xs) - margin, max(all_xs) + margin)\n",
        "\n",
        "    handles = [\n",
        "        Patch(color='C0', label='Train'),\n",
        "        Patch(color='C1', label='Test'),\n",
        "    ]\n",
        "    ax.legend(handles=handles, loc=\"lower right\")\n",
        "    return ax\n",
        "\n",
        "\n",
        "X = np.arange(2449)\n",
        "y = np.random.randint(0, 2, size=2449)\n",
        "tscv = ParametricTimeSeriesSplit(n_splits=30, test_size=30, step_size=13)\n",
        "axp = plot_cv_indices(cv=tscv, X=X, y=y)\n",
        "axp\n",
        "\n",
        "axp.axvline(2039, ymin=0.15, ymax=0.25,\n",
        "                    color=\"C2\", linestyle=\"--\", alpha=0.7, linewidth=1)\n",
        "axp.axvline(2052, ymin=0.15, ymax=0.25,\n",
        "                    color=\"C2\", linestyle=\"--\", alpha=0.7, linewidth=1)\n",
        "\n",
        "axp.text(2039, 2,\n",
        "        \"step size=13\", color=\"C2\", fontsize=9, ha=\"left\")\n",
        "\n",
        "axp.axvline(2052, ymin=0.30, ymax=0.38,\n",
        "                    color=\"C2\", linestyle=\"--\", alpha=0.7, linewidth=1)\n",
        "axp.axvline(2065, ymin=0.30, ymax=0.38,\n",
        "                    color=\"C2\", linestyle=\"--\", alpha=0.7, linewidth=1)\n",
        "\n",
        "axp.text(2052, 3,\n",
        "        \"step size=13\", color=\"C2\", fontsize=9, ha=\"left\")\n",
        "\n",
        "\n",
        "\n",
        "plt.title(\"Rolling Origin Time Series Cross-Validation\", fontsize=16)\n",
        "plt.show()"
      ],
      "id": "d485e2b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Results\n",
        "\n",
        "## Point Forecast results\n",
        "\n",
        "::: {layout-ncol=2}"
      ],
      "id": "bd7714e5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../..\") \n",
        "from project_utils import table_styler\n",
        "import glob\n",
        "\n",
        "# Load from file\n",
        "with open('results/all_model_points.pkl', 'rb') as f:\n",
        "    all_model_points = pickle.load(f)\n",
        "\n",
        "ward_0 = all_model_points[\"ward_0\"][[\"SRMSE\", \"RMSE\", \"MAE\"]]\n",
        "ward_0.reset_index(inplace=True)\n",
        "ward_0.rename(columns={\"index\": \"ward_0\"}, inplace=True)\n",
        "\n",
        "table_styler(ward_0, cells_format=[('font-size', '25px'), ('text-align', 'left')],\n",
        "              table_header=[('background-color', '#20808D'), ('text-align', 'left'),\n",
        "                            ('color', '#FBFAF4'), ('font-size', '30px'), ('min-width', '80px')],\n",
        "                            numeric_highlights = {\"SRMSE\": {\"condition\": \"less\", \"threshold\": 0.275, \"style\": \"font-weight: bold; background-color: None;\"}},\n",
        ")"
      ],
      "id": "f6a3a190",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ward_1 = all_model_points[\"ward_1\"][[\"SRMSE\", \"RMSE\", \"MAE\"]]\n",
        "ward_1.reset_index(inplace=True)\n",
        "ward_1.rename(columns={\"index\": \"ward_1\"}, inplace=True)\n",
        "\n",
        "table_styler(ward_1, cells_format=[('font-size', '25px'), ('text-align', 'left')],\n",
        "              table_header=[('background-color', '#20808D'), ('text-align', 'left'),\n",
        "                            ('color', '#FBFAF4'), ('font-size', '30px'), ('min-width', '80px')],\n",
        "                            numeric_highlights = {\"SRMSE\": {\"condition\": \"less\", \"threshold\": 0.275, \"style\": \"font-weight: bold; background-color: None;\"}},\n",
        ")"
      ],
      "id": "f295b993",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ward_2 = all_model_points[\"ward_2\"][[\"SRMSE\", \"RMSE\", \"MAE\"]]\n",
        "ward_2.reset_index(inplace=True)\n",
        "ward_2.rename(columns={\"index\": \"ward_2\"}, inplace=True)\n",
        "\n",
        "table_styler(ward_2, cells_format=[('font-size', '25px'), ('text-align', 'left')],\n",
        "              table_header=[('background-color', '#20808D'), ('text-align', 'left'),\n",
        "                            ('color', '#FBFAF4'), ('font-size', '30px'), ('min-width', '80px')],\n",
        "                            numeric_highlights = {\"SRMSE\": {\"condition\": \"less\", \"threshold\": 0.275, \"style\": \"font-weight: bold; background-color: None;\"}},\n",
        ")"
      ],
      "id": "e5f7ce94",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ward_3 = all_model_points[\"ward_3\"][[\"SRMSE\", \"RMSE\", \"MAE\"]]\n",
        "ward_3.reset_index(inplace=True)\n",
        "ward_3.rename(columns={\"index\": \"ward_3\"}, inplace=True)\n",
        "\n",
        "table_styler(ward_3, cells_format=[('font-size', '25px'), ('text-align', 'left')],\n",
        "              table_header=[('background-color', '#20808D'), ('text-align', 'left'),\n",
        "                            ('color', '#FBFAF4'), ('font-size', '30px'), ('min-width', '80px')],\n",
        "                            numeric_highlights = {\"SRMSE\": {\"condition\": \"less\", \"threshold\": 0.275, \"style\": \"font-weight: bold; background-color: None;\"}},\n",
        ")"
      ],
      "id": "9fe41e8b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ward_4 = all_model_points[\"ward_4\"][[\"SRMSE\", \"RMSE\", \"MAE\"]]\n",
        "ward_4.reset_index(inplace=True)\n",
        "ward_4.rename(columns={\"index\": \"ward_4\"}, inplace=True)\n",
        "\n",
        "table_styler(ward_4, cells_format=[('font-size', '25px'), ('text-align', 'left')],\n",
        "              table_header=[('background-color', '#20808D'), ('text-align', 'left'),\n",
        "                            ('color', '#FBFAF4'), ('font-size', '30px'), ('min-width', '80px')],\n",
        "                            numeric_highlights = {\"SRMSE\": {\"condition\": \"less\", \"threshold\": 0.275, \"style\": \"font-weight: bold; background-color: None;\"}},\n",
        ")"
      ],
      "id": "e7436d69",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ward_5 = all_model_points[\"ward_5\"][[\"SRMSE\", \"RMSE\", \"MAE\"]]\n",
        "ward_5.reset_index(inplace=True)\n",
        "ward_5.rename(columns={\"index\": \"ward_5\"}, inplace=True)\n",
        "\n",
        "table_styler(ward_5, cells_format=[('font-size', '25px'), ('text-align', 'left')],\n",
        "              table_header=[('background-color', '#20808D'), ('text-align', 'left'),\n",
        "                            ('color', '#FBFAF4'), ('font-size', '30px'), ('min-width', '80px')],\n",
        "                            numeric_highlights = {\"SRMSE\": {\"condition\": \"less\", \"threshold\": 0.275, \"style\": \"font-weight: bold; background-color: None;\"}},\n",
        ")"
      ],
      "id": "297e1c96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "## Forecast Distributions (Quantile scores)"
      ],
      "id": "27e1a1ab"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "# Load from file\n",
        "with open('results/avg_of_qswards.pkl', 'rb') as f:\n",
        "    qs = pickle.load(f)\n",
        "\n",
        "\n",
        "# qs_table = qs.reset_index().rename(columns={\"index\": \"Model\"})\n",
        "\n",
        "plt.figure(figsize=(18, 9))\n",
        "markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', 'X', 'd']\n",
        "my_qs = np.arange(0.05, 1.00, 0.05)\n",
        "for i, model in enumerate(qs.index.tolist()):\n",
        "    # if model != \"LR\":\n",
        "        plt.plot(my_qs, qs.loc[model], label=model, linewidth=1.3, alpha=1, marker=markers[i % len(markers)], markersize=4)\n",
        "    # plt.plot(my_qs, avg_of_wards.loc[model], label=model, linewidth=3)\n",
        "    # plt.plot(my_qs, avg_ward.loc[model], label=model)\n",
        "plt.xlabel(\"Quantiles\")\n",
        "plt.ylabel(\"Pinball Loss\")\n",
        "plt.title(\"Pinball Loss across different quantiles (Average across all wards)\", fontsize=20)\n",
        "plt.xticks(my_qs, [f\"{q:.2f}\" for q in my_qs], rotation=90)\n",
        "plt.legend(\n",
        "    loc='upper center', \n",
        "    bbox_to_anchor=(0.5, 0.1),  # (x, y) position, y<0 puts below the axis\n",
        "    ncol=len(qs.index),      # one column per model (horizontal)\n",
        "    frameon=True,  # no box around the legend\n",
        "    fontsize=16  # legend font size\n",
        ")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "id": "142c12c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forecast Distributions (Quantile scores)"
      ],
      "id": "0a91173a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "plt.figure(figsize=(18, 9))\n",
        "markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', 'X', 'd']\n",
        "my_qs = np.arange(0.05, 1.00, 0.05)\n",
        "for i, model in enumerate(qs.index.tolist()):\n",
        "    # if model != \"LR\":\n",
        "        plt.plot(my_qs, qs.loc[model], label=model, linewidth=1.3, alpha=1, marker=markers[i % len(markers)], markersize=4)\n",
        "    # plt.plot(my_qs, avg_of_wards.loc[model], label=model, linewidth=3)\n",
        "    # plt.plot(my_qs, avg_ward.loc[model], label=model)\n",
        "plt.xlabel(\"Quantiles\")\n",
        "plt.ylabel(\"Pinball Loss\")\n",
        "plt.title(\"Pinball Loss across different quantiles (Average across all wards)\", fontsize=20)\n",
        "plt.xticks(my_qs, [f\"{q:.2f}\" for q in my_qs], rotation=90)\n",
        "plt.legend(\n",
        "    loc='upper center', \n",
        "    bbox_to_anchor=(0.5, 0.1),  # (x, y) position, y<0 puts below the axis\n",
        "    ncol=len(qs.index),      # one column per model (horizontal)\n",
        "    frameon=False,  # no box around the legend\n",
        "    fontsize=16  # legend font size\n",
        ")\n",
        "plt.grid()\n",
        "# Highlight area between 0.45 and 0.85\n",
        "plt.axvspan(0.45, 0.85, color='green', alpha=0.08)\n",
        "plt.show()"
      ],
      "id": "fe3460f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forecast distributions (Quantile scores for 45% to 85%)"
      ],
      "id": "379b61b2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "plt.figure(figsize=(18, 9))\n",
        "markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', 'X', 'd']\n",
        "qs_viz = qs.loc[:, \"q0.45\":\"q0.85\"]\n",
        "cut_qs = np.arange(0.45, 0.9, 0.05)\n",
        "for i, model in enumerate(qs_viz.index.tolist()):\n",
        "    # if model != \"LR\":\n",
        "        plt.plot(cut_qs, qs_viz.loc[model], label=model, linewidth=1.3, alpha=1, marker=markers[i % len(markers)], markersize=4)\n",
        "    # plt.plot(my_qs, avg_of_wards.loc[model], label=model, linewidth=3)\n",
        "    # plt.plot(my_qs, avg_ward.loc[model], label=model)\n",
        "plt.xlabel(\"Quantiles\")\n",
        "plt.ylabel(\"Pinball Loss\")\n",
        "plt.title(\"Pinball Loss across different quantiles (Average across all wards)\", fontsize=20)\n",
        "# plt.xticks(cut_qs, [f\"{int(q*100)}%\" for q in cut_qs])\n",
        "plt.xticks(cut_qs, [f\"{q:.2f}\" for q in cut_qs], rotation=90)\n",
        "plt.legend(\n",
        "    loc='upper center', \n",
        "    bbox_to_anchor=(0.5, 0.1),  # (x, y) position, y<0 puts below the axis\n",
        "    ncol=len(qs_viz.index),      # one column per model (horizontal)\n",
        "    frameon=True,  # no box around the legend\n",
        "    fontsize=16  # legend font size\n",
        ")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "id": "a06c953e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Forecast distributions by each horizon"
      ],
      "id": "bcdb71dc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "plt.figure(figsize=(22, 11))\n",
        "# palette = sns.color_palette(\"Blues_r\", n_colors=len(hmm_conforms[\"ward_0\"].dist.columns))\n",
        "df_ = data_prep_f('ward_0', best_hmm_ets_tks[\"ward_0\"][1])\n",
        "dfml = data_prep('ward_0')\n",
        "\n",
        "train_size = len(occup_train_clean)\n",
        "train_df = df_[:train_size] # fit on train data only to avoid data leakage\n",
        "test_df = df_[train_size:train_size+30]\n",
        "\n",
        "train_dfml = dfml[:train_size] # fit on train data only to avoid data leakage\n",
        "test_dfml = dfml[train_size:train_size+30]\n",
        "\n",
        "pi_hmm = hmm_conforms[\"ward_0\"].generate_prediction_intervals(train_df, test_df.drop(columns=[\"ward_0\"]))\n",
        "sns.swarmplot(data=hmm_conforms[\"ward_0\"].dist)\n",
        "plt.xticks(rotation=90, fontsize=12)\n",
        "plt.title(\"Forecast Distribution of horizons\", fontsize=20)\n",
        "plt.ylabel(\"Value\")\n",
        "plt.show()"
      ],
      "id": "017635cb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Model Explainability\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"70%\"}"
      ],
      "id": "3b2f03de"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "# Load from file\n",
        "\n",
        "import pickle\n",
        "with open('quarto_result/hm_model.pkl', 'rb') as f:\n",
        "    hm_model = pickle.load(f)\n",
        "state_probs_train = hm_model.predict_proba()\n",
        "state_predict_train = hm_model.predict_states()\n",
        "\n",
        "df_ = data_prep(ward='ward_0')\n",
        "\n",
        "train_size = len(occup_train_clean)\n",
        "fit_df = df_[:train_size] # fit on train data only to avoid data leakage\n",
        "test_df = df_[train_size:train_size+30]\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "my_series = fit_df[-2074:][\"ward_0\"]\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(22, 11))\n",
        "\n",
        "# Plot the time series\n",
        "ax1.plot(my_series.index[-60:], my_series.values[-60:], label=\"Occupancy\", color=\"C0\")\n",
        "# ax1.plot(fit_df.index, fit_df[\"forecast\"], label=\"Occupancy\", color=\"C3\", linestyle='--')\n",
        "# ax1.plot(vis_data.index, vis_data[\"Discharges\"], label=\"Discharges\", color=\"C1\")\n",
        "ax1.set_ylabel(\"Occupancy level\")\n",
        "ax1.set_xlabel(\"Date\")\n",
        "ax1.legend(loc=\"upper left\")\n",
        "\n",
        "# Overlay probabilities as stacked area\n",
        "ax2 = ax1.twinx()\n",
        "ax2.stackplot(my_series.index[-60:],\n",
        "              state_probs_train[0][-60:], \n",
        "              state_probs_train[1][-60:],\n",
        "              labels=['State 0', 'State 1', 'State 2'], \n",
        "              alpha=0.25, colors=['C2','C3', 'C4'])\n",
        "\n",
        "ax2.set_ylabel(\"State Probability\", fontsize=20)\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.legend(loc=\"upper right\", fontsize=20)\n",
        "\n",
        "plt.title(\"Occupancy with Hidden State Probabilities\", fontsize=24)\n",
        "plt.setp(ax1.get_xticklabels(), rotation=30, ha=\"right\")  # <--- Add this line\n",
        "plt.show()"
      ],
      "id": "f8f1b28b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::: {.column width=\"30%\"}\n",
        "\n",
        "### Transition probabilities"
      ],
      "id": "f3b5819d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tp = pd.DataFrame(hm_model.A).round(3)\n",
        "tp.index = [\"State 0\", \"State 1\"]\n",
        "tp.columns = [\"State 0\", \"State 1\"]\n",
        "tp"
      ],
      "id": "3854f1fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "### Standard Deviations"
      ],
      "id": "fab96cb3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "stds = pd.DataFrame(hm_model.stds).round(2).T\n",
        "# change row name to standard deviation\n",
        "stds = stds.rename(index={0: \"standard deviations\"})\n",
        "stds.columns = [\"State 0\", \"State 1\"]\n",
        "stds"
      ],
      "id": "5541d50c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::::    \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Key findings\n",
        "\n",
        "## Learning outcomes from the current experiments\n",
        "\n",
        "- Although AR-RHMM may not always outperform all benchmark models in point forecasting metrics, there is a significant improvement in probabilistic forecasting, particularly in quantile loss across 40% and 85% quantiles.\n",
        "\n",
        "- The model is more reliable for probabilistic forecasting since it accounts for regime changes and provides a distribution of possible future values, making it suitable for decision-making in hospital resource management.\n",
        "\n",
        "- The model performs consistently well interms of `probabilistic forecasting` since the model is probabilistic in nature and captures regime changes.\n",
        "\n",
        "## Why AR-RHMM?\n",
        "\n",
        "- Captures regime changes in occupancy patterns (patterns of variability in hospital occupancy levels)\n",
        "- Models complex dependencies and non-linear relationships without less risk of overfitting and less need for extensive hyperparameter tuning\n",
        "- Produces interpretable results through regime-specific parameters\n",
        "- Provides reliable forecasts incorporating uncertainty coming from regime changes\n",
        "\n",
        "---\n",
        "\n",
        "## Next steps\n",
        "\n",
        "- Extend the AR-HMM to a multivariate framework (VAR-HMM) to jointly model multiple wards, capturing interdependencies and shared patterns in bed occupancy across different wards.\n",
        "\n",
        "- Develop an optimization framework that utilizes the probabilistic forecasts from the AR-HMM and VAR-HMM models to optimize bed allocation and resource management.\n",
        "\n",
        "- Test the models in real-world scenarios, collaborating with hospital staff to refine the models based on practical feedback and operational needs.\n",
        "\n",
        "\n",
        "\n",
        "# Thank you!\n",
        "\n",
        "<!-- \n",
        "## \n",
        "\n",
        "<div style=\"text-align:center; font-size:5em; margin-top:2em;\">\n",
        "  Thank you\n",
        "</div> -->"
      ],
      "id": "9b86b700"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "quarto-python",
      "language": "python",
      "display_name": "Python (quarto-python)",
      "path": "/Users/aslanm/Library/Jupyter/kernels/quarto-python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}