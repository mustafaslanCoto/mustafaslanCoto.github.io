{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "format:\n",
        "  revealjs:\n",
        "    theme: default\n",
        "    slide-number: c/t\n",
        "    width: 2100\n",
        "    height: 1200\n",
        "    footer: \"[Hospital Bed Occupancy Forecasting: A Regime-Switching AutoRegressive Hidden Markov Model Approach](https://mustafaslancoto.github.io/talks/presentations/wgsss.html)\"\n",
        "    margin: 0.06\n",
        "    auto-stretch: true\n",
        "    css: style.css\n",
        "---\n",
        "\n",
        "<!-- logo style -->\n",
        "\n",
        "<style>\n",
        "  .logo-left {\n",
        "    position: absolute;\n",
        "    top: 10px;\n",
        "    left: 20px;\n",
        "  }\n",
        "\n",
        "  .logo-right {\n",
        "    position: absolute;\n",
        "    top: 10px;\n",
        "    right: 20px;\n",
        "    display: flex;\n",
        "    gap: 10px;\n",
        "    align-items: flex-start;\n",
        "  }\n",
        "\n",
        "  .logo-left img,\n",
        "  .logo-right img {\n",
        "    height: 160px;\n",
        "    width: auto;\n",
        "    \n",
        "  }\n",
        "  \n",
        "/* title sytle */\n",
        "  h1 {\n",
        "    text-align: center;\n",
        "  }\n",
        "\n",
        "</style>\n",
        "\n",
        "<div class=\"logo-left\">\n",
        "  <img src=\"images/cu.png\" alt=\"Left Logo\">\n",
        "  <img src=\"images/wgsss.png\" alt=\"Left Logo 2\">\n",
        "  <img src=\"images/hcrw_back.png\" alt=\"Left Logo 3\">\n",
        "</div>\n",
        "\n",
        "<div class=\"logo-right\">\n",
        "  <img src=\"images/dlsg.png\" alt=\"Right Logo\">\n",
        "  <img src=\"images/care.png\" alt=\"Right Logo 2\">\n",
        "</div>\n",
        "\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <h1>Hospital Bed Occupancy Forecasting: A Regime-Switching AutoRegressive Hidden Markov Model Approach\n",
        "</h1>\n",
        "\n",
        "<div class=\"title-block\">\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <h3>\n",
        "  Mustafa Aslan, Cardiff University, UK <br>\n",
        "  Lead supervisor: Prof. Bahman Rostami-Tabar<br>\n",
        "  Co-supervisor: Dr. Jeremy Dixon <br>\n",
        "  Data Lab for Social Good <br>\n",
        "  Cardiff University, UK <br>\n",
        "  <!-- <span style=\"color: rgba(179, 0, 0, 0.7); font-weight: bold;\">Slides:</span>\n",
        "  <a href=\"https://mustafaslancoto.github.io/talks/\" style=\"color: #000; font-weight: bold; text-decoration: underline dotted;text-underline-offset: 5px\">\n",
        "    https://mustafaslancoto.github.io/talks/\n",
        "  </a> -->\n",
        "  </h3>\n",
        "  <p>16 Sep 2025</p>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "## Outline\n",
        "\n",
        "- The problem\n",
        "\n",
        "- Data and preliminary analysis\n",
        "\n",
        "- Experiment design and Modelling framework\n",
        "\n",
        "- Results\n",
        "\n",
        "- Key findings\n",
        "\n",
        "- Next steps\n",
        "\n",
        "# The Problem\n",
        "\n",
        "## Current Systemic Issues in Patient Flow\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/late_shift.png){width=\"100%\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        " - Unpredictable occupancy levels can result in redundant staffing and resource allocation, driving up costs, or in understaffing, which compromises patient care.\n",
        "\n",
        "<br>\n",
        "\n",
        " - Spikes in occupancy level drive overtime, sickness, and **staff burnout**.\n",
        "\n",
        " <br>\n",
        "\n",
        " - Lack of analytical solutions that can capture the complexity and uncertainty of patient flow and bed availability limits proactive decision-making.\n",
        " \n",
        ":::\n",
        "::::\n",
        "\n",
        "---\n",
        "\n",
        "## Why is it important?\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\" .fragment}\n",
        "\n",
        "### Why is an efficient bed management system crucial?\n",
        "- Hospital beds are one of most costly healthcare resources.\n",
        "    - NHS rates for acute mental health beds are generally between £760 and £850 per day in 2025\n",
        "- Bed shortages force some patients to wait in corridors, impacting care quality and safety.\n",
        "- Bed shortages increase staff pressure, burnout, and operational costs.\n",
        "\n",
        "![](images/bed_shortages.jpg){width=\"100%\"}\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\" .fragment}\n",
        "### Why a reliable forecasting approach is needed?\n",
        "\n",
        "- Mental health demand is volatile and influenced by unpredictable crisis, by seasonality, epidemics, and unforeseen events.\n",
        "- A reliable forecasting approach enables timely staffing, resource allocation, and improved patient care.\n",
        "- Anticipating demand allows decision-makers to balance scarce resources while improving patient flow and safety.\n",
        "- Robust forecasts enhance resilience to unexpected surges and reduce staff burnout.\n",
        ":::\n",
        "::::\n",
        "\n",
        "---\n",
        "\n",
        "## What do we forecast?\n",
        "\n",
        "- Daily hospital bed occupancy for 7 wards in a UK hospital.\n",
        "- **Forecast horizon:** 30 days ahead, supporting medium-term planning for bed allocation and staffing, generated at the end of each month for the following month.\n",
        "- Focus on **probabilistic forecasting** to capture uncertainty, offering prediction intervals rather than single-point estimates using a reliable model."
      ],
      "id": "c8dc1cba"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "plt.rcParams['figure.facecolor'] = \"#FBFAF4\"\n",
        "plt.rcParams['axes.facecolor'] = \"#FBFAF4\"\n",
        "# Example data\n",
        "dates = pd.date_range('2025-06-01', periods=90)\n",
        "actuals = np.random.poisson(22, 60)\n",
        "forecast = np.linspace(22, 26, 30) + np.random.normal(0, 1, 30)\n",
        "actual_future = np.linspace(22, 26, 30) + np.random.normal(0, 1.8, 30)\n",
        "lower = forecast - 3\n",
        "upper = forecast + 3\n",
        "\n",
        "plt.figure(figsize=(18,8))\n",
        "plt.plot(dates[:60], actuals, label=\"Historical Data\", color=\"C0\")\n",
        "plt.plot(dates[60:], forecast, label=\"Point forecast\", color=\"C1\")\n",
        "plt.plot(dates[60:], actual_future, label=\"Actual Future\", color=\"C2\")\n",
        "plt.fill_between(dates[60:], lower, upper, color=\"C1\", alpha=0.3, label=\"Prediction Interval\")\n",
        "plt.xlabel(\"Date\", fontdict={\"fontsize\": 16})\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.ylabel(\"Occupancy\", fontdict={\"fontsize\": 16})\n",
        "plt.title(\"30-Day Bed Occupancy Forecast\", fontdict={\"fontsize\": 18})\n",
        "plt.legend(fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "b1409dfd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Research Questions\n",
        "\n",
        "- How can we accurately forecast hospital bed occupancy to improve resource allocation and patient care?\n",
        "\n",
        "- How can probabilistic forecasting models be developed to account for the inherent uncertainty in patient flow and bed availability?\n",
        "\n",
        "- What are the hidden factors influencing bed occupancy, and how can they be incorporated into forecasting models?\n",
        "\n",
        "---\n",
        "\n",
        "# Data and preliminary analysis\n",
        "\n",
        "## Dataset\n",
        "\n",
        " Daily hospital occupancy data from a UK hospital with 7 wards, spanning from July 2018 to April 2025. The dataset includes:\n",
        "\n",
        "- Daily occupancy counts for each ward (daily number of patients staying in each ward)\n",
        "- Date-related features: day of the week, month, year, day of the month, week of the year\n",
        "- Holiday indicators\n",
        "\n",
        "\n",
        "## Data characteristics\n",
        "\n",
        "- Strong upward trend in occupancy over time\n",
        "\n",
        "- Spike after COVID-19 pandemic"
      ],
      "id": "e1698f91"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "# allow max rows to be displayed\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, root_mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline       \n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
        "import numpy as np\n",
        "from cubist import Cubist\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK, space_eval\n",
        "from hyperopt.pyll import scope\n",
        "pd.set_option('display.max_rows', 150)\n",
        "import pickle # for saving and loading models\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose, MSTL\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "from peshbeen.models import (ml_forecaster, ml_bidirect_forecaster, VARModel, MsHmmRegression, MsHmmVar)\n",
        "from peshbeen.model_selection import (cross_validate,  mv_cross_validate,\n",
        "                                      cv_tune, mv_cv_tune, prob_param_forecasts,\n",
        "                                      tune_ets, tune_sarima, ParametricTimeSeriesSplit,\n",
        "                                      forward_feature_selection, backward_feature_selection,\n",
        "                                      mv_forward_feature_selection, mv_backward_feature_selection,\n",
        "                                      hmm_forward_feature_selection, hmm_backward_feature_selection,\n",
        "                                      hmm_mv_forward_feature_selection, hmm_mv_backward_feature_selection,\n",
        "                                      hmm_cross_validate, hmm_mv_cross_validate, cv_lag_tune, \n",
        "                                      cv_hmm_lag_tune)\n",
        "from peshbeen.statplots import (plot_ccf, plot_PACF_ACF)\n",
        "from peshbeen.stattools import (unit_root_test, cross_autocorrelation,\n",
        "                                lr_trend_model, forecast_trend, pacf_strength, ccf_strength)\n",
        "from peshbeen.transformations import (fourier_terms, rolling_quantile,\n",
        "                        rolling_mean, rolling_std, expanding_mean, expanding_std,\n",
        "                        expanding_quantile, expanding_ets, box_cox_transform,\n",
        "                        back_box_cox_transform,undiff_ts, seasonal_diff, invert_seasonal_diff,\n",
        "                        nzInterval, zeroCumulative, kfold_target_encoder, target_encoder_for_test)\n",
        "from peshbeen.metrics import (MAPE, MASE, MSE, MAE, RMSE, SMAPE, CFE, CFE_ABS, WMAPE, SRMSE, RMSSE, SMAE)\n",
        "from peshbeen.prob_forecast import (ml_conformalizer, hmm_conformalizer, ets_conformalizer, bidirect_ts_conformalizer, var_conformalizer, bag_boost_aggr_conformalizer,\n",
        "                                       bidirect_aggr_conformalizer, ets_aggr_conformalizer, s_arima_aggr_conformalizer,\n",
        "                                       var_aggr_conformalizer, hmm_var_conformalizer)\n",
        "from sktime.transformations.series.boxcox import BoxCoxTransformer\n",
        "sns.set_context(\"talk\")\n",
        "from statsmodels.tsa.seasonal import STL, MSTL\n",
        "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "plt.rcParams['figure.facecolor'] = \"#FBFAF4\"\n",
        "plt.rcParams['axes.facecolor'] = \"#FBFAF4\"\n",
        "occup_raw = pd.read_excel(\"occupancy.xlsx\", index_col=\"Date\")\n",
        "# train-test split for both dataset and test size is 30 days for both datasets occupancy_hrs and occupancy_fourier\n",
        "train_size = len(occup_raw) - 360\n",
        "occup_train = occup_raw[:train_size]\n",
        "occup_test = occup_raw[train_size:]\n",
        "# filter the columns that contains \"ward\"\n",
        "ward_cols_ = [col for col in occup_raw.columns if \"ward\" in col]\n",
        "\n",
        "\n",
        "import pickle\n",
        "# Load from file\n",
        "with open('results/hmm_conforms.pkl', 'rb') as f:\n",
        "    hmm_conforms = pickle.load(f)\n",
        "\n",
        "\n",
        "# Load from file\n",
        "with open('results/lr_conforms.pkl', 'rb') as f:\n",
        "    lr_conforms = pickle.load(f)\n",
        "\n",
        "# Load from file\n",
        "with open('results/ml_conforms.pkl', 'rb') as f:\n",
        "    ml_conforms = pickle.load(f)\n",
        "\n",
        "\n",
        "# Load from file\n",
        "with open('results/ets_conforms.pkl', 'rb') as f:\n",
        "    ets_conforms = pickle.load(f)\n",
        "\n",
        "# Load from file\n",
        "with open('model_params/hmm_best_forward_lags.pkl', 'rb') as f:\n",
        "    hmm_best_forward_lags = pickle.load(f)\n",
        "\n",
        "# Load from file\n",
        "with open('model_params/best_ward_ets_tk.pkl', 'rb') as f:\n",
        "    best_ward_ets_tk = pickle.load(f)\n",
        "# Load from file\n",
        "with open('model_params/best_ward_ets_forecastk.pkl', 'rb') as f:\n",
        "    best_ward_ets_forecastk = pickle.load(f)\n",
        "\n",
        "# Load from file\n",
        "with open('model_params/best_hmm_ets_tks.pkl', 'rb') as f:\n",
        "    best_hmm_ets_tks = pickle.load(f)"
      ],
      "id": "96b36891",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "n_plots = len(ward_cols_)\n",
        "# create a grid with enough slots (2 rows, 4 cols = 8 slots here) and a larger figure size\n",
        "covid_start = pd.to_datetime(\"2020-03-01\")\n",
        "covid_end = pd.to_datetime(\"2022-03-31\")\n",
        "fig, axes = plt.subplots(2, 4, figsize=(22, 11))\n",
        "axes = axes.flatten()  # flatten to index linearly\n",
        "for i, ward in enumerate(ward_cols_):\n",
        "   axes[i].plot(occup_raw.index, occup_raw[ward])\n",
        "   axes[i].set_title(ward, fontsize=20)\n",
        "   axes[i].set_xlabel(\"Date\", fontsize=14)\n",
        "   axes[i].set_ylabel(\"Number of patients\", fontsize=14)\n",
        "   axes[i].tick_params(axis='x', labelsize=12, rotation=45)\n",
        "   axes[i].tick_params(axis='y', labelsize=12)\n",
        "    # Highlight the COVID period\n",
        "   axes[i].axvspan(covid_start, covid_end, color='red', alpha=0.1, label='COVID period')\n",
        "    # Add legend only to the first subplot\n",
        "   if i == 0:\n",
        "     axes[i].legend(loc='upper left', fontsize=12)\n",
        "   # axes[i].legend(fontsize=12)\n",
        "# hide any unused subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "   fig.delaxes(axes[j])\n",
        "plt.tight_layout(pad=2.0)\n",
        "plt.show()"
      ],
      "id": "3e33f1f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "## Seasonal patterns\n",
        "\n",
        "### Day of the week seasonality\n"
      ],
      "id": "dc80096c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "wards = [f\"ward_{i}\" for i in range(7)]  # e.g. 7 wards (0..6)\n",
        "n_plots = len(wards)\n",
        "\n",
        "# create a grid with enough slots (2 rows, 4 cols = 8 slots here)\n",
        "fig, axes = plt.subplots(2, 4, figsize=(22, 11))\n",
        "axes = axes.flatten()  # flatten to index linearly\n",
        "\n",
        "for ax, ward_col in zip(axes[:n_plots], wards):\n",
        "        # Plot each week's data (less transparent)\n",
        "        ward = occup_train.pivot_table(index=\"day_of_week\", columns=\"week_of_year\", values=ward_col, aggfunc='mean').reset_index()\n",
        "        mean = occup_train.groupby('day_of_week')[ward_col].mean()\n",
        "        for week in ward.columns[1:]:\n",
        "            ax.plot(ward.index, ward[week], alpha=0.2)  # Less transparent\n",
        "\n",
        "        # Plot average of all weeks (more transparent, thicker line)\n",
        "        ax.plot(mean.index, mean, color='black', linewidth=2, alpha=0.9, label='Weekly Average')  # More transparent\n",
        "\n",
        "        # Set x-ticks to weekday names in correct order\n",
        "        weekday_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "        ax.set_xticks(range(len(weekday_order)))\n",
        "        ax.set_xticklabels(weekday_order, rotation=30)\n",
        "        ax.set_xlabel(\"Day of Week\", fontsize=12)\n",
        "        ax.tick_params(axis='x', labelsize=12)\n",
        "        ax.tick_params(axis='y', labelsize=12)\n",
        "        ax.set_ylabel(\"Occupancy\", fontsize=12)\n",
        "        ax.set_title(f\"Weekly Occupancy for ward {ward_col}\", fontsize=12)\n",
        "        ax.legend(fontsize=12)\n",
        "\n",
        "# remove any unused axes (if grid has more slots than n_plots)\n",
        "for ax in axes[n_plots:]:\n",
        "    fig.delaxes(ax)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "223e102c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Seasonal patterns\n",
        "\n",
        "### Monthly seasonality"
      ],
      "id": "efade40c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "# year_df = occup_train[occup_train[\"year\"].isin([2019, 2020, 2021, 2022, 2023])]\n",
        "\n",
        "# choose how many wards to plot\n",
        "wards = [f\"ward_{i}\" for i in range(7)]  # e.g. 7 wards (0..6)\n",
        "n_plots = len(wards)\n",
        "\n",
        "# create a grid with enough slots (2 rows, 4 cols = 8 slots here)\n",
        "fig, axes = plt.subplots(2, 4, figsize=(22, 11))\n",
        "axes = axes.flatten()  # flatten to index linearly\n",
        "\n",
        "for ax, ward_col in zip(axes[:n_plots], wards):\n",
        "        # Plot each week's data (less transparent)\n",
        "        ward = occup_train.pivot_table(index=\"month\", columns=\"day_of_month\", values=ward_col, aggfunc='mean').reset_index()\n",
        "        mean = occup_train.groupby('month')[ward_col].mean()\n",
        "        for week in ward.columns[1:]:\n",
        "            ax.plot(ward.index, ward[week], alpha=0.2)  # Less transparent\n",
        "\n",
        "        # Plot average of all weeks (more transparent, thicker line)\n",
        "        ax.plot(mean.index, mean, color='black', linewidth=2, alpha=0.9, label='Monthly Average')  # More transparent\n",
        "        # Set x-ticks to month names in calendar order\n",
        "        month_order = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
        "        ax.set_xticks(range(len(month_order)))\n",
        "        ax.set_xticklabels(month_order, rotation=45)\n",
        "        ax.set_xlabel(\"Month\", fontsize=9)\n",
        "        ax.tick_params(axis='both', labelsize=9)\n",
        "        ax.tick_params(axis='x', labelsize=10)\n",
        "        ax.tick_params(axis='y', labelsize=12)\n",
        "        ax.set_ylabel(\"Occupancy\", fontsize=10)\n",
        "        ax.set_title(f\"Monthly Occupancy for {ward_col}\", fontsize=10)\n",
        "        ax.legend(loc='upper right', fontsize=10)\n",
        "\n",
        "# remove any unused axes (if grid has more slots than n_plots)\n",
        "for ax in axes[n_plots:]:\n",
        "    fig.delaxes(ax)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "81a5bfe4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Effect of holidays"
      ],
      "id": "4e1d6a5f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(24, 11))\n",
        "df = occup_train.loc[:\"2022-07-31\"]\n",
        "\n",
        "# Plot all days with small markers\n",
        "axes.scatter(df.index, df[\"ward_0\"], s=16, color=\"C0\", alpha=0.5, zorder=1)\n",
        "\n",
        "    \n",
        "# Get holiday data points and overlay larger markers\n",
        "holiday_data = df[df[\"holiday\"] != \"not_holiday\"]\n",
        "sns.scatterplot(data=holiday_data, x=holiday_data.index, y=\"ward_0\",\n",
        "                ax=axes, s=100, color=\"C1\", edgecolor=\"white\", linewidth=0.5,\n",
        "                label=\"Public Holidays\", zorder=3)\n",
        "axes.legend(loc='upper left', fontsize=16)\n",
        "\n",
        "# Add date boxes for each holiday\n",
        "for idx, (date, row) in enumerate(holiday_data.iterrows()):\n",
        "    occupancy = row['ward_0']\n",
        "    \n",
        "    # Format the date string\n",
        "    # date_str = date.strftime('%Y-%m-%d')\n",
        "    date_str = row['holiday']\n",
        "    \n",
        "    # Calculate text position (slightly offset from the point)\n",
        "    x_offset = pd.Timedelta(days=30)  # Adjust this for horizontal spacing\n",
        "    y_offset = 0.5  # Adjust this for vertical spacing\n",
        "    \n",
        "    # Alternate positioning to avoid overlap\n",
        "    if idx % 2 == 0:\n",
        "        text_x = date + x_offset\n",
        "        text_y = occupancy + y_offset\n",
        "        ha = 'left'\n",
        "    else:\n",
        "        text_x = date - x_offset\n",
        "        text_y = occupancy - y_offset\n",
        "        ha = 'right'\n",
        "    \n",
        "    # Add the date box\n",
        "    bbox_props = dict(boxstyle=\"round,pad=0.3\", facecolor='white', \n",
        "                     edgecolor='C1', alpha=0.5, linewidth=0.5)\n",
        "    \n",
        "    axes.annotate(date_str, \n",
        "                 xy=(date, occupancy),           # Point to annotate\n",
        "                 xytext=(text_x, text_y),        # Text position\n",
        "                 bbox=bbox_props,\n",
        "                 fontsize=12,\n",
        "                 rotation=50,\n",
        "                 ha=ha,\n",
        "                 va='center',\n",
        "                 arrowprops=dict(arrowstyle='->', \n",
        "                               connectionstyle='arc3,rad=0.1',\n",
        "                               color='white', alpha=0.6))\n",
        "\n",
        "axes.set_title(\"The effect of Public Holidays on Occupancy\", fontsize=18)\n",
        "axes.set_xlabel(\"Date\", fontsize=16)\n",
        "axes.tick_params(axis='x', labelsize=16)   # Sets fontsize for x-ticks\n",
        "axes.tick_params(axis='y', labelsize=16)   # Sets fontsize for y-ticks if needed\n",
        "axes.set_ylabel(\"Occupancy\", fontsize=16)\n",
        "plt.show()"
      ],
      "id": "376f1056",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key insights from data analysis\n",
        "\n",
        "### An important assumption to model: Stationarity\n",
        "\n",
        "One approach to make the series stationary is to take the first `difference` of the series.\n",
        "\n",
        "Original series: $y_t, y_{t-1}, y_{t-2}, y_{t-3}, \\ldots$ \\\n",
        "Differenced series: $y_t - y_{t-1}, y_{t-1} - y_{t-2}, y_{t-2} - y_{t-3}, \\ldots$"
      ],
      "id": "0f68d73e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "# Load from file\n",
        "\n",
        "import pickle\n",
        "with open('model_params/best_ward_ets_tk.pkl', 'rb') as f:\n",
        "    best_ward_ets_tk = pickle.load(f)\n",
        "\n",
        "\n",
        "import ruptures as rpt\n",
        "\n",
        "def stl_data_input(series, period=7, seasonal=2999):\n",
        "    # Apply STL decomposition\n",
        "    res = STL(series.interpolate(method=\"linear\").squeeze(), # Convert dataframe to a Series\n",
        "                                                        # to avoid error in Statsmodels\n",
        "            robust=True, period=period, seasonal=seasonal).fit()\n",
        "    \n",
        "    seasonal_component = res.seasonal\n",
        "    # residual_component = res.resid\n",
        "    # De-seasonlise original data\n",
        "    df_deseasonalised = series - seasonal_component\n",
        "\n",
        "    # # Perform linear interpolation on de-seasonalised data\n",
        "    df_deseasonalised_imputed = df_deseasonalised.interpolate(method=\"linear\")\n",
        "\n",
        "    # # Randomly sample residuals (with replacement) and add back\n",
        "    # sampled_residuals = np.random.choice(residual_component, size=len(residual_component), replace=True)\n",
        "    # # Match DataFrame index\n",
        "    # sampled_residuals = pd.Series(sampled_residuals, index=series.index)\n",
        "\n",
        "    # # Add seasonal component back to get the final imputed time series\n",
        "    df_imputed = df_deseasonalised_imputed + seasonal_component\n",
        "\n",
        "    return df_imputed\n",
        "\n",
        "def data_prep_f(ward, fourier_k):\n",
        "    ward_train = occup_train_clean[[ward]+cat_col_f]\n",
        "    ward_test = occup_test[[ward]+cat_col_f]\n",
        "    ward_test[ward] = ward_test[ward].interpolate(method=\"linear\")\n",
        "    ward_test[\"day_of_week\"] = ward_test.index.day_name()\n",
        "    df_all = pd.concat([ward_train, ward_test], axis=0)\n",
        "    ft = fourier_terms(start_end_index=(df_all.index.min(), df_all.index.max()),\n",
        "                period=365.25, num_terms=fourier_k)\n",
        "    return df_all.merge(ft, left_index=True, right_index=True, how=\"left\")\n",
        "\n",
        "def data_prep(ward):\n",
        "    ward_train = occup_train_clean[[ward]+cat_cols]\n",
        "    ward_test = occup_test[[ward]+cat_cols]\n",
        "    ward_test[ward] = ward_test[ward].interpolate(method=\"linear\")\n",
        "    ward_test[\"day_of_week\"] = ward_test.index.day_name()\n",
        "    ward_test[\"month\"] = ward_test.index.month_name()\n",
        "    return pd.concat([ward_train, ward_test], axis=0)\n",
        "\n",
        "## first input nan values using STL\n",
        "for ward in ward_cols_:\n",
        "    occup_train[ward] = stl_data_input(occup_train[ward], period=None, seasonal=2999)\n",
        "\n",
        "## replace the outlier values between 965 and 1040 by interpolation\n",
        "occup_train_clean = occup_train.copy()\n",
        "occup_train_nan = occup_train.copy()\n",
        "occup_train_nan.iloc[965-1:1040+1, occup_train_nan.columns.get_loc('ward_0')] = np.nan\n",
        "occup_train_clean['ward_0'] = stl_data_input(occup_train_nan['ward_0'], period=None, seasonal=2999)\n",
        "\n",
        "occup_train_nan.iloc[815-1:945+1, occup_train_nan.columns.get_loc('ward_4')] = np.nan\n",
        "occup_train_clean['ward_4'] = stl_data_input(occup_train_nan['ward_4'], period=None, seasonal=2999)\n",
        "\n",
        "## replace the outlier values between 1215 and 1395 by interpolation\n",
        "occup_train_nan.iloc[915-1:970+1, occup_train_nan.columns.get_loc('ward_5')] = np.nan\n",
        "occup_train_nan.iloc[1215-1:1395+1, occup_train_nan.columns.get_loc('ward_5')] = np.nan\n",
        "occup_train_clean['ward_5'] = stl_data_input(occup_train_nan['ward_5'], period=None, seasonal=2999)\n",
        "\n",
        "occup_train_clean[\"day_of_week\"] = occup_train_clean.index.day_name()\n",
        "occup_train_clean[\"month\"] = occup_train_clean.index.month_name()\n",
        "occup_train_clean[\"is_holiday\"] = np.where(occup_train_clean[\"is_holiday\"] == 1, \"holiday\", \"not holiday\")\n",
        "cat_cols = [\"day_of_week\", \"month\", \"is_holiday\"]\n",
        "cat_col_f = [\"day_of_week\", \"is_holiday\"]\n",
        "\n",
        "df_diff = pd.DataFrame()\n",
        "for w in ward_cols_:\n",
        "    df_diff[f'{w}'] = occup_train_clean[w]\n",
        "    df_diff[f'{w}_diff'] = occup_train_clean[w].diff()\n",
        "    df_diff[f'{w}_fitted_ets_trend'] = ExponentialSmoothing(occup_train_clean[w], **best_ward_ets_tk[w][0][0]).fit(**best_ward_ets_tk[w][0][1]).fittedvalues\n",
        "    df_diff[f'{w}_detrended'] = df_diff[f'{w}'] - df_diff[f'{w}_fitted_ets_trend']\n",
        "df_diff.dropna(inplace=True)\n",
        "\n",
        "# plot original differenced series in two plots in one graph\n",
        "fig, axs = plt.subplots(2, 1, figsize=(18, 9))\n",
        "axs[0].plot(df_diff.index, df_diff['ward_0'], label='Original Series', color='C0', alpha=0.9)\n",
        "axs[0].set_title('Original Series for ward_0')\n",
        "axs[0].legend(loc = 'lower right')\n",
        "axs[1].plot(df_diff.index, df_diff['ward_0_diff'], label='Differenced Series', color='C1', alpha=0.9)\n",
        "axs[1].set_title('Differenced Series for ward_0 occupancy')\n",
        "axs[1].legend(loc = 'lower right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "bf6ec870",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key insights from data analysis\n",
        "\n",
        "### Information from ACF and PACF plots after differencing"
      ],
      "id": "7ab01bf6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "fig, axes = plt.subplots(2, 1, figsize=(22, 11))\n",
        "plot_acf(df_diff['ward_0_diff'], lags=40, ax=axes[0])\n",
        "plot_pacf(df_diff['ward_0_diff'], lags=40, ax=axes[1])\n",
        "axes[0].set_title('ACF of Differenced Series for ward_0 occupancy')\n",
        "axes[1].set_title('PACF of Differenced Series for ward_0 occupancy')\n",
        "plt.tight_layout(pad=2)\n",
        "plt.show()"
      ],
      "id": "4147f2ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key insights from data analysis\n",
        "\n",
        "### Alternative approach to make the series stationary and keep the dependence structure"
      ],
      "id": "15521c3c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "# plot original differenced series in two plots in one graph\n",
        "plt.figure(figsize=(22, 11))\n",
        "plt.plot(df_diff.index[-500:], df_diff['ward_0'][-500:], label='Original Series', color='C0', alpha=0.9)\n",
        "plt.plot(df_diff.index[-500:], df_diff['ward_0_fitted_ets_trend'][-500:], label='Fitted ETS Trend', color='C1', alpha=0.9)\n",
        "plt.plot(df_diff.index[-500:], df_diff['ward_0_detrended'][-500:], label='Detrended Series', color='C2', alpha=0.9)\n",
        "plt.title('Original Series vs Detrended Series for ward_0')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "id": "c4647ecb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## An important assumption to model: Stationarity\n",
        "### Information from ACF and PACF plots after detrending"
      ],
      "id": "9387ed65"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "fig, axes = plt.subplots(2, 1, figsize=(22, 11))\n",
        "plot_acf(df_diff['ward_0_detrended'], lags=40, ax=axes[0])\n",
        "plot_pacf(df_diff['ward_0_detrended'], lags=40, ax=axes[1])\n",
        "axes[0].set_title('ACF of Detrended Series for ward_0 occupancy')\n",
        "axes[1].set_title('PACF of Detrended Series for ward_0 occupancy')\n",
        "plt.tight_layout(pad=2)\n",
        "plt.show()"
      ],
      "id": "d3c93b1e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Regime-Switching AutoRegressive Hidden Markov Model (RS-ARHMM) {.smaller}\n",
        "\n",
        "Let $y_t$ be the observed value at time $t$, modeled as a function of its $p$ lagged values, the regime-specific parameters associated with the hidden state $s_t$, and exogenous variables $\\mathbf{X}_t = (X_{t1}, \\ldots, X_{tM})$.\n",
        "\n",
        "The **RS-ARHMM** can be expressed as follows:\n",
        "\n",
        "$$\n",
        "y_t^{(s)} = \\beta_{0}^{(s)} + \\sum_{i=1}^{p} \\beta_{i}^{(s)} \\, y_{t-i} + \\sum_{j=1}^{M} \\beta_{p+j}^{(s)} \\, X_{tj} + \\epsilon_t^{(s)},\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $\\beta_{i}^{(s)}$ are the coefficients for the lagged values for regime $s$,\n",
        "- $\\beta_{p+j}^{(s)}$ are the coefficients for the exogenous variables for regime $s$,\n",
        "- $M$ is the number of exogenous variables,\n",
        "- $\\epsilon_t^{(s)}$ is the error term for regime $s$.\n",
        "\n",
        "The **Markov property**:\n",
        "$$\n",
        "P(s_t = k \\mid s_{1:t-1}) = P(s_t = k \\mid s_{t-1}), \\quad \\forall t \\geq 2.\n",
        "$$\n",
        "\n",
        "A HMM has the following components:\n",
        "\n",
        "- $S$: The set of regimes, $\\mathbb{S} = \\{S_1, S_2, \\ldots, S_K\\}$.\n",
        "- $P$: The transition probability matrix\n",
        "- $p_{ij} = P(s_t = S_j \\mid s_{t-1} = S_i)$ is the probability of transitioning from regime $S_i$ to regime $S_j$, s.t. $\\sum_{j=1}^{K} p_{ij} = 1$.\n",
        "\n",
        "**Transition matrix $P$** is defined as:\n",
        "\n",
        "$$\n",
        "P = \\begin{pmatrix}\n",
        "p_{11} & \\cdots & p_{1K} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "p_{K1} & \\cdots & p_{KK}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "## Modeling Framework {.smaller}\n",
        "\n",
        "### Forecasting with RS-ARHMM\n",
        "\n",
        "To forecast $h$ steps ahead, we employ a systematic three-stage approach that combines state prediction with regime-specific observation modeling.\n",
        "\n",
        "1. **State Prediction**: We predict the most probable state sequence for the next $h$ time steps using the transition matrix $\\mathbf{P}$. The probability of being in state $S_j$ at time $T+h$ is computed recursively as:\n",
        "\n",
        "$$\n",
        "P(s_{T+h} = S_j) = \\sum_{i=1}^{K} P(s_{T+h-1} = S_i) \\, p_{ij}\n",
        "$$\n",
        "\n",
        "The initial state distribution $P(s_{T+1} = S_j)$ for the first step is estimated from the last estimated state probabilities obtained from the forward algorithm at time $T$.\n",
        "\n",
        "2. **Regime-specific forecasts**: For each predicted regime, we utilize the corresponding regression model to forecast the observation at that time step, incorporating both lagged values and exogenous variables. The predicted observation $\\hat{y}_{T+h}$ for state $s_{T+h}$ is computed as:\n",
        "\n",
        "$$\n",
        "\\hat{y}_{T+h}^{s_{T+h}} = \\beta_{0}^{(s_{T+h})} + \\sum_{i=1}^{p} \\beta_{i}^{(s_{T+h})} \\, y_{T+h-i} + \\sum_{j=1}^{M} \\beta_{p+j}^{(s_{T+h})} \\, X_{T+h,j}\n",
        "$$\n",
        "\n",
        "\n",
        "For multi-step forecasting, we use the predicted observations as inputs for subsequent predictions. $y_{T+h-i}$ are the actual observed values for $i \\leq h$ and the previously predicted values for $i > h$.\n",
        "\n",
        "3. **Final Forecast Computation**: The final forecast for each time step is a weighted average of the forecasts from each regime, weighted by the predicted regime probabilities:\n",
        "\n",
        "$$\n",
        "\\hat{y}_{T+h} = \\sum_{j=1}^{K} P(s_{T+h} = S_j) \\, \\hat{y}_{T+h}^{(S_j)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## RS-ARHMM Parameter Estimation\n",
        ":::: {.columns}\n",
        "::: {.column width=\"30%\" .fragment}\n",
        "\n",
        "\n",
        "::: {.callout-note icon=\"false\"}\n",
        "## `EM (Expectation-Maximization) Algorithm`\n",
        "- Iteratively estimates parameters $\\Theta = \\{\\beta^{(s)}, \\sigma^{2(s)}, P, \\pi\\}$ to maximize likelihood.\n",
        "\n",
        "- Alternates between two steps:\n",
        "  - **E-Step:** Estimate regime probabilities given current $\\Theta$.\n",
        "  - **M-Step:** Update $\\Theta$ using these probabilities.\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.column width=\"35%\" .fragment}\n",
        "::: {.callout-note icon=\"false\"}\n",
        "## 1. E-Step\n",
        "\n",
        "- **State probability:**  \n",
        "$$\n",
        "\\gamma_t(i) = P(s_t = S_i \\mid y_{1:T}, \\Theta^{(k)})\n",
        "$$\n",
        "\n",
        "- **Transition probability:**  \n",
        "$$\n",
        "\\xi_t(i,j) = P(s_t = S_i, s_{t+1} = S_j \\mid y_{1:T}, \\Theta^{(k)})\n",
        "$$\n",
        "\n",
        "- Computed via **Forward-Backward algorithm**:\n",
        "  \n",
        "  - Forward:\n",
        "  \n",
        "  $\\alpha_t(i) = P(y_{1:t}, s_t = S_i \\mid \\Theta) = \\sum_{j=1}^{K} \\alpha_{t-1}(j) p_{ji} o_i(y_t)$\n",
        "  \n",
        "  - Backward:\n",
        "  \n",
        "  $b_t(i) = P(y_{t+1:T} \\mid s_t = S_i, \\Theta) = \\sum_{j=1}^{K} p_{ij} o_j(y_{t+1}) b_{t+1}(j)$\n",
        "\n",
        "\n",
        "**Emission (observation) likelihood:**  \n",
        "$$\n",
        "o_i(y_t) = \\frac{1}{\\sqrt{2\\pi \\sigma_i^2}} \\exp\\left( -\\frac{(y_t - \\mu_{i,t})^2}{2\\sigma_i^2} \\right)\n",
        "$$\n",
        "\n",
        "where $\\mu_{i,t}$ is AR + exogenous mean for state $i$.\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.column width=\"35%\" .fragment}\n",
        "::: {.callout-note icon=\"false\"}\n",
        "## 2. M-Step\n",
        "\n",
        "\n",
        "Update parameters using weighted averages with $\\gamma_t(i)$:\n",
        "\n",
        "- **Transition matrix:**  \n",
        "$$\n",
        "p_{ij}^{(k+1)} = \\frac{\\sum_{t=1}^{T-1} \\xi_t(i,j)}{\\sum_{t=1}^{T-1} \\gamma_t(i)}\n",
        "$$\n",
        "\n",
        "- **Initial state:**  \n",
        "$$\n",
        "\\pi_i^{(k+1)} = \\gamma_1(i)\n",
        "$$\n",
        "\n",
        "- **Regression (AR/exog) coefficients:**  \n",
        "$$\n",
        "\\beta^{(i,k+1)} = (\\mathbf{X}^T \\mathbf{W}_i \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W}_i \\mathbf{y}\n",
        "$$\n",
        "\n",
        "Where $\\mathbf{W}_i = \\text{diag}(\\gamma_1(i),\\dots,\\gamma_T(i))$\n",
        "\n",
        "- **Variance:**  \n",
        "$$\n",
        "\\sigma_i^{2(k+1)} = \\frac{\\sum_{t=1}^T \\gamma_t(i)(y_t-\\mu_{i,t})^2}{\\sum_{t=1}^T \\gamma_t(i)}\n",
        "$$\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.callout-note icon=\"false\" .fragment}\n",
        "## Repeat E-M Steps until convergence:\n",
        "$$|\\ell(\\Theta^{(k+1)}) - \\ell(\\Theta^{(k)})| < \\epsilon$$\n",
        "\n",
        ":::\n",
        ":::\n",
        "::::\n",
        "\n",
        "---\n",
        "\n",
        "## Why RS-ARHMM?\n",
        "\n",
        "::: {.incremental}\n",
        "- **Captures regime changes** in highly volatile time series data (e.g., healthcare), integrating them into the forecasting process, making it more accurate and reliable predictions.\n",
        "\n",
        "- **Traditional statistical and modern machine learning models** fail to account for such regime shifts.\n",
        "\n",
        "- **Models complex dependencies and non-linear relationships** with reduced risk of overfitting and less reliance on extensive hyperparameter tuning.\n",
        "\n",
        "- **Produces interpretable results** through regime-specific parameters.\n",
        "\n",
        "- **Provides reliable forecasts** that incorporate uncertainty arising from regime changes.\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "## Benchmark models\n",
        "\n",
        "**Statistical models:**\n",
        "\n",
        "- `Exponential Smoothing (ETS)`: A state space time series model capturing level, trend, and seasonality.\n",
        "- `Linear Regression`: A statistical model that estimates the linear relationship between predictors and a response variable.\n",
        "- `Lasso Regression`: A regression method with *L1* regularization, useful for variable selection and preventing overfitting.\n",
        "\n",
        "**Machine Learning models:**\n",
        "\n",
        "- `XGBoost`: An optimized gradient boosting library designed to be highly efficient and flexible. It uses level-wise tree growth, building trees level by level horizontally.\n",
        "- `LightGBM`: A gradient boosting framework that uses tree-based learning algorithms, known for its speed and efficiency. It uses leaf-wise tree growth.\n",
        "- `Random Forest`: An ensemble learning method that builds multiple decision trees and merges them together to get a more accurate and stable prediction.\n",
        "\n",
        "---\n",
        "\n",
        "## Probabilistic Forecasting using Conformal Prediction {.smaller}\n",
        "\n",
        "### Conformal Prediction for Time Series\n",
        "\n",
        "- Conformal prediction is a distribution-free, finite-sample valid method for constructing prediction intervals\n",
        "- It provides a way to quantify the uncertainty of point forecasts by generating prediction intervals that are guaranteed to contain the true future values with a specified probability.\n",
        "\n",
        "::: {.fragment}\n",
        "### Steps to Generate Prediction Intervals\n",
        ":::\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "::: {.fragment}\n",
        "1. **Point Forecasting**:\n",
        "\n",
        "- Generate point forecasts $\\hat{y}_{t+h}$ for the desired forecast horizon $h$ using rolling-origin cross-validation on a `calibration set` of 500 observations per horizon, derived from the last 500 days of training data.\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "2. **Residual Calculation**: Compute the residuals from the calibration data set:\n",
        "$$\n",
        "r_{t+h} = y_{t+h} - \\hat{y}_{t+h}\n",
        "$$\n",
        "\n",
        "where $y_{t+h}$ are the actual observed values and $\\hat{y}_{t+h}$ are the point forecasts for $t+h$.\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "3. **Nonconformity Scores**: Calculate the nonconformity scores associated with the residuals of each forecast horizon. Nonconformity scores are calculated as the absolute values of the residuals:\n",
        "$$\n",
        "A_{t+h} = |r_{t+h}|\n",
        "$$\n",
        "\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "::: {.fragment}\n",
        "4. **Quantile Calculation**: For a desired confidence level $1 - \\alpha$, compute the $(1 - \\alpha)$-quantile of the nonconformity scores:\n",
        "\n",
        "$$\n",
        "q_{1-\\alpha}^{(h)} = \\text{Quantile}_{1-\\alpha}(\\{A_{t-n+h}\\}_{t=t-n}^{n})\n",
        "$$\n",
        "\n",
        "where $n$ is the size of the calibration set.\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "5. **Prediction Intervals**: Construct the prediction intervals for the forecast horizon $h$ as:\n",
        "$$\n",
        "\\left[\\hat{y}_{t+h} - q_{1-\\alpha}^{(h)}, \\hat{y}_{t+h} + q_{1-\\alpha}^{(h)}\\right]\n",
        "$$\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "6. **Distribution Approximation**: For each forecast horizon, generate a distribution of possible future values by adding residuals calculated from calibration set to the point forecasts.\n",
        "\n",
        "$$\n",
        "y_{t+h}^{(i)} = \\hat{y}_{t+h} + r_, \\quad r_i \\in \\{r_1, r_2, \\ldots, r_n\\}\n",
        "$$\n",
        "\n",
        "where $n$ is the number of residuals in the calibration set and $i = 1, 2, \\ldots, n$.\n",
        ":::\n",
        "\n",
        ":::\n",
        "::::\n",
        "\n",
        "---\n",
        "\n",
        "## Forecasting metrics\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "### Point Forecasting Metrics\n",
        "\n",
        "- Scaled Root Mean Squared Error (SRMSE)\n",
        "$$\n",
        "SRMSE = \\frac{\\sqrt{\\frac{1}{h} \\sum_{t\n",
        "=n+1}^{n+h} (y_t - \\hat{y}_t)^2}}{\\frac{1}{n-1} \\sum_{t=2}^{n} y_t}\n",
        "$$\n",
        "\n",
        "- Root Mean Squared Error (RMSE)\n",
        "$$\n",
        "RMSE = \\sqrt{\\frac{1}{n} \\sum_{t=1}^{n} (y_t - \\hat{y}_t)^2}\n",
        "$$\n",
        "\n",
        "- Mean Absolute Error (MAE)\n",
        "$$\n",
        "MAE = \\frac{1}{n} \\sum_{t=1}^{n} |y_t - \\hat{y}_t|\n",
        "$$\n",
        "\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "### Probabilistic Forecasting Metrics\n",
        "- Qunatile Loss (QL)\n",
        "$$\n",
        "QL_{\\alpha} = 2 \\sum_{t=1}^{h} \\left[\\alpha (y_t - \\hat{y}_t^{(\\alpha)}) \\mathbb{1}_{\\{y_t > \\hat{y}_t^{(\\alpha)}\\}} + (1 - \\alpha) (\\hat{y}_t^{(\\alpha)} - y_t) \\mathbb{1}_{\\{y_t \\leq \\hat{y}_t^{(\\alpha)}\\}}\\right]\n",
        "$$\n",
        "\n",
        ":::\n",
        "::::\n",
        "---\n",
        "\n",
        "## Feature engineering\n",
        "\n",
        "- Time-based features: Extracted day and month from timestamps to capture temporal patterns.\n",
        "- Public holidays: Created binary indicators for public holidays to account for their impact on hospital occupancy.\n",
        "- Lagged features: Created lagged versions of key variables to incorporate past information into the models.\n",
        "- Fourier terms: Added Fourier series terms for linear models to capture seasonality in the data.\n",
        "- Rolling window features: Computed rolling means and standard deviations to see if improving the model's performance.\n",
        "\n",
        "---\n",
        "\n",
        "## Training and validation setup\n",
        "- The last 360 days of data are reserved as the test set to evaluate model performance\n",
        "- The remaining data is used for:\n",
        "    -   training and validation\n",
        "    -   hyperparameter tuning through rolling-origin cross-validation with a fixed window size of 30 days and step size of 13 days to capture yearly seasonality.\n",
        "    -   Generate probabilistic forecasts using conformal prediction using optimized models on the training data.\n",
        "- `The train-test split ensures that the models are evaluated on unseen data, providing a realistic assessment of their forecasting capabilities.`\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Cross validation setup\n",
        "\n",
        "Rolling-origin cross-validation is used to evaluate model performance over time"
      ],
      "id": "853ff141"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "def plot_cv_indices(cv, X, y, ax=None,\n",
        "                    train_tail=50, step_size=13,\n",
        "                    annotate=True, first_n=5, last_n=1):\n",
        "    \"\"\"\n",
        "    Visualize cross-validation indices for any CV splitter.\n",
        "    Shows the first_n splits, ellipsis, and last_n splits.\n",
        "    \"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(24, 11))\n",
        "\n",
        "    splits = list(cv.split(X))\n",
        "    total_splits = len(splits)\n",
        "\n",
        "    # Indices for splits to show: first_n at bottom, last_n at top\n",
        "    show_indices = list(range(first_n)) + list(range(total_splits - last_n, total_splits))\n",
        "    yticks = []\n",
        "    yticklabels = []\n",
        "    plot_ys = []\n",
        "\n",
        "    # Gather all x-values for shown splits\n",
        "    all_xs = []\n",
        "    for plot_idx, split_idx in enumerate(show_indices):\n",
        "        train, test = splits[split_idx]\n",
        "        tail_len = train_tail + split_idx * step_size\n",
        "        train_tail_idx = train[-tail_len:] if len(train) > tail_len else train\n",
        "        all_xs.extend(train_tail_idx)\n",
        "        all_xs.extend(test)\n",
        "\n",
        "    # Compute y-locations: first_n at bottom, last at top, ellipsis in between\n",
        "    for i in range(first_n):\n",
        "        yticks.append(i + 0.5)\n",
        "        yticklabels.append(f\"Split {show_indices[i]+1}\")\n",
        "        plot_ys.append(i + 0.5)\n",
        "    ellipsis_y = first_n + 0.5\n",
        "    yticks.append(ellipsis_y)\n",
        "    yticklabels.append(\"\")\n",
        "    plot_ys.append(None)\n",
        "    yticks.append(first_n + 1.5)\n",
        "    yticklabels.append(f\"Split {show_indices[-1]+1}\")\n",
        "    plot_ys.append(first_n + 1.5)\n",
        "\n",
        "    # Plot the splits\n",
        "    for plot_idx, split_idx in enumerate(show_indices):\n",
        "        if plot_idx < first_n:\n",
        "            y_pos = plot_idx + 0.5\n",
        "        else:\n",
        "            y_pos = first_n + 1.5  # last split at top\n",
        "        train, test = splits[split_idx]\n",
        "        tail_len = train_tail + split_idx * step_size\n",
        "        train_tail_idx = train[-tail_len:] if len(train) > tail_len else train\n",
        "\n",
        "        ax.scatter(train_tail_idx, [y_pos] * len(train_tail_idx),\n",
        "                   c=\"C0\", marker=\"s\", s=50,\n",
        "                   label=\"Train (tail)\" if plot_idx == 0 else \"\")\n",
        "        ax.scatter(test, [y_pos] * len(test),\n",
        "                   c=\"C1\", marker=\"s\", s=50,\n",
        "                   label=\"Test\" if plot_idx == 0 else \"\")\n",
        "\n",
        "        if annotate:\n",
        "            ax.text(train_tail_idx[0], y_pos + 0.15,\n",
        "                    f\"Train size ={len(train)}\", color=\"C0\", fontsize=14, ha=\"left\")\n",
        "            ax.text(test[len(test)//2]+10, y_pos + 0.15,\n",
        "                    f\"Test size ={len(test)}\", color=\"C1\", fontsize=14, ha=\"center\")\n",
        "\n",
        "    # Add ellipsis at a representative x-position (center of shown data)\n",
        "    if all_xs:\n",
        "        ellipsis_x = 0.5 * (min(all_xs) + max(all_xs))\n",
        "    else:\n",
        "        ellipsis_x = 0\n",
        "    ax.text(ellipsis_x, ellipsis_y, \"......\", fontsize=32, ha=\"center\", va=\"center\", color=\"gray\")\n",
        "\n",
        "    ax.set(\n",
        "        yticks=yticks,\n",
        "        yticklabels=yticklabels,\n",
        "        xlabel=\"Data size\",\n",
        "        ylabel=\"CV iteration\",\n",
        "        ylim=(0, yticks[-1] + 1),\n",
        "    )\n",
        "    # Tighten xlim to shown data (add small margin)\n",
        "    if all_xs:\n",
        "        margin = max(1, int(0.01 * (max(all_xs) - min(all_xs))))\n",
        "        ax.set_xlim(min(all_xs) - margin, max(all_xs) + margin)\n",
        "\n",
        "    handles = [\n",
        "        Patch(color='C0', label='Train'),\n",
        "        Patch(color='C1', label='Test'),\n",
        "    ]\n",
        "    ax.legend(handles=handles, loc=\"lower right\")\n",
        "    return ax\n",
        "\n",
        "\n",
        "X = np.arange(2449)\n",
        "y = np.random.randint(0, 2, size=2449)\n",
        "tscv = ParametricTimeSeriesSplit(n_splits=30, test_size=30, step_size=13)\n",
        "axp = plot_cv_indices(cv=tscv, X=X, y=y)\n",
        "axp\n",
        "\n",
        "axp.axvline(2039, ymin=0.15, ymax=0.25,\n",
        "                    color=\"C2\", linestyle=\"--\", alpha=0.7, linewidth=1)\n",
        "axp.axvline(2052, ymin=0.15, ymax=0.25,\n",
        "                    color=\"C2\", linestyle=\"--\", alpha=0.7, linewidth=1)\n",
        "\n",
        "axp.text(2039, 2,\n",
        "        \"step size=13\", color=\"C2\", fontsize=14, ha=\"left\")\n",
        "\n",
        "axp.axvline(2052, ymin=0.30, ymax=0.38,\n",
        "                    color=\"C2\", linestyle=\"--\", alpha=0.7, linewidth=1)\n",
        "axp.axvline(2065, ymin=0.30, ymax=0.38,\n",
        "                    color=\"C2\", linestyle=\"--\", alpha=0.7, linewidth=1)\n",
        "\n",
        "axp.text(2052, 3,\n",
        "        \"step size=13\", color=\"C2\", fontsize=14, ha=\"left\")\n",
        "\n",
        "axp.tick_params(axis='x', labelsize=16)\n",
        "axp.tick_params(axis='y', labelsize=16)\n",
        "\n",
        "plt.title(\"Rolling Origin Time Series Cross-Validation\", fontsize=18)\n",
        "plt.show()"
      ],
      "id": "b1ba4c65",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results\n",
        "\n",
        "## Point Forecast results\n",
        "\n",
        "::: {layout-ncol=2}"
      ],
      "id": "c44c2327"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../..\") \n",
        "from project_utils import table_styler\n",
        "import glob\n",
        "\n",
        "# Load from file\n",
        "with open('results/all_model_points.pkl', 'rb') as f:\n",
        "    all_model_points = pickle.load(f)\n",
        "\n",
        "ward_0 = all_model_points[\"ward_0\"][[\"SRMSE\", \"RMSE\", \"MAE\"]]\n",
        "ward_0.reset_index(inplace=True)\n",
        "ward_0.rename(columns={\"index\": \"ward_0\"}, inplace=True)\n",
        "\n",
        "table_styler(ward_0, cells_format=[('font-size', '25px'), ('text-align', 'left')],\n",
        "              table_header=[('background-color', '#20808D'), ('text-align', 'left'),\n",
        "                            ('color', '#FBFAF4'), ('font-size', '30px'), ('min-width', '80px')],\n",
        "                            numeric_highlights = {\"SRMSE\": {\"condition\": \"less\", \"threshold\": 0.275, \"style\": \"font-weight: bold; background-color: None;\"}},\n",
        ")"
      ],
      "id": "7716d4fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ward_1 = all_model_points[\"ward_1\"][[\"SRMSE\", \"RMSE\", \"MAE\"]]\n",
        "ward_1.reset_index(inplace=True)\n",
        "ward_1.rename(columns={\"index\": \"ward_1\"}, inplace=True)\n",
        "\n",
        "table_styler(ward_1, cells_format=[('font-size', '25px'), ('text-align', 'left')],\n",
        "              table_header=[('background-color', '#20808D'), ('text-align', 'left'),\n",
        "                            ('color', '#FBFAF4'), ('font-size', '30px'), ('min-width', '80px')],\n",
        "                            numeric_highlights = {\"SRMSE\": {\"condition\": \"less\", \"threshold\": 0.34, \"style\": \"font-weight: bold; background-color: None;\"}},\n",
        ")"
      ],
      "id": "dc4ff931",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ward_2 = all_model_points[\"ward_2\"][[\"SRMSE\", \"RMSE\", \"MAE\"]]\n",
        "ward_2.reset_index(inplace=True)\n",
        "ward_2.rename(columns={\"index\": \"ward_2\"}, inplace=True)\n",
        "\n",
        "table_styler(ward_2, cells_format=[('font-size', '25px'), ('text-align', 'left')],\n",
        "              table_header=[('background-color', '#20808D'), ('text-align', 'left'),\n",
        "                            ('color', '#FBFAF4'), ('font-size', '30px'), ('min-width', '80px')],\n",
        "                            numeric_highlights = {\"SRMSE\": {\"condition\": \"less\", \"threshold\": 0.275, \"style\": \"font-weight: bold; background-color: None;\"}},\n",
        ")"
      ],
      "id": "63862f12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ward_3 = all_model_points[\"ward_3\"][[\"SRMSE\", \"RMSE\", \"MAE\"]]\n",
        "ward_3.reset_index(inplace=True)\n",
        "ward_3.rename(columns={\"index\": \"ward_3\"}, inplace=True)\n",
        "\n",
        "table_styler(ward_3, cells_format=[('font-size', '25px'), ('text-align', 'left')],\n",
        "              table_header=[('background-color', '#20808D'), ('text-align', 'left'),\n",
        "                            ('color', '#FBFAF4'), ('font-size', '30px'), ('min-width', '80px')],\n",
        "                            numeric_highlights = {\"SRMSE\": {\"condition\": \"less\", \"threshold\": 0.23, \"style\": \"font-weight: bold; background-color: None;\"}},\n",
        ")"
      ],
      "id": "5cdb91fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ward_4 = all_model_points[\"ward_4\"][[\"SRMSE\", \"RMSE\", \"MAE\"]]\n",
        "ward_4.reset_index(inplace=True)\n",
        "ward_4.rename(columns={\"index\": \"ward_4\"}, inplace=True)\n",
        "\n",
        "table_styler(ward_4, cells_format=[('font-size', '25px'), ('text-align', 'left')],\n",
        "              table_header=[('background-color', '#20808D'), ('text-align', 'left'),\n",
        "                            ('color', '#FBFAF4'), ('font-size', '30px'), ('min-width', '80px')],\n",
        "                            numeric_highlights = {\"SRMSE\": {\"condition\": \"less\", \"threshold\": 0.3, \"style\": \"font-weight: bold; background-color: None;\"}},\n",
        ")"
      ],
      "id": "a384fd88",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ward_5 = all_model_points[\"ward_5\"][[\"SRMSE\", \"RMSE\", \"MAE\"]]\n",
        "ward_5.reset_index(inplace=True)\n",
        "ward_5.rename(columns={\"index\": \"ward_5\"}, inplace=True)\n",
        "\n",
        "table_styler(ward_5, cells_format=[('font-size', '25px'), ('text-align', 'left')],\n",
        "              table_header=[('background-color', '#20808D'), ('text-align', 'left'),\n",
        "                            ('color', '#FBFAF4'), ('font-size', '30px'), ('min-width', '80px')],\n",
        "                            numeric_highlights = {\"SRMSE\": {\"condition\": \"less\", \"threshold\": 0.332, \"style\": \"font-weight: bold; background-color: None;\"}},\n",
        ")"
      ],
      "id": "73ca9005",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "## Forecast Distributions (Quantile scores)"
      ],
      "id": "324bc5d0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "# Load from file\n",
        "with open('results/avg_of_qswards.pkl', 'rb') as f:\n",
        "    qs = pickle.load(f)\n",
        "\n",
        "\n",
        "# qs_table = qs.reset_index().rename(columns={\"index\": \"Model\"})\n",
        "\n",
        "plt.figure(figsize=(24, 11))\n",
        "markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', 'X', 'd']\n",
        "my_qs = np.arange(0.05, 1.00, 0.05)\n",
        "for i, model in enumerate(qs.index.tolist()):\n",
        "    # if model != \"LR\":\n",
        "        plt.plot(my_qs, qs.loc[model], label=model, linewidth=1.4, alpha=1, marker=markers[i % len(markers)], markersize=5)\n",
        "    # plt.plot(my_qs, avg_of_wards.loc[model], label=model, linewidth=3)\n",
        "    # plt.plot(my_qs, avg_ward.loc[model], label=model)\n",
        "plt.xlabel(\"Quantiles\", fontdict={'fontsize': 16})\n",
        "plt.ylabel(\"Pinball Loss\", fontdict={'fontsize': 16})\n",
        "plt.title(\"Pinball Loss across different quantiles (Average across all wards)\", fontsize=24)\n",
        "plt.xticks(my_qs, [f\"{q:.2f}\" for q in my_qs], fontdict={'fontsize': 16}, rotation=90)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.legend(\n",
        "    loc='upper center', \n",
        "    bbox_to_anchor=(0.5, 0.1),  # (x, y) position, y<0 puts below the axis\n",
        "    ncol=len(qs.index),      # one column per model (horizontal)\n",
        "    frameon=True,  # no box around the legend\n",
        "    fontsize=20  # legend font size\n",
        ")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "id": "c719eca5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forecast Distributions (Quantile scores)"
      ],
      "id": "bcbf9f6b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "plt.figure(figsize=(24, 11))\n",
        "markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', 'X', 'd']\n",
        "my_qs = np.arange(0.05, 1.00, 0.05)\n",
        "for i, model in enumerate(qs.index.tolist()):\n",
        "    # if model != \"LR\":\n",
        "        plt.plot(my_qs, qs.loc[model], label=model, linewidth=1.4, alpha=1, marker=markers[i % len(markers)], markersize=5)\n",
        "    # plt.plot(my_qs, avg_of_wards.loc[model], label=model, linewidth=3)\n",
        "    # plt.plot(my_qs, avg_ward.loc[model], label=model)\n",
        "plt.xlabel(\"Quantiles\", fontdict={'fontsize': 16})\n",
        "plt.ylabel(\"Pinball Loss\", fontdict={'fontsize': 16})\n",
        "plt.title(\"Pinball Loss across different quantiles (Average across all wards)\", fontsize=24)\n",
        "plt.xticks(my_qs, [f\"{q:.2f}\" for q in my_qs], fontdict={'fontsize': 16}, rotation=90)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.legend(\n",
        "    loc='upper center', \n",
        "    bbox_to_anchor=(0.5, 0.1),  # (x, y) position, y<0 puts below the axis\n",
        "    ncol=len(qs.index),      # one column per model (horizontal)\n",
        "    frameon=False,  # no box around the legend\n",
        "    fontsize=20  # legend font size\n",
        ")\n",
        "plt.grid()\n",
        "# Highlight area between 0.45 and 0.85\n",
        "plt.axvspan(0.45, 0.85, color='green', alpha=0.08)\n",
        "plt.show()"
      ],
      "id": "12984eb0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forecast distributions (Quantile scores for 45% to 85%)"
      ],
      "id": "fab1dac2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "plt.figure(figsize=(22, 11))\n",
        "markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', 'X', 'd']\n",
        "qs_viz = qs.loc[:, \"q0.45\":\"q0.85\"]\n",
        "cut_qs = np.arange(0.45, 0.9, 0.05)\n",
        "for i, model in enumerate(qs_viz.index.tolist()):\n",
        "    # if model != \"LR\":\n",
        "        plt.plot(cut_qs, qs_viz.loc[model], label=model, linewidth=1.4, alpha=1, marker=markers[i % len(markers)], markersize=5)\n",
        "    # plt.plot(my_qs, avg_of_wards.loc[model], label=model, linewidth=3)\n",
        "    # plt.plot(my_qs, avg_ward.loc[model], label=model)\n",
        "plt.xlabel(\"Quantiles\", fontdict={'fontsize': 16})\n",
        "plt.ylabel(\"Pinball Loss\", fontdict={'fontsize': 16})\n",
        "plt.title(\"Pinball Loss across different quantiles (Average across all wards)\", fontsize=24)\n",
        "# plt.xticks(cut_qs, [f\"{int(q*100)}%\" for q in cut_qs])\n",
        "plt.xticks(cut_qs, [f\"{q:.2f}\" for q in cut_qs], fontdict={'fontsize': 16}, rotation=90)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.legend(\n",
        "    loc='upper center', \n",
        "    bbox_to_anchor=(0.5, 0.1),  # (x, y) position, y<0 puts below the axis\n",
        "    ncol=len(qs_viz.index),      # one column per model (horizontal)\n",
        "    frameon=True,  # no box around the legend\n",
        "    fontsize=20  # legend font size\n",
        ")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "id": "23f7dbcd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Model Explainability\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"70%\"}"
      ],
      "id": "8df86e66"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "100%",
        "fig-height": "100%"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "# Load from file\n",
        "\n",
        "import pickle\n",
        "with open('quarto_result/hm_model.pkl', 'rb') as f:\n",
        "    hm_model = pickle.load(f)\n",
        "state_probs_train = hm_model.predict_proba()\n",
        "state_predict_train = hm_model.predict_states()\n",
        "\n",
        "df_ = data_prep(ward='ward_0')\n",
        "\n",
        "train_size = len(occup_train_clean)\n",
        "fit_df = df_[:train_size] # fit on train data only to avoid data leakage\n",
        "test_df = df_[train_size:train_size+30]\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "my_series = fit_df[-2074:][\"ward_0\"]\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(22, 10))\n",
        "\n",
        "# Plot the time series\n",
        "ax1.plot(my_series.index[-60:], my_series.values[-60:], label=\"Occupancy\", color=\"C0\")\n",
        "# ax1.plot(fit_df.index, fit_df[\"forecast\"], label=\"Occupancy\", color=\"C3\", linestyle='--')\n",
        "# ax1.plot(vis_data.index, vis_data[\"Discharges\"], label=\"Discharges\", color=\"C1\")\n",
        "ax1.set_ylabel(\"Occupancy level\")\n",
        "ax1.set_xlabel(\"Date\")\n",
        "ax1.legend(loc=\"upper left\")\n",
        "\n",
        "# Overlay probabilities as stacked area\n",
        "ax2 = ax1.twinx()\n",
        "ax2.stackplot(my_series.index[-60:],\n",
        "              state_probs_train[0][-60:], \n",
        "              state_probs_train[1][-60:],\n",
        "              labels=['State 0', 'State 1', 'State 2'], \n",
        "              alpha=0.25, colors=['C2','C3', 'C4'])\n",
        "\n",
        "ax2.set_ylabel(\"State Probability\", fontsize=20)\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.legend(loc=\"upper right\", fontsize=20)\n",
        "\n",
        "plt.title(\"Occupancy with Hidden State Probabilities\", fontsize=24)\n",
        "plt.setp(ax1.get_xticklabels(), rotation=30, ha=\"right\")  # <--- Add this line\n",
        "plt.show()"
      ],
      "id": "ff24dce2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::: {.column width=\"30%\"}\n",
        "\n",
        "### Transition probabilities"
      ],
      "id": "0bd961f4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from IPython.display import display, Markdown\n",
        "tp = pd.DataFrame(hm_model.A).round(3)\n",
        "tp.index = [\"State 0\", \"State 1\"]\n",
        "tp.columns = [\"State 0\", \"State 1\"]\n",
        "tp.reset_index(inplace=True)\n",
        "tp.rename(columns={\"index\": \"T. Matrix\"}, inplace=True)\n",
        "\n",
        "# Assume 'df' is your DataFrame\n",
        "\n",
        "display(Markdown(tp.to_markdown(index=False)))"
      ],
      "id": "5e015d18",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "### Standard Deviations"
      ],
      "id": "6bfbb4d5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "stds = pd.DataFrame(hm_model.stds).round(3).T\n",
        "# change row name to standard deviation\n",
        "stds = stds.rename(index={0: \"sd\"})\n",
        "stds.columns = [\"State 0\", \"State 1\"]\n",
        "stds.reset_index(inplace=True)\n",
        "stds.rename(columns={\"index\": \"Std. Dev.\"}, inplace=True)\n",
        "display(Markdown(stds.to_markdown(index=False)))"
      ],
      "id": "d04b504f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::::    \n",
        "\n",
        "\n",
        "# Key findings\n",
        "\n",
        "## Learning outcomes from the current experiments\n",
        "\n",
        "- Although AR-RHMM may not always outperform all benchmark models in point forecasting metrics, there is a significant improvement in probabilistic forecasting, particularly in quantile loss across 40% and 85% quantiles.\n",
        "\n",
        "- The model is more reliable for probabilistic forecasting since it accounts for regime changes and provides a distribution of possible future values, making it suitable for decision-making in hospital resource management.\n",
        "\n",
        "- The model performs consistently well interms of `probabilistic forecasting` since the model is probabilistic in nature and captures regime changes.\n",
        "\n",
        "# Next steps\n",
        "\n",
        "## Next steps\n",
        "\n",
        "- Extend the AR-HMM to a multivariate framework (VAR-HMM) to jointly model multiple wards, capturing interdependencies and shared patterns in bed occupancy across different wards.\n",
        "\n",
        "- Develop an optimization framework that utilizes the probabilistic forecasts from the AR-HMM and VAR-HMM models to optimize bed allocation and resource management.\n",
        "\n",
        "- Test the models in real-world scenarios, collaborating with hospital staff to refine the models based on practical feedback and operational needs.\n",
        "\n",
        "\n",
        "\n",
        "# Any questions or thoughts? 💬\n",
        "\n",
        "![](images/follow_us.png){fig-align=\"center\"}"
      ],
      "id": "a36cd6c3"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "quarto-python",
      "language": "python",
      "display_name": "Python (quarto-python)",
      "path": "/Users/aslanm/Library/Jupyter/kernels/quarto-python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}