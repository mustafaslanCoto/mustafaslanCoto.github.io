{"entries":[],"headings":["outline","finite-markov-decision-processes","finite-markov-decision-processes-1","the-agentenvironment-interface","the-agentenvironment-interface-1","the-agentenvironment-interface-2","returns-and-episodes","returns-and-episodes-1","policies-and-value-functions","policies-and-value-functions-continued","policies-and-value-functions-continued-1","bellman-equations","policies-and-value-functions-continued-2","bellman-equations-1","optimal-policies-and-optimal-value-functions","dynamic-programming","dynamic-programming-1","policy-evalulation-prediction","policy-evalulation-prediction-1","policy-improvement","policy-iteration","policy-iteration-1","value-iteration","value-iteration-1","generalized-policy-iteration","monte-carlo-methods","monte-carlo-methods-1","monte-carlo-prediction","monte-carlo-control","monte-carlo-control-without-exploring-starts","monte-carlo-control-without-exploring-starts-1","off-policy-prediction-via-importance-sampling","off-policy-prediction-via-importance-sampling-1","off-policy-prediction-via-importance-sampling-continued","incremental-implementation","incremental-implementation-continued","off-policy-monte-carlo-control","temporal-difference-learning","temporal-difference-learning-1","td-prediction","td-prediction-1","sarsa-on-policy-td-control","q-learning-off-policy-td-control","expected-sarsa","td-prediction-2","maximization-bias-and-double-learning","maximization-bias-and-double-learning-1","n-step-bootstrapping","n-step-bootstrapping-1","n-step-td-prediction","n-step-td-prediction-1","n-step-sarsa","n-step-sarsa-1","n-step-off-policy-learning","n-step-off-policy-learning-1","on-policy-prediction-with-approximation","linear-methods","tutorials-on-rl"]}