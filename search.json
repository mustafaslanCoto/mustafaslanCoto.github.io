[
  {
    "objectID": "CV/LICENSE.html",
    "href": "CV/LICENSE.html",
    "title": "GNU General Public License",
    "section": "",
    "text": "Version 3, 29 June 2007\nCopyright © 2007 Free Software Foundation, Inc. &lt;http://fsf.org/&gt;\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\n\nThe GNU General Public License is a free, copyleft license for software and other kinds of works.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program–to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nTo protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others.\nFor example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights.\nDevelopers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it.\nFor the developers’ and authors’ protection, the GPL clearly explains that there is no warranty for this free software. For both users’ and authors’ sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions.\nSome devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users’ freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users.\nFinally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free.\nThe precise terms and conditions for copying, distribution and modification follow.\n\n\n\n\n\n“This License” refers to version 3 of the GNU General Public License.\n“Copyright” also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n“The Program” refers to any copyrightable work licensed under this License. Each licensee is addressed as “you”. “Licensees” and “recipients” may be individuals or organizations.\nTo “modify” a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a “modified version” of the earlier work or a work “based on” the earlier work.\nA “covered work” means either the unmodified Program or a work based on the Program.\nTo “propagate” a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo “convey” a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays “Appropriate Legal Notices” to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\n\n\nThe “source code” for a work means the preferred form of the work for making modifications to it. “Object code” means any non-source form of a work.\nA “Standard Interface” means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe “System Libraries” of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A “Major Component”, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe “Corresponding Source” for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work’s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\n\n\nAll rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\n\n\nNo covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work’s users, your or third parties’ legal rights to forbid circumvention of technological measures.\n\n\n\nYou may convey verbatim copies of the Program’s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\n\n\nYou may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\n\na) The work must carry prominent notices stating that you modified it, and giving a relevant date.\nb) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to “keep intact all notices”.\nc) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it.\nd) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so.\n\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an “aggregate” if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation’s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\n\n\nYou may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\n\na) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange.\nb) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge.\nc) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b.\nd) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements.\ne) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d.\n\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA “User Product” is either (1) a “consumer product”, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, “normally used” refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n“Installation Information” for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\n\n\n“Additional permissions” are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\n\na) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or\nb) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or\nc) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or\nd) Limiting the use for publicity purposes of names of licensors or authors of the material; or\ne) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or\nf) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors.\n\nAll other non-permissive additional terms are considered “further restrictions” within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\n\n\nYou may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\n\n\nYou are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\n\n\n\nEach time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\nAn “entity transaction” is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party’s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\n\n\nA “contributor” is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor’s “contributor version”.\nA contributor’s “essential patent claims” are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, “control” includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor’s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a “patent license” is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To “grant” such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. “Knowingly relying” means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient’s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is “discriminatory” if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\n\n\nIf conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\n\n\n\nNotwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such.\n\n\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License “or any later version” applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy’s public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\n\n\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n\n\nIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n\n\nIf the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.\nEND OF TERMS AND CONDITIONS\n\n\n\n\nIf you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the “copyright” line and a pointer to where the full notice is found.\n&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.\nAlso add information on how to contact you by electronic and paper mail.\nIf the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode:\n&lt;program&gt;  Copyright (C) &lt;year&gt;  &lt;name of author&gt;\nThis program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'.\nThis is free software, and you are welcome to redistribute it\nunder certain conditions; type 'show c' for details.\nThe hypothetical commands show w and show c should show the appropriate parts of the General Public License. Of course, your program’s commands might be different; for a GUI interface, you would use an “about box”.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a “copyright disclaimer” for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see &lt;http://www.gnu.org/licenses/&gt;.\nThe GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read &lt;http://www.gnu.org/philosophy/why-not-lgpl.html&gt;."
  },
  {
    "objectID": "CV/LICENSE.html#preamble",
    "href": "CV/LICENSE.html#preamble",
    "title": "GNU General Public License",
    "section": "",
    "text": "The GNU General Public License is a free, copyleft license for software and other kinds of works.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program–to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nTo protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others.\nFor example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights.\nDevelopers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it.\nFor the developers’ and authors’ protection, the GPL clearly explains that there is no warranty for this free software. For both users’ and authors’ sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions.\nSome devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users’ freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users.\nFinally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free.\nThe precise terms and conditions for copying, distribution and modification follow."
  },
  {
    "objectID": "CV/LICENSE.html#terms-and-conditions",
    "href": "CV/LICENSE.html#terms-and-conditions",
    "title": "GNU General Public License",
    "section": "",
    "text": "“This License” refers to version 3 of the GNU General Public License.\n“Copyright” also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n“The Program” refers to any copyrightable work licensed under this License. Each licensee is addressed as “you”. “Licensees” and “recipients” may be individuals or organizations.\nTo “modify” a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a “modified version” of the earlier work or a work “based on” the earlier work.\nA “covered work” means either the unmodified Program or a work based on the Program.\nTo “propagate” a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo “convey” a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays “Appropriate Legal Notices” to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\n\n\nThe “source code” for a work means the preferred form of the work for making modifications to it. “Object code” means any non-source form of a work.\nA “Standard Interface” means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe “System Libraries” of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A “Major Component”, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe “Corresponding Source” for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work’s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\n\n\nAll rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\n\n\nNo covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work’s users, your or third parties’ legal rights to forbid circumvention of technological measures.\n\n\n\nYou may convey verbatim copies of the Program’s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\n\n\nYou may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\n\na) The work must carry prominent notices stating that you modified it, and giving a relevant date.\nb) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to “keep intact all notices”.\nc) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it.\nd) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so.\n\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an “aggregate” if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation’s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\n\n\nYou may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\n\na) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange.\nb) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge.\nc) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b.\nd) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements.\ne) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d.\n\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA “User Product” is either (1) a “consumer product”, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, “normally used” refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n“Installation Information” for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\n\n\n“Additional permissions” are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\n\na) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or\nb) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or\nc) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or\nd) Limiting the use for publicity purposes of names of licensors or authors of the material; or\ne) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or\nf) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors.\n\nAll other non-permissive additional terms are considered “further restrictions” within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\n\n\nYou may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\n\n\nYou are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\n\n\n\nEach time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\nAn “entity transaction” is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party’s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\n\n\nA “contributor” is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor’s “contributor version”.\nA contributor’s “essential patent claims” are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, “control” includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor’s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a “patent license” is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To “grant” such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. “Knowingly relying” means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient’s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is “discriminatory” if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\n\n\nIf conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\n\n\n\nNotwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such.\n\n\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License “or any later version” applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy’s public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\n\n\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n\n\nIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n\n\nIf the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.\nEND OF TERMS AND CONDITIONS"
  },
  {
    "objectID": "CV/LICENSE.html#how-to-apply-these-terms-to-your-new-programs",
    "href": "CV/LICENSE.html#how-to-apply-these-terms-to-your-new-programs",
    "title": "GNU General Public License",
    "section": "",
    "text": "If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the “copyright” line and a pointer to where the full notice is found.\n&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.\nAlso add information on how to contact you by electronic and paper mail.\nIf the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode:\n&lt;program&gt;  Copyright (C) &lt;year&gt;  &lt;name of author&gt;\nThis program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'.\nThis is free software, and you are welcome to redistribute it\nunder certain conditions; type 'show c' for details.\nThe hypothetical commands show w and show c should show the appropriate parts of the General Public License. Of course, your program’s commands might be different; for a GUI interface, you would use an “about box”.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a “copyright disclaimer” for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see &lt;http://www.gnu.org/licenses/&gt;.\nThe GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read &lt;http://www.gnu.org/philosophy/why-not-lgpl.html&gt;."
  },
  {
    "objectID": "bins/CV/CV.html",
    "href": "bins/CV/CV.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "PhD in Probabilistic Machine Learning in Healthcare Management\nCardiff University, Cardiff, UK Oct 2024 — Present\nPhD in Probabilistic Machine Learning in Healthcare Management\n\n\nRecipient of WGSSS‑ESRC Studentship Award\nProject focused on enhancing discharge care coordination in healthcare and social care using a probabilistic data-driven modelling approach\n\nSupervisors: Prof Bahman Rostami‑Tabar, Dr. Jeremy Dixon\n\n\nMSc in Financial Mathematics\nMiddle East Technical University, Ankara, Turkey Oct 2017 — Aug 2021\n\nDissertation: Effects of Exchange Rate Volatility and Firm-Specific Features on the Rates of Returns of the Manufacturing Firms Listed in Borsa İstanbul: A CAPM Approach\n\nStatistical&Machine learning techniques used: Markov Switching GARCH Models, ARIMA, Panel Data Econometrics, Principal Component Analysis\n\n\nBSc in Business Administration\nMiddle East Technical University, Turkey Oct 2011 — Aug 2015"
  },
  {
    "objectID": "bins/CV/CV.html#education",
    "href": "bins/CV/CV.html#education",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "PhD in Probabilistic Machine Learning in Healthcare Management\nCardiff University, Cardiff, UK Oct 2024 — Present\nPhD in Probabilistic Machine Learning in Healthcare Management\n\n\nRecipient of WGSSS‑ESRC Studentship Award\nProject focused on enhancing discharge care coordination in healthcare and social care using a probabilistic data-driven modelling approach\n\nSupervisors: Prof Bahman Rostami‑Tabar, Dr. Jeremy Dixon\n\n\nMSc in Financial Mathematics\nMiddle East Technical University, Ankara, Turkey Oct 2017 — Aug 2021\n\nDissertation: Effects of Exchange Rate Volatility and Firm-Specific Features on the Rates of Returns of the Manufacturing Firms Listed in Borsa İstanbul: A CAPM Approach\n\nStatistical&Machine learning techniques used: Markov Switching GARCH Models, ARIMA, Panel Data Econometrics, Principal Component Analysis\n\n\nBSc in Business Administration\nMiddle East Technical University, Turkey Oct 2011 — Aug 2015"
  },
  {
    "objectID": "bins/CV/CV.html#work-experince",
    "href": "bins/CV/CV.html#work-experince",
    "title": "Curriculum Vitae",
    "section": "WORK EXPERINCE",
    "text": "WORK EXPERINCE\nReporting and Data Analytics Executive\nAKBANK (a leading bank in Turkey), Istanbul Jun 2022 — Sep 2024\n\nApplied time-series forecasting and machine learning techniques, including ARIMA, Bayesian Time Series, Prophet, ANN, LSTM, Random Forest, LightGBM and XGBoost, to historical data for making long-horizon forecasts of daily customer call volume. The best model achieved over 94% accuracy (1-MAPE) in forecasting all days of the next month.\nImplemented machine learning models (e.g., XGBoost, LightGBM, CatBoost) to predict customer behavior with an over recall rate of 70% and an accuracy rate of 85%, enabling fewer customer complaints and increase sales by 14%.\nAnalyzing large amounts of data to identify trends and find patterns, signals and hidden stories within customer calls data.\nApplied machine learning techniques (Z-score, IsolationForest) to detect anomalies.\nApplied unsupervised machine learning techniques (DBSCAN, Gaussian mixture, K-means) to cluster customers.\nHyperparameter tuning for machine learning models using Hyperopt, Optuna and KerasTuner.\n\nResearch Associate\nThe Economic Policy Research Foundation of Turkey (Think Tank), Ankara Jan 2022 — Apr 2024\n\nDetermined areas of research to increase knowledge in the particular field.\nUtilizing inferential statistics such as hypothesis testing (e.g., t-test, ANOVA test, population proportion test), confidence intervals, correlation analysis and regression analysis to make inferences and draw conclusions about data.\nDeveloped statistical models (regression analysis, panel data modeling) for regional development projects to contribute to data-driven decisions.\n\nSenior Process Development Analyst\nETI GIDA Inc (a major FMCG player in Turkey), Eskisehir Jun 2019 — Jan 2022\n\nInteracted with internal customers to understand business needs and translate into requirements and project scope.\nAssessed the impact of current business processes on users and stakeholders and evaluated potential areas for improvement.\nMaintained strong working knowledge of ERP (SAP), CRM and business intelligence tools and operational features.\n\nInternal Auditor Ankara, Turkey\nTurk Telekom Inc. (the telecom giant of Turkey), Ankara Nov 2015 — Jun 2019\n\nPerformed strategic planning, execution and finalization of audits using data analytics and critical thinking skills.\nInvestigated discrepancies discovered during the auditing process.\nRecommended new methods to improve internal controls and operating efficiency."
  },
  {
    "objectID": "bins/CV/CV.html#research-interests",
    "href": "bins/CV/CV.html#research-interests",
    "title": "Curriculum Vitae",
    "section": "RESEARCH INTERESTS",
    "text": "RESEARCH INTERESTS\n\nData-driven decision making\nMachine learning\nReinforcement learning and stocastic optimization in decision making\nTime series forecasting\nProbabilistic forecasting\nConformal prediction for time series forecasting"
  },
  {
    "objectID": "bins/CV/CV.html#skills-expertise",
    "href": "bins/CV/CV.html#skills-expertise",
    "title": "Curriculum Vitae",
    "section": "SKILLS & EXPERTISE",
    "text": "SKILLS & EXPERTISE\nExpertise: Mathematical and Statistical Modeling, Machine Learning, Time Series Analysis and Forecasting, Stocastic Optimization and Reinforecement Learning, Statistical and Explanatory Data Analysis\nProgramming: Python, SQL\nReporting: Quarto, Advanced Excel, QlikView, SAS\nLanguages English (Fluent), Kurdish (Native), Turkish (Native)"
  },
  {
    "objectID": "bins/CV/CV.html#professional-development",
    "href": "bins/CV/CV.html#professional-development",
    "title": "Curriculum Vitae",
    "section": "PROFESSIONAL DEVELOPMENT",
    "text": "PROFESSIONAL DEVELOPMENT\n\nCertification:\n\nData Science: Machine Learning, HarvardX (edX)\n\nBooks (some of my go-to books):\n\nForecasting: Principles and Practice (Hyndman et al, 2021)\nIntroduction to Statistical Learning with Applications in R (Tibshirani et al, 2019)\nThe Elements of Statistical Learning (Tibshirani et al, 2008)\nProbabilistic Machine Learning: An Introduction (Murphy. 2022)\nReinforcement Learning: An Introduction (Barto et al, 2018)\nReinforcement Learning and Stochastic Optimization (Powell, 2022)\nTime Series Forecasting in Python (Peixeiro, 2022)\nA Student’s Guide to Bayesian Statistics (Lambert, 2018)\nDive into deep learning (Zhang, 2022)"
  },
  {
    "objectID": "bins/CV/CV.html#references",
    "href": "bins/CV/CV.html#references",
    "title": "Curriculum Vitae",
    "section": "References",
    "text": "References\nAvailable upon request"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Researcher & Data Scientist",
    "section": "",
    "text": "I am currently pursuing a PhD at Cardiff Business School, where I specialise in developing AI&machine learning models for healthcare management. My research involves developing and refining advanced statistical&machine learning techniques to analyse complex datasets, aiming to enhance decision-making processes in real-world applications. In addition to my PhD work, I manage the Learning Labs workshop series, providing training on machine learning, statistics, reinforcement learning and forecasting methodologies using Python. I am also responsible for organizing seminars for the Data Lab for Social Good, delivering sessions on AI, machine learning, forecasting, mathematical modeling, and optimization techniques."
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "MUSTAFA ASLAN",
    "section": "",
    "text": "Forecasting Mental Health Hospital Bed Occupancy: A Regime-Switching AutoRegressive Hidden Markov Model Approach\n\n\n5th Welsh Postgraduate Research Cluster Workshop in Economy, Enterprise and Productivity\n\n\n\n\n\nSep 16, 2025\n\n\nMustafa Aslan\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html",
    "href": "talks/wgsss/occupancy_nb.html",
    "title": "Add date features to data",
    "section": "",
    "text": "# import libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport pandas as pd\n# allow max rows to be displayed\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 50)\n# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.model_selection import TimeSeriesSplit, KFold\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, root_mean_squared_error\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.pipeline import Pipeline       \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, AdaBoostRegressor, GradientBoostingRegressor\nimport numpy as np\nfrom cubist import Cubist\nfrom hyperopt import fmin, tpe, hp, Trials, STATUS_OK, space_eval\nfrom hyperopt.pyll import scope\npd.set_option('display.max_rows', 150)\nimport pickle # for saving and loading models\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.seasonal import seasonal_decompose, MSTL\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom peshbeen.models import (ml_forecaster, ml_bidirect_forecaster, VARModel, MsHmmRegression, MsHmmVar)\nfrom peshbeen.model_selection import (cross_validate,  mv_cross_validate,\n                                      cv_tune, mv_cv_tune, prob_param_forecasts,\n                                      tune_ets, tune_sarima, ParametricTimeSeriesSplit,\n                                      forward_feature_selection, backward_feature_selection,\n                                      mv_forward_feature_selection, mv_backward_feature_selection,\n                                      hmm_forward_feature_selection, hmm_backward_feature_selection,\n                                      hmm_mv_forward_feature_selection, hmm_mv_backward_feature_selection,\n                                      hmm_cross_validate, hmm_mv_cross_validate, cv_lag_tune, \n                                      cv_hmm_lag_tune)\nfrom peshbeen.statplots import (plot_ccf, plot_PACF_ACF)\nfrom peshbeen.stattools import (unit_root_test, cross_autocorrelation,\n                                lr_trend_model, forecast_trend, pacf_strength, ccf_strength)\nfrom peshbeen.transformations import (fourier_terms, rolling_quantile,\n                        rolling_mean, rolling_std, expanding_mean, expanding_std,\n                        expanding_quantile, expanding_ets, box_cox_transform,\n                        back_box_cox_transform,undiff_ts, seasonal_diff, invert_seasonal_diff,\n                        nzInterval, zeroCumulative, kfold_target_encoder, target_encoder_for_test)\nfrom peshbeen.metrics import (MAPE, MASE, MSE, MAE, RMSE, SMAPE, CFE, CFE_ABS, WMAPE, SRMSE, RMSSE, SMAE)\nfrom peshbeen.prob_forecast import (ml_conformalizer, hmm_conformalizer, ets_conformalizer, bidirect_ts_conformalizer, var_conformalizer, bag_boost_aggr_conformalizer,\n                                       bidirect_aggr_conformalizer, ets_aggr_conformalizer, s_arima_aggr_conformalizer,\n                                       var_aggr_conformalizer, hmm_var_conformalizer)\nfrom sktime.transformations.series.boxcox import BoxCoxTransformer\nsns.set_context(\"talk\")\nfrom statsmodels.tsa.seasonal import STL, MSTL\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n# import excel file\noccupancy_hrs = pd.read_excel('hr_occupancy_df.xlsx')\noccupancy_hrs.set_index('Date', inplace=True)\nward_cols_ = occupancy_hrs.columns[:-1].tolist()\noccupancy_hrs.rename(columns={col: \"ward_\" + str(i) for i, col in enumerate(ward_cols_)}, inplace=True)\nward_cols_ = occupancy_hrs.columns[:-1].tolist()\n\n# assign NA any values below 24\noccupancy_hrs[occupancy_hrs &lt;= 24] = np.nan\n\n# ## Add month name and week day\n# occupancy_hrs['month'] = occupancy_hrs.index.month_name()\n# occupancy_hrs['week_day'] = occupancy_hrs.index.day_name()\nward_cols_\n\n['ward_0', 'ward_1', 'ward_2', 'ward_3', 'ward_4', 'ward_5', 'ward_6']\n# import refer_source and followup excel file\nrefer_source = pd.read_excel('refer_source.xlsx')\nfollowup = pd.read_excel('followup_service.xlsx')\ncmht_refer_df = refer_source[[\"refer_date\", \"CMHT\"]]\ncmht_refer_df.rename(columns = {\"refer_date\": \"Date\", \"CMHT\": \"cmht_refer\"}, inplace=True)\ncmht_refer_df.set_index(\"Date\", inplace=True)\n\ncmht_followup_df = followup[[\"disc_date\", \"CMHRS\"]]\ncmht_followup_df.rename(columns = {\"disc_date\": \"Date\", \"CMHRS\": \"cmht_followup\"}, inplace=True)\ncmht_followup_df.set_index(\"Date\", inplace=True)\n# convert hourly occupancy to daily occupancy so we can approximate dailly number of patients\n\noccupancy_days = occupancy_hrs.copy()\n\noccupancy_days[[\"ward_0\", \"ward_1\", \"ward_2\", \"ward_3\", \"ward_4\", \"ward_5\", \"ward_6\"]] = (\n    (occupancy_hrs[[\"ward_0\", \"ward_1\", \"ward_2\", \"ward_3\", \"ward_4\", \"ward_5\", \"ward_6\"]] / 24)\n    .round(0)\n)\n\n\noccupancy_days[\"total\"] = occupancy_days[[\"ward_0\", \"ward_1\", \"ward_2\", \"ward_3\", \"ward_4\", \"ward_5\", \"ward_6\"]].sum(axis=1)\noccupancy_days = occupancy_days.merge(cmht_refer_df, left_index=True, right_index=True, how='left', validate='one_to_one')\noccupancy_days = occupancy_days.merge(cmht_followup_df, left_index=True, right_index=True, how='left', validate='one_to_one')\n\n\nholidays = pd.read_excel('holidays_eng.xlsx', index_col='Date')\nholidays.index = pd.to_datetime(holidays.index)\n# fig, ax = plt.subplots(2, 3, figsize=(15, 8))\n# # for i, ward in enumerate(ward_cols_):\n# ward_num = 0\n# for j in range(2):\n#     for k in range(3):\n#         ax[j, k].plot(occupancy_days.index, occupancy_days[f\"ward_{ward_num}\"], label=f\"Ward {ward_num}\")\n#         ax[j, k].set_title(f\"Ward {ward_num} Occupancy\")\n#         # ax[j, k].legend()\n#         ward_num += 1\n\n# plt.tight_layout()\n# plt.show()\noccupancy_days[\"day_of_week\"] = occupancy_days.index.weekday\noccupancy_days[\"day_of_month\"] = occupancy_days.index.day\noccupancy_days[\"month\"] = occupancy_days.index.month\noccupancy_days[\"week_of_year\"] = occupancy_days.index.isocalendar().week\noccupancy_days[\"is_weekend\"] = occupancy_days[\"day_of_week\"].isin([5, 6])\noccupancy_days[\"year\"] = occupancy_days.index.year\noccupancy = occupancy_days.merge(holidays, left_index=True, right_index=True, how='left')\noccupancy[\"holiday\"].fillna(\"not_holiday\", inplace=True)\noccupancy[\"is_holiday\"] = occupancy[\"holiday\"].apply(lambda x: \"holiday\" if x != \"not_holiday\" else\"not_holiday\")\n\nfourier_term = fourier_terms(start_end_index=[occupancy_days.index.min(), occupancy_days.index.max()], period=365.25, num_terms=2)\noccupancy = occupancy.merge(fourier_term, left_index=True, right_index=True, how='left')\n# train-test split for both dataset and test size is 30 days for both datasets occupancy_hrs and occupancy_fourier\ntrain_size = len(occupancy) - 360\noccup_train = occupancy[:train_size]\noccup_test = occupancy[train_size:]"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#analysis-on-the-effect-of-weekdays-month-days-etc-on-occupancy-data",
    "href": "talks/wgsss/occupancy_nb.html#analysis-on-the-effect-of-weekdays-month-days-etc-on-occupancy-data",
    "title": "Add date features to data",
    "section": "Analysis on the effect of weekdays, month days etc on occupancy data",
    "text": "Analysis on the effect of weekdays, month days etc on occupancy data"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#weekly-occupancy-for-each-ward",
    "href": "talks/wgsss/occupancy_nb.html#weekly-occupancy-for-each-ward",
    "title": "Add date features to data",
    "section": "Weekly Occupancy for Each Ward",
    "text": "Weekly Occupancy for Each Ward\n\nwards = [f\"ward_{i}\" for i in range(7)]  # e.g. 7 wards (0..6)\nn_plots = len(wards)\n\n# create a grid with enough slots (2 rows, 4 cols = 8 slots here)\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\naxes = axes.flatten()  # flatten to index linearly\n\nfor ax, ward_col in zip(axes[:n_plots], wards):\n        # Plot each week's data (less transparent)\n        ward = occup_train.pivot_table(index=\"day_of_week\", columns=\"week_of_year\", values=ward_col, aggfunc='mean').reset_index()\n        mean = occup_train.groupby('day_of_week')[ward_col].mean()\n        for week in ward.columns[1:]:\n            ax.plot(ward.index, ward[week], alpha=0.2)  # Less transparent\n\n        # Plot average of all weeks (more transparent, thicker line)\n        ax.plot(mean.index, mean, color='black', linewidth=2, alpha=0.9, label='Weekly Average')  # More transparent\n        ax.set_xlabel(\"Day of Week\", fontsize=9)\n        ax.tick_params(axis='both', labelsize=8)  # &lt;-- Correct way\n        ax.set_ylabel(\"Occupancy\", fontsize=9)\n        ax.set_title(f\"Weekly Occupancy for ward {ward_col}\", fontsize=9)\n        ax.legend(fontsize = 9)  # &lt;-- Only 'Weekly Average' will show up\n\n# remove any unused axes (if grid has more slots than n_plots)\nfor ax in axes[n_plots:]:\n    fig.delaxes(ax)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#occupancy-by-day-of-month-for-each-month",
    "href": "talks/wgsss/occupancy_nb.html#occupancy-by-day-of-month-for-each-month",
    "title": "Add date features to data",
    "section": "Occupancy by day of month for each month",
    "text": "Occupancy by day of month for each month\n\nyear_df = occup_train[occup_train[\"year\"].isin([2019, 2020, 2021, 2022, 2023])]\n\n# choose how many wards to plot\nwards = [f\"ward_{i}\" for i in range(7)]  # e.g. 7 wards (0..6)\nn_plots = len(wards)\n\n# create a grid with enough slots (2 rows, 4 cols = 8 slots here)\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\naxes = axes.flatten()  # flatten to index linearly\n\nfor ax, ward_col in zip(axes[:n_plots], wards):\n        # Plot each week's data (less transparent)\n        ward = occup_train.pivot_table(index=\"day_of_month\", columns=\"month\", values=ward_col, aggfunc='mean').reset_index()\n        mean = occup_train.groupby('day_of_month')[ward_col].mean()\n        for week in ward.columns[1:]:\n            ax.plot(ward.index, ward[week], alpha=0.2)  # Less transparent\n\n        # Plot average of all weeks (more transparent, thicker line)\n        ax.plot(mean.index, mean, color='black', linewidth=2, alpha=0.9, label='Monthly Average')  # More transparent\n        ax.set_xlabel(\"Day of Month\", fontsize=9)\n        ax.tick_params(axis='both', labelsize=9)  # &lt;-- Correct way\n        ax.tick_params(axis='x', labelsize=10)  # x-axis tick labels size\n        ax.tick_params(axis='y', labelsize=12) # y-axis tick labels size\n        ax.set_ylabel(\"Occupancy\", fontsize=10)\n        ax.set_title(f\"Monthly Occupancy for {ward_col}\", fontsize=10)\n        ax.legend(loc='upper right', fontsize=10)\n\n# remove any unused axes (if grid has more slots than n_plots)\nfor ax in axes[n_plots:]:\n    fig.delaxes(ax)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nyear_df = occup_train[occup_train[\"year\"].isin([2019, 2020, 2021, 2022, 2023])]\n\n# choose how many wards to plot\nwards = [f\"ward_{i}\" for i in range(7)]  # e.g. 7 wards (0..6)\nn_plots = len(wards)\n\n# create a grid with enough slots (2 rows, 4 cols = 8 slots here)\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\naxes = axes.flatten()  # flatten to index linearly\n\nfor ax, ward_col in zip(axes[:n_plots], wards):\n        # Plot each week's data (less transparent)\n        ward = occup_train.pivot_table(index=\"month\", columns=\"day_of_month\", values=ward_col, aggfunc='mean').reset_index()\n        mean = occup_train.groupby('month')[ward_col].mean()\n        for week in ward.columns[1:]:\n            ax.plot(ward.index, ward[week], alpha=0.2)  # Less transparent\n\n        # Plot average of all weeks (more transparent, thicker line)\n        ax.plot(mean.index, mean, color='black', linewidth=2, alpha=0.9, label='Monthly Average')  # More transparent\n        ax.set_xlabel(\"Day of Month\", fontsize=9)\n        ax.tick_params(axis='both', labelsize=9)  # &lt;-- Correct way\n        ax.tick_params(axis='x', labelsize=10)  # x-axis tick labels size\n        ax.tick_params(axis='y', labelsize=12) # y-axis tick labels size\n        ax.set_ylabel(\"Occupancy\", fontsize=10)\n        ax.set_title(f\"Monthly Occupancy for {ward_col}\", fontsize=10)\n        ax.legend(loc='upper right', fontsize=10)\n\n# remove any unused axes (if grid has more slots than n_plots)\nfor ax in axes[n_plots:]:\n    fig.delaxes(ax)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#occupancy-by-each-month-for-eaxh-year",
    "href": "talks/wgsss/occupancy_nb.html#occupancy-by-each-month-for-eaxh-year",
    "title": "Add date features to data",
    "section": "Occupancy by each month for eaxh year",
    "text": "Occupancy by each month for eaxh year\n\nyear_df = occup_train[occup_train[\"year\"].isin([2019, 2020, 2021, 2022, 2023])]\n\n# choose how many wards to plot\nwards = [f\"ward_{i}\" for i in range(7)]  # e.g. 7 wards (0..6)\nn_plots = len(wards)\n\n# create a grid with enough slots (2 rows, 4 cols = 8 slots here)\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\naxes = axes.flatten()  # flatten to index linearly\n\nfor ax, ward_col in zip(axes[:n_plots], wards):\n    ward = year_df.pivot_table(index=\"month\", columns=\"year\",\n                               values=ward_col, aggfunc='mean').reset_index()\n    mean = year_df.groupby('month')[ward_col].mean()\n\n    # plot each year's monthly line (faint)\n    for year in ward.columns[1:]:\n        ax.plot(ward['month'], ward[year], alpha=0.2)\n\n    # plot yearly average (bold)\n    ax.plot(mean.index, mean, color='black', linewidth=2, alpha=0.9, label='Yearly Average')\n\n    # ticks/labels\n    ax.set_xlabel(\"Month\", fontsize=9)\n    ax.set_xticks(ward['month'])                         # positions = month labels\n    ax.set_xticklabels(ward['month'], rotation=45, fontsize=9)  # show month names\n    ax.tick_params(axis='y', labelsize=10)\n    ax.set_ylabel(\"Occupancy\", fontsize=10)\n    ax.set_title(f\"Monthly Occupancy for {ward_col}\", fontsize=10)\n    ax.legend(loc='upper right', fontsize=8, ncol=2)\n\n# remove any unused axes (if grid has more slots than n_plots)\nfor ax in axes[n_plots:]:\n    fig.delaxes(ax)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#occupancy-by-weekends",
    "href": "talks/wgsss/occupancy_nb.html#occupancy-by-weekends",
    "title": "Add date features to data",
    "section": "Occupancy by weekends",
    "text": "Occupancy by weekends\n\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\naxes = axes.flatten()             # 8 axes, indexed 0..7\n\nyear_df = occup_train[occup_train[\"year\"].isin([2019, 2020, 2021, 2022, 2023])]\n\nfor ward_num, ax in enumerate(axes[:7]):   # only use the first 7 axes\n        mean = occup_train.groupby('is_weekend')[f\"ward_{ward_num}\"].mean()\n        ax.bar(mean.index, mean, color='C0', alpha=0.9)\n        # ax[i, j].bar(mean.index, mean, color='black', linewidth=2, alpha=0.9, label='Monthly Average')  # More transparent\n        ax.set_xlabel(\"is weekend\", fontsize=9)\n        ax.set_xticks([0, 1], [\"Weekday\", \"Weekend\"], fontsize=12)  # &lt;-- this changes tick labels\n        ax.tick_params(axis='x', labelsize=10)  # x-axis tick labels size\n        ax.tick_params(axis='y', labelsize=12) # y-axis tick labels size\n        ax.set_ylabel(\"Occupancy\", fontsize=10)\n        ax.set_title(f\"Monthly Occupancy for ward {ward_num}\", fontsize=10)\n        ax.legend(loc='upper right', fontsize=10)\n\n# Optionally remove/hide any remaining axes (here there is one left)\nfor ax in axes[7:]:\n    fig.delaxes(ax)            # removes the empty subplot from figure\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\naxes = axes.flatten()             # 8 axes, indexed 0..7\n\nyear_df = occup_train[occup_train[\"year\"].isin([2019, 2020, 2021, 2022, 2023])]\n\nfor ward_num, ax in enumerate(axes[:7]):   # only use the first 7 axes\n    sns.boxplot(x='is_holiday', y=f'ward_{ward_num}', data=year_df, hue='year',\n                palette=\"Set3\", ax=ax)\n    mean = year_df.groupby('is_holiday')[f\"ward_{ward_num}\"].mean()\n    sns.scatterplot(x=mean.index, y=mean, color='black', linewidth=2, alpha=0.9,\n                    label='Yearly Average', ax=ax)\n\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"not holiday\", \"holiday\"], fontsize=12)\n    ax.tick_params(axis='both', labelsize=9)\n    ax.set_ylabel(\"Occupancy\", fontsize=10)\n    ax.set_title(f\"Monthly Occupancy for ward {ward_num}\", fontsize=10)\n    ax.legend(loc='upper right', fontsize=7, ncol=3)\n\n# Optionally remove/hide any remaining axes (here there is one left)\nfor ax in axes[7:]:\n    fig.delaxes(ax)            # removes the empty subplot from figure\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#occupancy-by-the-week-of-year",
    "href": "talks/wgsss/occupancy_nb.html#occupancy-by-the-week-of-year",
    "title": "Add date features to data",
    "section": "Occupancy by the week of year",
    "text": "Occupancy by the week of year\n\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\naxes = axes.flatten()             # 8 axes, indexed 0..7\n\nyear_df = occup_train[occup_train[\"year\"].isin([2019, 2020, 2021, 2022, 2023])]\n\nfor ward_num, ax in enumerate(axes[:7]):   # only use the first 7 axes\n        # Plot each week's data (less transparent)\n        ward = year_df.pivot_table(index=\"week_of_year\", columns=\"year\", values=f\"ward_{ward_num}\", aggfunc='mean').reset_index()\n        mean = year_df.groupby('week_of_year')[f\"ward_{ward_num}\"].mean()\n        for year in ward.columns[1:]:\n            ax.plot(ward.index, ward[year], alpha=0.2, label=f\"{year}\")  # Less transparent\n\n        # Plot average of all weeks (more transparent, thicker line)\n        ax.plot(mean.index, mean, color='black', linewidth=2, alpha=0.9, label='Average')  # More transparent\n        ax.set_xlabel(\"week\", fontsize=9)\n        ax.tick_params(axis='both', labelsize=9)  # &lt;-- Correct way\n        ax.tick_params(axis='x', labelsize=10)  # x-axis tick labels size\n        ax.tick_params(axis='y', labelsize=12) # y-axis tick labels size\n        ax.set_ylabel(\"Occupancy\", fontsize=10)\n        ax.set_title(f\"Weekly Occupancy for ward {ward_num}\", fontsize=10)\n        ax.legend(loc='upper right', fontsize=8, ncol=3)\n\n# Optionally remove/hide any remaining axes (here there is one left)\nfor ax in axes[7:]:\n    fig.delaxes(ax)            # removes the empty subplot from figure\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\naxes = axes.flatten()             # 8 axes, indexed 0..7\n\nyear_df = occup_train[occup_train[\"year\"].isin([2019, 2020, 2021, 2022, 2023])]\n\nfor ward_num, ax in enumerate(axes[:7]):   # only use the first 7 axes\n        # Plot each week's data (less transparent)\n        ward = year_df.pivot_table(index=\"holiday\", columns=\"year\", values=f\"ward_{ward_num}\", aggfunc='mean').reset_index()\n        mean = year_df.groupby('holiday')[f\"ward_{ward_num}\"].mean()\n        for year in ward.columns[1:]:\n            ax.plot(ward.index, ward[year], alpha=0.2, label=f\"{year}\")  # Less transparent\n\n        # Plot average of all weeks (more transparent, thicker line)\n        ax.plot(mean.index, mean, color='black', linewidth=2, alpha=0.9, label='Yearly Average')  # More transparent\n        ax.set_xlabel(\"week\", fontsize=9)\n        ax.tick_params(axis='both', labelsize=9)  # &lt;-- Correct way\n        ax.tick_params(axis='x', labelsize=10, rotation=45)  # x-axis tick labels size\n        ax.tick_params(axis='y', labelsize=12) # y-axis tick labels size\n        ax.set_ylabel(\"Occupancy\", fontsize=10)\n        ax.set_title(f\"Weekly Occupancy for ward {ward_num}\", fontsize=10)\n        ax.legend(loc='upper right', fontsize=7, ncol=3)\n\n# Optionally remove/hide any remaining axes (here there is one left)\nfor ax in axes[7:]:\n    fig.delaxes(ax)            # removes the empty subplot from figure\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#change-point-and-outlier-detection-methods",
    "href": "talks/wgsss/occupancy_nb.html#change-point-and-outlier-detection-methods",
    "title": "Add date features to data",
    "section": "Change point and outlier detection methods",
    "text": "Change point and outlier detection methods\n\nimport ruptures as rpt\n\n\ndef stl_data_input(series, period=7, seasonal=2999):\n    # Apply STL decomposition\n    res = STL(series.interpolate(method=\"linear\").squeeze(), # Convert dataframe to a Series\n                                                        # to avoid error in Statsmodels\n            robust=True, period=period, seasonal=seasonal).fit()\n    \n    seasonal_component = res.seasonal\n    # residual_component = res.resid\n    # De-seasonlise original data\n    df_deseasonalised = series - seasonal_component\n\n    # # Perform linear interpolation on de-seasonalised data\n    df_deseasonalised_imputed = df_deseasonalised.interpolate(method=\"linear\")\n\n    # # Randomly sample residuals (with replacement) and add back\n    # sampled_residuals = np.random.choice(residual_component, size=len(residual_component), replace=True)\n    # # Match DataFrame index\n    # sampled_residuals = pd.Series(sampled_residuals, index=series.index)\n\n    # # Add seasonal component back to get the final imputed time series\n    df_imputed = df_deseasonalised_imputed + seasonal_component\n\n    return df_imputed\n\n## first input nan values using STL\n\n\nfor ward in ward_cols_:\n    occup_train[ward] = stl_data_input(occup_train[ward], period=None, seasonal=2999)"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#ward-0---change-point-detection-and-imputation",
    "href": "talks/wgsss/occupancy_nb.html#ward-0---change-point-detection-and-imputation",
    "title": "Add date features to data",
    "section": "Ward 0 - Change point detection and imputation",
    "text": "Ward 0 - Change point detection and imputation\n\ny_00 = occup_train['ward_0'].values  # Example time series data\nalgo_rbf = rpt.Pelt(model=\"rbf\").fit(y_00)\nresult_rbf_00 = algo_rbf.predict(pen=40)\nrpt.display(y_00, result_rbf_00, figsize=(12, 4))\nplt.show()\n\n\n\n\n\n\n\n\n\nresult_rbf_00\n\n[655, 965, 1040, 1500, 2060, 2130]\n\n\n\n## replace the outlier values between 965 and 1040 by interpolation\noccup_train_clean = occup_train.copy()\noccup_train_nan = occup_train.copy()\noccup_train_nan.iloc[965-1:1040+1, occup_train_nan.columns.get_loc('ward_0')] = np.nan\noccup_train_clean['ward_0'] = stl_data_input(occup_train_nan['ward_0'], period=None, seasonal=2999)\n\n\ny_01 = occup_train_clean[\"ward_0\"].values  # Example time series data\nalgo_rbf = rpt.Pelt(model=\"rbf\").fit(y_01)\nresult_rbf_01 = algo_rbf.predict(pen=50) ## best to try 1 and 3 piecewise (till 655, 655-1490 and 1490-...)\nrpt.display(y_01, result_rbf_01, figsize=(12, 4))\nplt.show()\n\n\n\n\n\n\n\n\n\nresult_rbf_01\n# change_points_ward_0 = [655, 1490], [1490] \n\n[655, 1490, 2130]"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#ward-1---change-point-detection-and-imputation",
    "href": "talks/wgsss/occupancy_nb.html#ward-1---change-point-detection-and-imputation",
    "title": "Add date features to data",
    "section": "Ward 1 - Change point detection and imputation",
    "text": "Ward 1 - Change point detection and imputation\n\ny_10 = occup_train['ward_1'].values  # Example time series data\nalgo_rbf = rpt.Pelt(model=\"rbf\").fit(y_10)\nresult_rbf_10 = algo_rbf.predict(pen=60) # best to try 1 and 2 with after 1440 break(till 1440, and 1440-...)\nrpt.display(y_10, result_rbf_10, figsize=(12, 4))\nplt.show()\n\n\n\n\n\n\n\n\n\nresult_rbf_10\n\n[1020, 1440, 2130]\n\n\n\n# change_points[\"ward_1\"] = [1440], [1020, 1440]"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#ward-2---change-point-detection-and-imputation",
    "href": "talks/wgsss/occupancy_nb.html#ward-2---change-point-detection-and-imputation",
    "title": "Add date features to data",
    "section": "Ward 2 - Change point detection and imputation",
    "text": "Ward 2 - Change point detection and imputation\n\ny_20 = occup_train['ward_2'].values  # Example time series data\nalgo_rbf = rpt.Pelt(model=\"rbf\").fit(y_20)\nresult_rbf_20 = algo_rbf.predict(pen=50) # best to try 1 and 2 with after 1500 break(till 1500, and 1500-...)\nrpt.display(y_20, result_rbf_20, figsize=(12, 4))\nplt.show()\n\n\n\n\n\n\n\n\n\nresult_rbf_20\n\n[340, 465, 1020, 1500, 2130]"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#ward-3---change-point-detection-and-imputation",
    "href": "talks/wgsss/occupancy_nb.html#ward-3---change-point-detection-and-imputation",
    "title": "Add date features to data",
    "section": "Ward 3 - Change point detection and imputation",
    "text": "Ward 3 - Change point detection and imputation\n\ny_30 = occup_train['ward_3'].values  # Example time series data\nalgo_rbf = rpt.Pelt(model=\"rbf\").fit(y_30)\nresult_rbf_30 = algo_rbf.predict(pen=50) # best to try just 1 because the changes are subtle and the last change has less data points\nrpt.display(y_30, result_rbf_30, figsize=(12, 4))\nplt.show()\n\n\n\n\n\n\n\n\n\nresult_rbf_30\n\n[520, 885, 1855, 2130]"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#ward-4---change-point-detection-and-imputation",
    "href": "talks/wgsss/occupancy_nb.html#ward-4---change-point-detection-and-imputation",
    "title": "Add date features to data",
    "section": "Ward 4 - Change point detection and imputation",
    "text": "Ward 4 - Change point detection and imputation\n\ny_40 = occup_train['ward_4'].values  # Example time series data\nalgo_rbf = rpt.Pelt(model=\"rbf\").fit(y_40)\nresult_rbf_40 = algo_rbf.predict(pen=35)\nrpt.display(y_40, result_rbf_40, figsize=(12, 4))\nplt.show()\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[154], line 3\n      1 y_40 = occup_train['ward_4'].values  # Example time series data\n      2 algo_rbf = rpt.Pelt(model=\"rbf\").fit(y_40)\n----&gt; 3 result_rbf_40 = algo_rbf.predict(pen=35)\n      4 rpt.display(y_40, result_rbf_40, figsize=(12, 4))\n      5 plt.show()\n\nFile ~/Desktop/my_desk/phd_research/phd_project/quarto_py_env/lib/python3.13/site-packages/ruptures/detection/pelt.py:131, in Pelt.predict(self, pen)\n    123 if not sanity_check(\n    124     n_samples=self.cost.signal.shape[0],\n    125     n_bkps=0,\n    126     jump=self.jump,\n    127     min_size=self.min_size,\n    128 ):\n    129     raise BadSegmentationParameters\n--&gt; 131 partition = self._seg(pen)\n    132 bkps = sorted(e for s, e in partition.keys())\n    133 return bkps\n\nFile ~/Desktop/my_desk/phd_research/phd_project/quarto_py_env/lib/python3.13/site-packages/ruptures/detection/pelt.py:72, in Pelt._seg(self, pen)\n     70         continue\n     71     # we update with the right partition\n---&gt; 72     tmp_partition.update({(t, bkp): self.cost.error(t, bkp) + pen})\n     73     subproblems.append(tmp_partition)\n     75 # finding the optimal partition\n\nFile ~/Desktop/my_desk/phd_research/phd_project/quarto_py_env/lib/python3.13/site-packages/ruptures/costs/costrbf.py:81, in CostRbf.error(self, start, end)\n     79 sub_gram = self.gram[start:end, start:end]\n     80 val = np.diagonal(sub_gram).sum()\n---&gt; 81 val -= sub_gram.sum() / (end - start)\n     82 return val\n\nFile ~/Desktop/my_desk/phd_research/phd_project/quarto_py_env/lib/python3.13/site-packages/numpy/_core/_methods.py:52, in _sum(a, axis, dtype, out, keepdims, initial, where)\n     50 def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n     51          initial=_NoValue, where=True):\n---&gt; 52     return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n\nKeyboardInterrupt: \n\n\n\n\nresult_rbf_40\n\n[175, 705, 815, 945, 1215, 1475, 1630, 1830, 2130]\n\n\n\n## replace the outlier values between 815 and 945 by interpolation where they were also NaN values before interpolation \n# occup_train_nan = occup_train.copy()\noccup_train_nan.iloc[815-1:945+1, occup_train_nan.columns.get_loc('ward_4')] = np.nan\noccup_train_clean['ward_4'] = stl_data_input(occup_train_nan['ward_4'], period=None, seasonal=2999)\n\n\ny_41 = occup_train_clean['ward_4'].values  # Example time series data\nalgo_rbf = rpt.Pelt(model=\"rbf\").fit(y_41)\nresult_rbf_41 = algo_rbf.predict(pen=50) ## try linear trend and 2 piecewise trend for ward_4 (after 705)\nrpt.display(y_41, result_rbf_41, figsize=(12, 4))\nplt.show()\n\n\n\n\n\n\n\n\n\nresult_rbf_41\n\n[175, 705, 1255, 1470, 2130]\n\n\n\nchange_points_ward_4 = [705], [1255]"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#ward-5---change-point-detection-and-imputation",
    "href": "talks/wgsss/occupancy_nb.html#ward-5---change-point-detection-and-imputation",
    "title": "Add date features to data",
    "section": "Ward 5 - Change point detection and imputation",
    "text": "Ward 5 - Change point detection and imputation\n\ny_50 = occup_train['ward_5'].values  # Example time series data\nalgo_rbf = rpt.Pelt(model=\"rbf\").fit(y_50)\nresult_rbf_50 = algo_rbf.predict(pen=35)\nrpt.display(y_50, result_rbf_50, figsize=(12, 4))\nplt.show()\n\n\n\n\n\n\n\n\n\nresult_rbf_50\n\n[915, 970, 1225, 1395, 2130]\n\n\n\n## replace the outlier values between 1215 and 1395 by interpolation\noccup_train_nan.iloc[915-1:970+1, occup_train_nan.columns.get_loc('ward_5')] = np.nan\noccup_train_nan.iloc[1215-1:1395+1, occup_train_nan.columns.get_loc('ward_5')] = np.nan\noccup_train_clean['ward_5'] = stl_data_input(occup_train_nan['ward_5'], period=None, seasonal=2999)\n\n\ny_51 = occup_train_clean['ward_5'].values  # Example time series data\nalgo_rbf = rpt.Pelt(model=\"rbf\").fit(y_51)\nresult_rbf_51 = algo_rbf.predict(pen=50) ## try linear trend and 3 piecewise trend for ward_5 (830, 830-1425, and 1425+)\nrpt.display(y_51, result_rbf_51, figsize=(12, 4))\nplt.show()\n\n\n\n\n\n\n\n\n\nresult_rbf_51\n\n[830, 1425, 2130]\n\n\n\nchange_points_ward_5 = [830, 1425]"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#ward-6---change-point-detection-and-imputation",
    "href": "talks/wgsss/occupancy_nb.html#ward-6---change-point-detection-and-imputation",
    "title": "Add date features to data",
    "section": "Ward 6 - Change point detection and imputation",
    "text": "Ward 6 - Change point detection and imputation\n\ny_60 = occup_train['ward_6'].values  # Example time series data\nalgo_rbf = rpt.Pelt(model=\"rbf\").fit(y_60)\nresult_rbf_60 = algo_rbf.predict(pen=40) # better to just try 1 because the only change is in the begining which might reflect the nature of data\nrpt.display(y_60, result_rbf_60, figsize=(12, 4))\nplt.show()\n\n\n\n\n\n\n\n\n\n# change_points = {\"ward_0\": change_points_ward_0, \"ward_1\": change_points_ward_1, \"ward_2\": change_points_ward_2,\n#                  \"ward_4\": change_points_ward_4, \"ward_5\": change_points_ward_5}\n\n\n# # Save to file\n# with open('model_params/change_points.pkl', 'wb') as f:\n#     pickle.dump(change_points, f)\n\n# Load from file\nwith open('model_params/change_points.pkl', 'rb') as f:\n    change_points = pickle.load(f)"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#guerrero-transformation",
    "href": "talks/wgsss/occupancy_nb.html#guerrero-transformation",
    "title": "Add date features to data",
    "section": "guerrero transformation",
    "text": "guerrero transformation\n\n\n# from sktime.transformations.series.boxcox import BoxCoxTransformer\n# guerrero_lambdas = {}\n# for w in ward_cols_:\n#     # Make transformer and perform Box-Cox transform\n#     transformer = BoxCoxTransformer(\n#         method=\"guerrero\",\n#         sp=7 # sp should be set to the seasonal period\n#     )  \n\n#     transformer.fit_transform(occup_train_clean[w])\n#     guerrero_lambdas[w] = transformer.lambda_\n\n# # Save to file\n# with open('model_params/guerrero_lambdas.pkl', 'wb') as f:\n#     pickle.dump(guerrero_lambdas, f)\n\n# Load from file\nwith open('model_params/guerrero_lambdas.pkl', 'rb') as f:\n    guerrero_lambdas = pickle.load(f)\n\n\nfitted_trend0, model_tr0 = lr_trend_model(occup_train_clean[\"ward_0\"], breakpoints=[655], type='piecewise')\nfitted_trend_lr0, model_lr0 = lr_trend_model(occup_train_clean[\"ward_0\"])\n\nforecast360_w0 = forecast_trend(model=model_tr0, H=360, start=len(occup_train_clean[\"ward_0\"]), breakpoints=[655])\nforecast360_lr0 = forecast_trend(model=model_lr0, H=360, start=len(occup_train_clean[\"ward_0\"]))\n\n\nfig, ax = plt.subplots(figsize=(15, 8))\nax.plot(occup_train_clean.index, (occup_train_clean[\"ward_0\"]), label=\"Original Data\", color='blue', alpha=0.5)\nax.plot(occup_train_clean.index, fitted_trend0, label=\"Fitted Trend\", color='red', linewidth=2)\nax.plot(occup_train_clean.index, fitted_trend_lr0, label=\"Fitted Trend\", color='purple', linewidth=2)\nax.plot(occup_test.index, occup_test[\"ward_0\"], label=\"Detrended Data\", color='green', alpha=0.7)\nax.plot(occup_test.index, forecast360_w0, label=\"Forecasted Trend\", color='orange', linewidth=2)\nax.plot(occup_test.index, forecast360_lr0, label=\"Forecasted Trend\", color='pink', linewidth=2)\n\n\n\n\n\n\n\n\n\n# RMSE(occup_test[\"ward_2\"], forecast360_w2), RMSE(occup_test[\"ward_2\"], forecast360_lr2)\n\n(np.float64(3.8212542303129404), np.float64(5.129657020736392))\n\n\n\ndef cross_validate_trend(series, cv_split, test_size, metrics, type = \"linear\", break_points = None, step_size=None):\n    \"\"\"\n    Run cross-validation using time series splits.\n\n    Args:\n        model (class): Machine learning model class (e.g., CatBoostRegressor, LGBMRegressor).\n        df (pd.DataFrame): Input data.\n        cv_split (int): Number of splits in TimeSeriesSplit.\n        test_size (int): Size of test window.\n        metrics (list): List of metric functions.\n    \n    Returns:\n        pd.DataFrame: Performance metrics for CV.\n    \"\"\"\n    tscv = ParametricTimeSeriesSplit(n_splits=cv_split, test_size=test_size, step_size=step_size)\n    metrics_dict = {m.__name__: [] for m in metrics}\n    for train_index, test_index in tscv.split(series):\n        train, test = series[train_index], series[test_index]\n        fitted_trend, model_ = lr_trend_model(train, breakpoints=break_points, type= type)\n        forecast = forecast_trend(model=model_, H=test_size, start=len(train), breakpoints=break_points)\n        # Evaluate each metric\n        for m in metrics:\n            if m.__name__ in [\"MASE\", \"SMAE\", \"SRMSE\", \"RMSSE\"]:\n                eval_val = m(np.array(test), forecast, train)\n            else:\n                eval_val = m(np.array(test), forecast)\n            metrics_dict[m.__name__].append(eval_val)\n    overall_performance = [[m.__name__, np.mean(metrics_dict[m.__name__])] for m in metrics]\n    return pd.DataFrame(overall_performance).rename(columns={0: \"eval_metric\", 1: \"score\"})\n\n\ncross_validate_trend(series=occup_train_clean[\"ward_0\"], cv_split=10, test_size=30, step_size=1, metrics=[SRMSE, MAE, RMSE, MASE], type='piecewise', break_points=[655])\n\n\n\n\n\n\n\n\neval_metric\nscore\n\n\n\n\n0\nSRMSE\n0.498297\n\n\n1\nMAE\n5.501857\n\n\n2\nRMSE\n5.757344\n\n\n3\nMASE\n6.703189\n\n\n\n\n\n\n\n\ncross_validate_trend(series=occup_train_clean[\"ward_0\"], cv_split=10, test_size=30, step_size=1, metrics=[SRMSE, MAE, RMSE, MASE], type='linear', break_points=None)\n\n\n\n\n\n\n\n\neval_metric\nscore\n\n\n\n\n0\nSRMSE\n0.360618\n\n\n1\nMAE\n3.803619\n\n\n2\nRMSE\n4.166597\n\n\n3\nMASE\n4.634138\n\n\n\n\n\n\n\n\n# score_list = []\n# for b in range(500, 1900):\n#     cv = cross_validate_trend(series=occup_train_clean[\"ward_0\"], cv_split=30, test_size=30, step_size=1, metrics=[MAE, RMSE, MASE], type='piecewise', break_points=[b])\n#     score = [b]+cv[\"score\"].tolist()\n#     score_list.append(score)\n\n\nCross-validation results for trend forecasting\n\nFor Ward 0, use normal linear regression, perform better than piecewise linear regression.\nFor Ward 1, use piecewise linear regression, perform better than normal linear regression.\nFor Ward 2, use piecewise linear regression, perform better than normal linear regression.\nFor Ward 3, use normal linear regression, perform better than piecewise linear regression.\nFor Ward 4, use piecewise linear regression, perform better than normal linear regression. just 1255\nFor Ward 5, use piecewise linear regression, perform better than normal linear regression.\n\n\n# trend_lr_models = pd.DataFrame([[\"ward_0\", \"linear regression\", np.nan],\n#  [\"ward_1\", \"piecewise regression\", 1440],\n#  [\"ward_2\", \"piecewise regression\", 1500],\n#  [\"ward_3\", \"linear regression\", np.nan],\n#  [\"ward_4\", \"piecewise regression\", 1255],\n#  [\"ward_5\", \"piecewise regression\", [830, 1425]],\n#  [\"ward_6\", \"linear regression\", np.nan]]).rename(columns={0: \"ward\", 1: \"model\", 2: \"breakpoint\"})\n\n# # Save to file\n# with open('model_params/trend_lr_models.pkl', 'wb') as f:\n#     pickle.dump(trend_lr_models, f)\n\n# Load from file\nwith open('model_params/trend_lr_models.pkl', 'rb') as f:\n    trend_lr_models = pickle.load(f)\n\n\n# var_rmse = []\n# for i in range(3, 3000, 2):\n#     res = STL(occup_train_clean[\"ward_0\"], period=7, seasonal=i).fit()\n#     var_rmse.append([i, np.var(res.resid), np.sqrt((res.resid**2).mean())])\n# df_result = pd.DataFrame(var_rmse, columns=[\"seasonal\", \"var\", \"rmse\"])\n# df_result.sort_values(by=\"rmse\")\n\n\n\n# for w in ward_cols_:\n#     strength = seasonality_strength(occup_train_clean[w], period=7, seasonal=3)\n#     print(f\"seasonality_strength for {w}: {strength}\")\n# for w in ward_cols_:\n#     strength = trend_strength(occup_train_clean[w], period=7, seasonal=3)\n#     print(f\"trend_strength for {w}: {strength}\")"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#acf-test-and-pacf-test",
    "href": "talks/wgsss/occupancy_nb.html#acf-test-and-pacf-test",
    "title": "Add date features to data",
    "section": "ACF test and PACF test",
    "text": "ACF test and PACF test\n\n# Stationary check for each ward using ADF test and KPSS\nfor ward in ward_cols_:\n    print(f\"Unit root test for {ward}:\")\n    unit_root_test(occup_train_clean[ward], n_lag=26, method=\"ADF\")\n    unit_root_test(occup_train_clean[ward], n_lag=20, method=\"KPSS\")\n\nUnit root test for ward_0:\nADF p-value: 0.000796 and data is stationary at 5% significance level\nKPSS p-value: 0.010000 and data is non-stationary at 5% significance level\nUnit root test for ward_1:\nADF p-value: 0.006183 and data is stationary at 5% significance level\nKPSS p-value: 0.010000 and data is non-stationary at 5% significance level\nUnit root test for ward_2:\nADF p-value: 0.000232 and data is stationary at 5% significance level\nKPSS p-value: 0.047429 and data is non-stationary at 5% significance level\nUnit root test for ward_3:\nADF p-value: 0.000318 and data is stationary at 5% significance level\nKPSS p-value: 0.010000 and data is non-stationary at 5% significance level\nUnit root test for ward_4:\nADF p-value: 0.000184 and data is stationary at 5% significance level\nKPSS p-value: 0.010000 and data is non-stationary at 5% significance level\nUnit root test for ward_5:\nADF p-value: 0.000000 and data is stationary at 5% significance level\nKPSS p-value: 0.100000 and data is stationary at 5% significance level\nUnit root test for ward_6:\nADF p-value: 0.000000 and data is stationary at 5% significance level\nKPSS p-value: 0.010000 and data is non-stationary at 5% significance level\n\n\n\n# from statsmodels.tsa.stattools import adfuller\n# def adf_lag(y):\n#     # Run ADF test with automatic lag selection using AIC and BIC\n#     adf_aic = adfuller(y, autolag=\"AIC\")\n#     adf_bic = adfuller(y, autolag=\"BIC\")\n\n#     auto_results = pd.DataFrame({\n#         \"Criterion\": [\"AIC\", \"BIC\"],\n#         \"ADF Statistic\": [adf_aic[0], adf_bic[0]],\n#         \"p-value\": [adf_aic[1], adf_bic[1]],\n#         \"Used Lags\": [adf_aic[2], adf_bic[2]]\n#     })\n#     return auto_results\n\n\noccup_train_guerrero = occup_train_clean.copy()\n\nfor w in ward_cols_:\n    # Apply the Box-Cox transformation using the saved lambda\n    occup_train_guerrero[w] = box_cox_transform(occup_train_clean[w],\n                                                box_cox_lmda=guerrero_lambdas[w])[0]\n\n\n# Stationary check for each ward using ADF test and KPSS\nfor ward in ward_cols_:\n    print(f\"Unit root test for {ward}:\")\n    unit_root_test(occup_train_guerrero[ward], n_lag=10, method=\"ADF\")\n    unit_root_test(occup_train_guerrero[ward], n_lag=10, method=\"KPSS\")\n\nUnit root test for ward_0:\nADF p-value: 0.000027 and data is stationary at 5% significance level\nKPSS p-value: 0.010000 and data is non-stationary at 5% significance level\nUnit root test for ward_1:\nADF p-value: 0.000006 and data is stationary at 5% significance level\nKPSS p-value: 0.010000 and data is non-stationary at 5% significance level\nUnit root test for ward_2:\nADF p-value: 0.000010 and data is stationary at 5% significance level\nKPSS p-value: 0.010000 and data is non-stationary at 5% significance level\nUnit root test for ward_3:\nADF p-value: 0.000178 and data is stationary at 5% significance level\nKPSS p-value: 0.010000 and data is non-stationary at 5% significance level\nUnit root test for ward_4:\nADF p-value: 0.000270 and data is stationary at 5% significance level\nKPSS p-value: 0.010000 and data is non-stationary at 5% significance level\nUnit root test for ward_5:\nADF p-value: 0.000000 and data is stationary at 5% significance level\nKPSS p-value: 0.020208 and data is non-stationary at 5% significance level\nUnit root test for ward_6:\nADF p-value: 0.000000 and data is stationary at 5% significance level\nKPSS p-value: 0.010000 and data is non-stationary at 5% significance level\n\n\n\n# occup_train_fe = occup_train_clean.copy()\n\n# ## Since whole wards are non-stationary, first we detrended using differencing and also difference between original series-linear trends to try both\n# for ward, model in zip(ward_cols_, trend_lr_models[\"model\"]):\n#     occup_train_fe[f\"{ward}_diff\"] = occup_train_fe[ward].diff()\n#     occup_train_fe[f\"{ward}_gr_diff\"] = occup_train_guerrero[ward].diff()\n#     br = list(trend_lr_models.loc[trend_lr_models[\"ward\"]==ward, \"breakpoint\"])\n#     if ward == \"ward_5\":\n#         br = br[0]\n#     # print(f\"breakpoints for {ward}: {br}\")\n#     if model == \"linear regression\":\n#         occup_train_fe[f\"{ward}_lr_diff\"] = occup_train_fe[ward] - lr_trend_model(occup_train_fe[ward])[0]\n#         occup_train_fe[f\"{ward}_gr_lr_diff\"] = occup_train_guerrero[ward] - lr_trend_model(occup_train_guerrero[ward])[0]\n#     else:\n#         occup_train_fe[f\"{ward}_lr_diff\"] = occup_train_fe[ward] - lr_trend_model(occup_train_fe[ward], breakpoints=br,  type=\"piecewise\")[0]\n#         occup_train_fe[f\"{ward}_gr_lr_diff\"] = occup_train_guerrero[ward] - lr_trend_model(occup_train_guerrero[ward], breakpoints=br, type=\"piecewise\")[0]\n\n# ## Then since referrals from CMHTs and follow-ups to CMHTs are intermitted, we create rolling window features to see if there might be significant patterns\n# windows= [14, 21, 28, 30, 35, 42, 60, 90]\n# for window in windows:\n#     # occup_train_fe[f\"cmh_refer_w{window}_sum\"] = occup_train_fe[\"cmht_refer\"].rolling(window=window).sum()\n#     occup_train_fe[f\"cmh_refer_w{window}_mean\"] = occup_train_fe[\"cmht_refer\"].rolling(window=window).mean()\n#     occup_train_fe[f\"cmh_refer_w{window}_std\"] = occup_train_fe[\"cmht_refer\"].rolling(window=window).std()\n#     # occup_train_fe[f\"cmh_refer_w{window}_mean_diff\"] = occup_train_fe[f\"cmh_refer_w{window}_mean\"].diff(1)\n#     # occup_train_fe[f\"cmh_refer_w{window}_std_diff\"] = occup_train_fe[f\"cmh_refer_w{window}_std\"].diff(1)\n# # occup_train_fe[f\"cmh_refer_expand_mean30\"] = occup_train_fe[\"cmht_refer\"].shift(30).expanding().mean()\n# # occup_train_fe[f\"cmh_refer_expand_std30\"] = occup_train_fe[\"cmht_refer\"].shift(30).expanding().std()\n\n# for window in windows:\n#     # occup_train_fe[f\"cmh_follow_w{window}_sum\"] = occup_train_fe[\"cmht_followup\"].rolling(window=window).sum()\n#     occup_train_fe[f\"cmh_follow_w{window}_mean\"] = occup_train_fe[\"cmht_followup\"].rolling(window=window).mean()\n#     occup_train_fe[f\"cmh_follow_w{window}_std\"] = occup_train_fe[\"cmht_followup\"].rolling(window=window).std()\n#     # occup_train_fe[f\"cmh_follow_w{window}_mean_diff\"] = occup_train_fe[f\"cmh_follow_w{window}_mean\"].diff(1)\n#     # occup_train_fe[f\"cmh_follow_w{window}_std_diff\"] = occup_train_fe[f\"cmh_follow_w{window}_std\"].diff(1)\n\n# # occup_train_fe[f\"cmh_follow_expand_mean30\"] = occup_train_fe[\"cmht_followup\"].shift(30).expanding().mean()\n# # occup_train_fe[f\"cmh_follow_expand_std30\"] = occup_train_fe[\"cmht_followup\"].shift(30).expanding().std()\n\n# occup_train_fe.dropna(inplace=True)\n# occup_train_fe = occup_train_fe.drop(columns=[\"year\", \"holiday\"])\n\n\n# ## Feature Engineering for Wards\n# fe_wards = occup_train_fe.filter(regex=r'ward_.*_.*gr_', axis=1)\n# # fe_w1 = occup_train_fe.filter(like=\"ward_1\", axis=1)\n# # fe_w2 = occup_train_fe.filter(like=\"ward_2\", axis=1)\n# # fe_w3 = occup_train_fe.filter(like=\"ward_3\", axis=1)\n# # fe_w4 = occup_train_fe.filter(like=\"ward_4\", axis=1)\n# # fe_w5 = occup_train_fe.filter(like=\"ward_5\", axis=1)\n# # fe_w6 = occup_train_fe.filter(like=\"ward_6\", axis=1)\n# fe_refer = occup_train_fe.filter(like=\"cmh_refer_\", axis=1)\n# fe_follow = occup_train_fe.filter(like=\"cmh_follow_\", axis=1)\n# # fe_refer = fe_refer.drop(fe_refer.filter(like=\"_sum\").columns, axis=1)\n\n\n# ccf_strength(fe_wards[\"ward_1_gr_lr_diff\"], fe_refer[\"cmh_refer_w60_mean\"],\n#              adjusted=True, n_lags=90)[[\"lags\", \"ccf_value\"]].reset_index(drop=True)\n\n\nparam_space_s = {\"trend\": hp.choice(\"trend\", [None, \"add\", \"mul\"]),\n         \"seasonal\": hp.choice(\"seasonal\", [None, \"add\", \"mul\"]),\n                 \"seasonal_periods\":7,\n        \"damped_trend\": hp.choice(\"damped_trend\", [True, False]),\n    'smoothing_level': hp.uniform('smoothing_level', 0, 0.99),\n    'smoothing_trend': hp.uniform('smoothing_trend', 0, 0.99),\n    'smoothing_seasonal': hp.uniform('smoothing_seasonal', 0, 0.99),\n    'damping_trend': hp.uniform('damping_trend', 0, 0.99)\n}\n\nparam_space_t = {\"trend\": hp.choice(\"trend\", [None, \"add\", \"mul\"]),\n        \"damped_trend\": hp.choice(\"damped_trend\", [True, False]),\n    'smoothing_level': hp.uniform('smoothing_level', 0, 0.99),\n    'smoothing_trend': hp.uniform('smoothing_trend', 0, 0.99),\n    'damping_trend': hp.uniform('damping_trend', 0, 0.99)\n}\n\n\n# ets_season_param = {}\n# for w in ward_cols_:\n#    print(f\"Tuning ETS model for {w} for whole components\")\n#    ets_season_param[w] = tune_ets(data=occup_train_clean[w], param_space=param_space_s, cv_splits=30,\n#              horizon=30, step_size=13, eval_metric=SRMSE, eval_num=1000, verbose=False)\n   \n# # Save to file\n# with open('model_params/ets_season_param.pkl', 'wb') as f:\n#     pickle.dump(ets_season_param, f)\n\n# Load from file\nwith open('model_params/ets_season_param.pkl', 'rb') as f:\n    ets_season_param = pickle.load(f)\n\n\n# ets_trend30_param = {}\n# for w in ward_cols_:\n#    print(f\"Tuning ETS model for {w} for trend-30 components\")\n#    ets_trend30_param[w] = tune_ets(data=occup_train_clean[w], param_space=param_space_t, cv_splits=30,\n#              horizon=30, step_size=13, eval_metric=SRMSE, eval_num=1000, verbose=False)\n\n\n# # Save to file\n# with open('model_params/ets_trend30_param.pkl', 'wb') as f:\n#     pickle.dump(ets_trend30_param, f)\n\n# Load from file\nwith open('model_params/ets_trend30_param.pkl', 'rb') as f:\n    ets_trend30_param = pickle.load(f)\n\n\n\n# ets_trend45_param = {}\n# for w in ward_cols_:\n#    print(f\"Tuning ETS model for {w} for trend-45 components\")\n#    ets_trend45_param[w] = tune_ets(data=occup_train_clean[w], param_space=param_space_t, cv_splits=30,\n#              horizon=45, step_size=13, eval_metric=SRMSE, eval_num=1000, verbose=False)\n\n\n# # Save to file\n# with open('model_params/ets_trend45_param.pkl', 'wb') as f:\n#     pickle.dump(ets_trend45_param, f)\n\n# Load from file\nwith open('model_params/ets_trend45_param.pkl', 'rb') as f:\n    ets_trend45_param = pickle.load(f)\n\n\n# ets_trend60_param = {}\n# for w in ward_cols_:\n#    print(f\"Tuning ETS model for {w} for trend-60 components\")\n#    ets_trend60_param[w] = tune_ets(data=occup_train_clean[w], param_space=param_space_t, cv_splits=30,\n#              horizon=60, step_size=13, eval_metric=SRMSE, eval_num=1000, verbose=False)\n\n# # Save to file\n# with open('model_params/ets_trend60_param.pkl', 'wb') as f:\n#     pickle.dump(ets_trend60_param, f)\n\n# Load from file\nwith open('model_params/ets_trend60_param.pkl', 'rb') as f:\n    ets_trend60_param = pickle.load(f)\n\n\n# ets_trend90_param = {}\n# for w in ward_cols_:\n#    print(f\"Tuning ETS model for {w} for trend-90 components\")\n#    ets_trend90_param[w] = tune_ets(data=occup_train_clean[w], param_space=param_space_t, cv_splits=30,\n#              horizon=90, step_size=13, eval_metric=SRMSE, eval_num=1000, verbose=False)\n\n# # Save to file\n# with open('model_params/ets_trend90_param.pkl', 'wb') as f:\n#     pickle.dump(ets_trend90_param, f)\n\n# Load from file\nwith open('model_params/ets_trend90_param.pkl', 'rb') as f:\n    ets_trend90_param = pickle.load(f)\n\n\n\n\n# ets_trend180_param = {}\n# for w in ward_cols_:\n#    print(f\"Tuning ETS model for {w} for trend-180 components\")\n#    ets_trend180_param[w] = tune_ets(data=occup_train_clean[w], param_space=param_space_t, cv_splits=30,\n#              horizon=180, step_size=13, eval_metric=SRMSE, eval_num=1000, verbose=False)\n\n\n# # Save to file\n# with open('model_params/ets_trend180_param.pkl', 'wb') as f:\n#     pickle.dump(ets_trend180_param, f)\n\n# Load from file\nwith open('model_params/ets_trend180_param.pkl', 'rb') as f:\n    ets_trend180_param = pickle.load(f)\n\n\n\n\n# ets_trend120_param = {}\n# for w in ward_cols_:\n#    print(f\"Tuning ETS model for {w} for trend-120 components\")\n#    ets_trend120_param[w] = tune_ets(data=occup_train_clean[w], param_space=param_space_t, cv_splits=30,\n#              horizon=120, step_size=13, eval_metric=SRMSE, eval_num=1000, verbose=False)\n\n\n# # Save to file\n# with open('model_params/ets_trend120_param.pkl', 'wb') as f:\n#     pickle.dump(ets_trend120_param, f)\n\n# Load from file\nwith open('model_params/ets_trend120_param.pkl', 'rb') as f:\n    ets_trend120_param = pickle.load(f)\n\n# # Load from file\n# with open('model_params/all_trend_ets_params.pkl', 'rb') as f:\n#     all_trend_ets_params = pickle.load(f)\n\n# # Load from file\n# with open('model_params/all_trend_ets_params.pkl', 'rb') as f:\n#     all_trend_ets_params = pickle.load(f)\n\nall_possible_ets_params = {\n    \"trend_30\": ets_trend30_param,\n    \"trend_45\": ets_trend45_param,\n    \"trend_60\": ets_trend60_param,\n    \"trend_90\": ets_trend90_param,\n    \"trend_120\": ets_trend120_param,\n    \"trend_180\": ets_trend180_param,\n    'linear': {w: None for w in ward_cols_}\n}\n\n\nfor t in all_possible_ets_params:\n    if t == 'linear':\n        continue\n    else:\n        for w in ward_cols_:\n            if all_possible_ets_params[t][w][0] == {'damped_trend': True}:\n                lst = list(all_possible_ets_params[t][w])\n                all_possible_ets_params[t][w] = lst\n                all_possible_ets_params[t][w][0] = {'damped_trend': False}\n                all_possible_ets_params[t][w] = tuple(lst)\n                print(f\"Changed damped_trend for {w} in {t} ETS params\")\n\n\noccup_train_clean[\"day_of_week\"] = occup_train_clean.index.day_name()\noccup_train_clean[\"month\"] = occup_train_clean.index.month_name()\noccup_train_clean[\"is_holiday\"] = np.where(occup_train_clean[\"is_holiday\"] == 1, \"holiday\", \"not holiday\")\ncat_cols = [\"day_of_week\", \"month\", \"is_holiday\"]\ncat_col_f = [\"day_of_week\", \"is_holiday\"]\n\nChanged damped_trend for ward_4 in trend_45 ETS params\nChanged damped_trend for ward_3 in trend_90 ETS params\nChanged damped_trend for ward_5 in trend_90 ETS params\nChanged damped_trend for ward_5 in trend_180 ETS params\n\n\n\ndef data_prep_f(ward, fourier_k):\n    ward_train = occup_train_clean[[ward]+cat_col_f]\n    ward_test = occup_test[[ward]+cat_col_f]\n    ward_test[ward] = ward_test[ward].interpolate(method=\"linear\")\n    ward_test[\"day_of_week\"] = ward_test.index.day_name()\n    df_all = pd.concat([ward_train, ward_test], axis=0)\n    ft = fourier_terms(start_end_index=(df_all.index.min(), df_all.index.max()),\n                period=365.25, num_terms=fourier_k)\n    return df_all.merge(ft, left_index=True, right_index=True, how=\"left\")\n\ndef data_prep(ward):\n    ward_train = occup_train_clean[[ward]+cat_cols]\n    ward_test = occup_test[[ward]+cat_cols]\n    ward_test[ward] = ward_test[ward].interpolate(method=\"linear\")\n    ward_test[\"day_of_week\"] = ward_test.index.day_name()\n    ward_test[\"month\"] = ward_test.index.month_name()\n    return pd.concat([ward_train, ward_test], axis=0)\n\n\n# Load from file\nwith open('model_params/best_ward_ets_tk.pkl', 'rb') as f:\n    best_ward_ets_tk = pickle.load(f)\n# Load from file\nwith open('model_params/ml_best_ward_ets.pkl', 'rb') as f:\n    ml_best_ward_ets = pickle.load(f)\n\n\ndf_diff = pd.DataFrame()\nfor w in ward_cols_:\n    df_diff[f'{w}'] = occup_train_clean[w]\n    df_diff[f'{w}_diff'] = occup_train_clean[w].diff()\n    df_diff[f'{w}_fitted_ets_trend'] = ExponentialSmoothing(occup_train_clean[w], **best_ward_ets_tk[w][0][0]).fit(**best_ward_ets_tk[w][0][1]).fittedvalues\n    df_diff[f'{w}_detrended'] = df_diff[f'{w}'] - df_diff[f'{w}_fitted_ets_trend']\ndf_diff.dropna(inplace=True)\n\n\n# plot original differenced series in two plots in one graph\nplt.figure(figsize=(15, 10))\nplt.plot(df_diff.index[-500:], df_diff['ward_0'][-500:], label='Original Differenced Series', color='C0', alpha=0.9)\nplt.plot(df_diff.index[-500:], df_diff['ward_0_fitted_ets_trend'][-500:], label='Fitted ETS Trend', color='C1', alpha=0.9)\nplt.plot(df_diff.index[-500:], df_diff['ward_0_detrended'][-500:], label='Detrended Series', color='C2', alpha=0.9)\nplt.title('Original Differenced Series vs Detrended Series for ward_0')\nplt.legend(loc='lower right')\n\n\n\n\n\n\n\n\n\n# plot original differenced series in two plots in one graph\nfig, axs = plt.subplots(3, 1, figsize=(15, 10))\naxs[0].plot(df_diff.index, df_diff['ward_0'], label='Original Differenced Series', color='C0', alpha=0.9)\naxs[0].set_title('Original Differenced Series for ward_0')\naxs[0].legend(loc = 'lower right')\naxs[1].plot(df_diff.index, df_diff['ward_0_fitted_ets_trend'], label='Fitted ETS Trend', color='C1', alpha=0.9)\naxs[1].set_title('Fitted ETS Trend for ward_0 occupancy')\naxs[1].legend(loc = 'lower right')\naxs[2].plot(df_diff.index, df_diff['ward_0_detrended'], label='detrended series', color='C2', alpha=0.9)\naxs[2].set_title('Detrended Series for ward_0 occupancy')\naxs[2].legend(loc = 'lower right')\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(2, 1, figsize=(15, 10))\nplot_acf(df_diff['ward_0_diff'], lags=40, ax=axes[0])\nplot_pacf(df_diff['ward_0_diff'], lags=40, ax=axes[1])\naxes[0].set_title('ACF of Differenced Series for ward_0 occupancy')\naxes[1].set_title('PACF of Differenced Series for ward_0 occupancy')\nplt.tight_layout(pad=2)\nplt.show()"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#ets-results",
    "href": "talks/wgsss/occupancy_nb.html#ets-results",
    "title": "Add date features to data",
    "section": "ETS results",
    "text": "ETS results\n\n# Load from file\nwith open('model_params/ets_season_param.pkl', 'rb') as f:\n    ets_season_param = pickle.load(f)\n    \ndef ets_ward_perf(ward, step_size=13, days_in_test=None):\n    if days_in_test is not None:\n        df_ = data_prep(ward)[:-days_in_test]\n    else:\n        df_ = data_prep(ward)\n    metrics = [SRMSE, MAE, RMSE, MASE]\n    perfmance_ets = {m.__name__: [] for m in metrics}\n    tscv = ParametricTimeSeriesSplit(n_splits=30, test_size=30, step_size=step_size)\n    for train_index, test_index in tscv.split(df_):\n        train, test = df_.iloc[train_index], df_.iloc[test_index]\n        y_test = np.array(test[ward])\n        fit = ExponentialSmoothing(train[ward],\n                                   **ets_season_param[ward][0]).fit(**ets_season_param[ward][1])\n        y_forecast = fit.forecast(steps=30)\n    \n        #Evaluate using the specified metric\n        for m in metrics:\n            if m in [MASE, SMAE, SRMSE, RMSSE]:\n                eval_val = m(y_test, y_forecast, np.array(train[ward]))\n            else:\n                eval_val = m(y_test, y_forecast)\n            perfmance_ets[m.__name__].append(eval_val)\n    overall_performance = [[m.__name__, np.mean(perfmance_ets[m.__name__])] for m in metrics]\n    overall_performance = pd.DataFrame(overall_performance).rename(columns={0: \"eval_metric\", 1: \"ets\"})\n    return overall_performance\n\n# ets_pointf_perform = {}\n# for w_ in ward_cols_:\n#     ets_pointf_perform[w_] = ets_ward_perf(w_)\n\n# # Save to file\n# with open('results/ets_pointf_perform.pkl', 'wb') as f:\n#     pickle.dump(ets_pointf_perform, f)\n\n# Load from file\nwith open('results/ets_pointf_perform.pkl', 'rb') as f:\n    ets_pointf_perform = pickle.load(f)\n\n\nets_pointf_perform[\"ward_0\"]\n\n\n\n\n\n\n\n\neval_metric\nets\n\n\n\n\n0\nSRMSE\n0.309981\n\n\n1\nMAE\n3.167916\n\n\n2\nRMSE\n3.679475\n\n\n3\nMASE\n3.801991"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#lr-cv-results",
    "href": "talks/wgsss/occupancy_nb.html#lr-cv-results",
    "title": "Add date features to data",
    "section": "LR CV results",
    "text": "LR CV results\n\n\n# Load from file\nwith open('model_params/best_ward_ets_tk.pkl', 'rb') as f:\n    best_ward_ets_tk = pickle.load(f)\n# Load from file\nwith open('model_params/best_ward_ets_forecastk.pkl', 'rb') as f:\n    best_ward_ets_forecastk = pickle.load(f)\n\n# ml_best_ward_etsforecast = {col: best_ward_ets_forecastk[col][0] for col in ward_cols_}\n\n# # Save to file\n# with open('model_params/ml_best_ward_etsforecast.pkl', 'wb') as f:\n#     pickle.dump(ml_best_ward_etsforecast, f)\n# Load from file\nwith open('model_params/ml_best_ward_etsforecast.pkl', 'rb') as f:\n    ml_best_ward_etsforecast = pickle.load(f)\n\n\n# Load from file\nwith open('model_params/ward_best_forward_lags.pkl', 'rb') as f:\n    ward_best_forward_lags = pickle.load(f)\n    \ndef lr_performance(ets_param, step_size=13, test_datasize=None):\n    lr_pointf_perform = {}\n    for w in ward_cols_:\n        k = ets_param[w][1]\n        if test_datasize is not None:\n            df_ = data_prep_f(ward=w, fourier_k=k, days_in_test=test_datasize)\n        else:\n            df_ = data_prep_f(ward=w, fourier_k=k)\n        lr_model_ets = ml_forecaster(LinearRegression(), lags=ward_best_forward_lags[w], target_col=w,\n                                    cat_variables=cat_col_f, trend=\"ets\",\n                                ets_params=ets_param[w][0])\n        cv_m = cross_validate(model=lr_model_ets, df=df_, cv_split=30, test_size=30,\n                            step_size=step_size, metrics=[SRMSE, MAE, RMSE, MASE])\n        cv_m.rename(columns={\"score\": \"linearRegression\"}, inplace = True)\n        lr_pointf_perform[w] = cv_m\n    return lr_pointf_perform\n\n# lr_pointf_perform_ = lr_performance()\n\n# with open('results/lr_pointf_perform_.pkl', 'wb') as f:\n#     pickle.dump(lr_pointf_perform_, f)\n\n\n# Load from file\nwith open('results/lr_pointf_perform_.pkl', 'rb') as f:\n    lr_pointf_perform_ = pickle.load(f)\n\n\n# lr_pointf_perform_t = lr_performance(best_ward_ets_tk)\nlr_pointf_perform_ = lr_performance(best_ward_ets_forecastk)\n\n\n# # # Save to file\n# with open('results/lr_pointf_perform_.pkl', 'wb') as f:\n#     pickle.dump(lr_pointf_perform_, f)\n# # Load from file\n# with open('results/lr_pointf_perform_.pkl', 'rb') as f:\n#     lr_pointf_perform_ = pickle.load(f)"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#ml-ressults",
    "href": "talks/wgsss/occupancy_nb.html#ml-ressults",
    "title": "Add date features to data",
    "section": "ML ressults",
    "text": "ML ressults\n\n# Load from file\nwith open('model_params/ml_best_ward_ets.pkl', 'rb') as f:\n    ml_best_ward_ets = pickle.load(f)\n\n# Load from file\nwith open('model_params/_best_xgb_params.pkl', 'rb') as f:\n    _best_xgb_params = pickle.load(f)\n# Load from file\nwith open('model_params/_best_lgb_params.pkl', 'rb') as f:\n    _best_lgb_params = pickle.load(f)\nwith open('model_params/_best_rf_params.pkl', 'rb') as f:\n    _best_rf_params = pickle.load(f)\nwith open('model_params/_best_lasso_params.pkl', 'rb') as f:\n    _best_lasso_params = pickle.load(f)\n\n\ndef ml_performance(ets_param, step_size=13, test_datasize=None):\n    ml_pointf_perform = {}\n\n    for w in ward_cols_:\n        if test_datasize is not None:\n            df_ = data_prep(ward=w, days_in_test=test_datasize)\n        else:\n            df_ = data_prep(w)\n        model_param_ = _best_xgb_params[w][0]\n        model_lags_ = _best_xgb_params[w][1]\n        xgb_model_ets = ml_forecaster(XGBRegressor(**model_param_), lags=model_lags_, target_col=w,\n                                    cat_variables=cat_cols, trend=\"ets\",\n                                    ets_params=ets_param[w])\n        cv_m = cross_validate(model=xgb_model_ets, df=df_, cv_split=30, test_size=30,\n                            step_size=step_size, metrics=[SRMSE, MAE, RMSE, MASE])\n        cv_m.rename(columns={\"score\": \"XGBoost\"}, inplace = True)\n        ml_pointf_perform[w] = cv_m\n\n        model_param_ = _best_lgb_params[w][0]\n        model_lags_ = _best_lgb_params[w][1]\n        lgb_model_ets = ml_forecaster(LGBMRegressor(**model_param_, verbose=-1), lags=model_lags_, target_col=w,\n                                    cat_variables=cat_cols, trend=\"ets\",\n                                ets_params=ets_param[w])\n        cv_m = cross_validate(model=lgb_model_ets, df=df_, cv_split=30, test_size=30,\n                            step_size=step_size, metrics=[SRMSE, MAE, RMSE, MASE])\n        cv_m.rename(columns={\"score\": \"LightGBM\"}, inplace = True)\n        ml_pointf_perform[w] = ml_pointf_perform[w].merge(cv_m, left_on=\"eval_metric\", right_on=\"eval_metric\")\n\n        model_param_ = _best_rf_params[w][0]\n        model_lags_ = _best_rf_params[w][1]\n        rf_model_ets = ml_forecaster(RandomForestRegressor(**model_param_), lags=model_lags_, target_col=w,\n                                    cat_variables=cat_cols, trend=\"ets\",\n                                ets_params=ets_param[w])\n        cv_m = cross_validate(model=rf_model_ets, df=df_, cv_split=30, test_size=30,\n                            step_size=step_size, metrics=[SRMSE, MAE, RMSE, MASE])\n        cv_m.rename(columns={\"score\": \"RandomForest\"}, inplace = True)\n        ml_pointf_perform[w] = ml_pointf_perform[w].merge(cv_m, left_on=\"eval_metric\", right_on=\"eval_metric\")\n\n\n        model_param_ = _best_lasso_params[w][0]\n        model_lags_ = _best_lasso_params[w][1]\n        lasso_model_ets = ml_forecaster(Lasso(**model_param_), lags=model_lags_, target_col=w,\n                                    cat_variables=cat_cols, trend=\"ets\",\n                                ets_params=ets_param[w])\n        cv_m = cross_validate(model=lasso_model_ets, df=df_, cv_split=30, test_size=30,\n                            step_size=step_size, metrics=[SRMSE, MAE, RMSE, MASE])\n        cv_m.rename(columns={\"score\": \"Lasso\"}, inplace = True)\n        ml_pointf_perform[w] = ml_pointf_perform[w].merge(cv_m, left_on=\"eval_metric\", right_on=\"eval_metric\")\n        print(f\"Completed ML CV for {w}\")\n    return ml_pointf_perform\n\n\nml_pointf_perform_ = ml_performance(ml_best_ward_etsforecast)\n# print(\"forecast_ets done\")\n# ml_pointf_perform_f = ml_performance(ml_best_ward_ets)\n\nCompleted ML CV for ward_0\nCompleted ML CV for ward_1\nCompleted ML CV for ward_2\nCompleted ML CV for ward_3\nCompleted ML CV for ward_4\nCompleted ML CV for ward_5\nCompleted ML CV for ward_6\n\n\n\n# with open('results/ml_pointf_perform_.pkl', 'wb') as f:\n#     pickle.dump(ml_pointf_perform_, f)\n\n# Load from file\nwith open('results/ml_pointf_perform_.pkl', 'rb') as f:\n    ml_pointf_perform_ = pickle.load(f)"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#hmm-results",
    "href": "talks/wgsss/occupancy_nb.html#hmm-results",
    "title": "Add date features to data",
    "section": "HMM results",
    "text": "HMM results\n\n# Load from file\nwith open('model_params/hmm_best_forward_lags.pkl', 'rb') as f:\n    hmm_best_forward_lags = pickle.load(f)\n\n# Load from file\nwith open('model_params/best_ward_ets_tk.pkl', 'rb') as f:\n    best_ward_ets_tk = pickle.load(f)\n# Load from file\nwith open('model_params/best_ward_ets_forecastk.pkl', 'rb') as f:\n    best_ward_ets_forecastk = pickle.load(f)\n\n# Load from file\nwith open('model_params/best_hmm_ets_tks.pkl', 'rb') as f:\n    best_hmm_ets_tks = pickle.load(f)\n\ndef hmm_performance(ets_param, test_datasize=None):\n    hmm_pointf_perform = {}\n    for w in ward_cols_:\n        k = ets_param[w][1]\n        ets_paramshmm = ets_param[w][0]\n        hmm_lags = sorted(hmm_best_forward_lags[w][0][\"best_lags\"])\n        s = best_hmm_ets_tks[w][2]\n\n        if test_datasize is not None:\n            df_ = data_prep_f(ward=w, fourier_k=k, days_in_test=test_datasize)\n        else:\n            df_ = data_prep_f(ward=w, fourier_k=k)\n\n        hm_model = MsHmmRegression(n_components=s, target_col=w, cat_variables=cat_col_f, lags=hmm_lags,\n                            trend=\"ets\", ets_params=ets_paramshmm, random_state=42, n_iter=300, tol=1e-3)\n        fit_df = df_[:-360] # fit on train data only to avoid data leakage\n        hm_model.fit_em(fit_df)\n        cv_ = hmm_cross_validate(model=hm_model, df=df_, cv_split=30, test_size=30, step_size=13, n_iter=100, metrics=[SRMSE, MAE, RMSE, MASE])\n        cv_.rename(columns={\"score\": \"HMM\"}, inplace = True)\n        hmm_pointf_perform[w] = cv_\n        print(f\"Completed HMM CV for {w}\")\n    return hmm_pointf_perform\n\n\nhmm_performance_ = hmm_performance(best_ward_ets_forecastk)\nhmm_performance_past = hmm_performance(best_hmm_ets_tks)"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#all-results",
    "href": "talks/wgsss/occupancy_nb.html#all-results",
    "title": "Add date features to data",
    "section": "all results",
    "text": "all results\n\n# Load from file\nwith open('results/all_model_points.pkl', 'rb') as f:\n    all_model_points = pickle.load(f)\n\n\nimport copy\nimport statsmodels.api as sm\nfrom scipy.stats import norm, multivariate_normal\nfrom sklearn.linear_model import LinearRegression\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom scipy.special import logsumexp\n\nclass MsHmmRegression:\n    \"\"\"\n    Hidden Markov Model Regression for time series with EM parameter estimation.\n\n    Args:\n        n_components (int): Number of hidden states.\n        target_col (str): Name of the target variable.\n        lag_list (list): List of integer lags to include as features.\n        method (str): 'posterior' for soft state assignment, 'viterbi' for hard paths.\n        startprob_prior (float): Prior for initial state probabilities.\n        transmat_prior (float): Prior for transition matrix.\n        ets_params (tuple, optional): A tuple (model_params, fit_params) for exponential smoothing. Ex.g. ({'trend': 'add', 'seasonal': 'add'}, {'damped_trend': True}). If trend is \"ets\", this will be used.\n        change_points (list or None): List of change points for piecewise linear regression to handle trend\n        add_constant (bool): Whether to add constant to regressors.\n        difference (int or None): Order of differencing to apply to target.\n        trend (str or None): Type of trend to remove ('linear', 'ets', etc.). Default is None.\n        cat_variables (list or None): List of categorical columns.\n        n_iter (int): Maximum number of EM iterations.\n        tol (float): Convergence tolerance for EM.\n        coefficients (np.ndarray or None): Initial regression coefficients.\n        stds (np.ndarray or None): Initial state std deviations.\n        init_state (np.ndarray or None): Initial state distribution.\n        trans_matrix (np.ndarray or None): Initial transition matrix.\n        random_state (int or None): Random seed.\n        verbose (bool): Print progress if True.\n    \"\"\"\n\n    def __init__(self, n_components, target_col, lags, method=\"posterior\",\n                 startprob_prior=1e3, transmat_prior=1e5, add_constant=True,\n                 difference=None, trend=None, ets_params = None, change_points=None,\n                 cat_variables=None, lag_transform=None, n_iter=100, tol=1e-6,\n                 coefficients=None, stds=None, init_state=None, trans_matrix=None,\n                 box_cox=False, lamda=None, box_cox_biasadj=False, season_diff=None,\n                 ridge=1e-5, var_floor=1e-5, \n                 random_state=None, verbose=False):\n        self.N = n_components\n        self.target_col = target_col\n        self.diff = difference\n        self.cons = add_constant\n        self.cat_variables = cat_variables\n        # lags must be a list of integers or an integer\n        if not isinstance(lags, (int, list)):\n            raise ValueError(\"Lags must be an integer or a list of integers.\")\n\n        self.lags = [i for i in range(1, lags + 1)] if isinstance(lags, int) else lags\n        self.method = method\n        self.box_cox = box_cox\n        self.lamda = lamda\n        self.biasadj = box_cox_biasadj\n        self.trend = trend\n        if ets_params is not None:\n            self.ets_model = ets_params[0]\n            self.ets_fit = ets_params[1]\n        else:\n            self.ets_model = None\n            self.ets_fit = None\n        \n        self.cps = change_points\n        self.season_diff = season_diff\n        self.lag_transform = lag_transform\n        self.iter = n_iter\n        self.tol = tol\n        self.ridge = ridge\n        self.var_floor = var_floor\n        self.verb = verbose\n\n\n        # RNG for reproducibility\n        self.rng = np.random.default_rng(random_state)\n        if init_state is None:\n            self.sp = startprob_prior\n            self.alpha_p = np.repeat(self.sp, self.N)\n            self.pi = self.rng.dirichlet(self.alpha_p) # Initial state probabilities using Dirichlet distribution\n        else:\n            self.pi = np.array(init_state)\n        if trans_matrix is None:\n            self.tm = transmat_prior\n            self.alpha_t = np.repeat(self.tm, self.N) # \n            self.A = self.rng.dirichlet(self.alpha_t, size=self.N)\n        else:\n            self.A = np.array(trans_matrix)\n\n        self.coeffs = coefficients\n        self.stds = stds\n\n\n    def data_prep(self, df):\n        \"\"\"\n        Prepare the data: encode categoricals, add lags, trend, differencing.\n        \"\"\"\n        dfc = df.copy()\n        # Categorical variable encoding\n        if self.cat_variables is not None:\n            # if self.target_encode ==True:\n            #     for col in self.cat_variables:\n            #         encode_col = col+\"_target_encoded\"\n            #         dfc[encode_col] = kfold_target_encoder(dfc, col, self.target_col, 36)\n            #     self.df_encode = dfc.copy()\n            #     dfc = dfc.drop(columns = self.cat_variables)\n            #     # If target encoding is not used, convert categories to dummies    \n\n            # else:\n            for col, cat in self.cat_var.items():\n                dfc[col] = dfc[col].astype('category')\n                # Set categories for categorical columns\n                dfc[col] = dfc[col].cat.set_categories(cat)\n            dfc = pd.get_dummies(dfc, dtype=np.float64)\n\n            for i in self.drop_categ:\n                dfc.drop(list(dfc.filter(regex=i)), axis=1, inplace=True)\n        \n        if self.target_col in dfc.columns:\n            # Apply Box–Cox transformation if specified\n            if self.box_cox:\n                self.is_zero = np.any(np.array(dfc[self.target_col]) &lt; 1) # check for zero or negative values\n                trans_data, self.lamda = box_cox_transform(x=dfc[self.target_col],\n                                                        shift=self.is_zero,\n                                                        box_cox_lmda=self.lamda)\n                dfc[self.target_col] = trans_data\n\n            if self.trend is not None:\n                self.len = len(dfc)\n                self.target_orig = dfc[self.target_col] # Store original values for later use during forecasting\n                if self.trend == \"linear\":\n                    # self.lr_model = LinearRegression().fit(np.arange(self.len).reshape(-1, 1), self.target_orig)\n                    # dfc[self.target_col] = dfc[self.target_col] - self.lr_model.predict(np.arange(self.len).reshape(-1, 1))\n                    if self.cps is not None:\n                        trend, self.lr_model = lr_trend_model(self.target_orig, breakpoints=self.cps, type='piecewise')\n                    else:\n                        trend, self.lr_model = lr_trend_model(self.target_orig)\n                    dfc[self.target_col] = dfc[self.target_col] - trend\n                if self.trend == \"ets\":\n                    self.ets_model_fit = ExponentialSmoothing(self.target_orig, **self.ets_model).fit(**self.ets_fit)\n                    dfc[self.target_col] = dfc[self.target_col] - self.ets_model_fit.fittedvalues.values\n\n\n            # Apply differencing if specified\n            if self.diff is not None or self.season_diff is not None:\n                self.orig = dfc[self.target_col].tolist()\n                if self.diff is not None:\n                    dfc[self.target_col] = np.diff(dfc[self.target_col], n=self.diff,\n                                                prepend=np.repeat(np.nan, self.diff))\n                if self.season_diff is not None:\n                    self.orig_d = dfc[self.target_col].tolist()\n                    dfc[self.target_col] = seasonal_diff(dfc[self.target_col], self.season_diff)\n\n            # Create lag features based on lags parameter\n            if self.lags is not None:\n                for lag in self.lags:\n                    dfc[f\"{self.target_col}_lag_{lag}\"] = dfc[self.target_col].shift(lag)\n            # Create additional lag transformations if specified\n            if self.lag_transform is not None:\n                for func in self.lag_transform:\n                    if isinstance(func, (expanding_std, expanding_mean)):\n                        dfc[f\"{func.__class__.__name__}_shift_{func.shift}\"] = func(dfc[self.target_col])\n                    elif isinstance(func, expanding_quantile):\n                        dfc[f\"{func.__class__.__name__}_shift_{func.shift}_q{func.quantile}\"] = func(dfc[self.target_col])\n                    elif isinstance(func, rolling_quantile):\n                        dfc[f\"{func.__class__.__name__}_{func.window_size}_shift_{func.shift}_q{func.quantile}\"] = func(dfc[self.target_col])\n                    else:\n                        dfc[f\"{func.__class__.__name__}_{func.window_size}_shift_{func.shift}\"] = func(dfc[self.target_col])\n                        \n            self.df = dfc.dropna()\n            self.X = self.df.drop(columns=self.target_col)\n            self.y = self.df[self.target_col]\n            if self.cons:\n                self.X = sm.add_constant(self.X)\n            self.col_names = self.X.columns.tolist() if hasattr(self.X, 'columns') else [f\"x{i}\" for i in range(self.X.shape[1])]\n            self.X = np.array(self.X)\n            self.y = np.array(self.y)\n            self.T = len(self.y)\n            if self.coeffs is None or self.stds is None:\n                # Initial fit: use unweighted least squares for all states\n                coeffs = []\n                stds = []\n                for i in range(self.N):\n                    # Least squares fit\n                    coeff_i = np.linalg.lstsq(self.X, self.y, rcond=None)[0]\n                    coeffs.append(coeff_i)\n                    y_pred = self.X @ coeff_i\n                    resid = self.y - y_pred\n                    var_i = np.mean(resid ** 2)\n                    stds.append(np.sqrt(var_i))\n                self.coeffs = np.row_stack(coeffs)\n                self.stds = np.array(stds)\n\n        else:\n            return dfc.dropna()\n\n\n    def compute_coeffs(self, w_floor=1e-5):\n\n        # Update regression coefficients and stds for each state\n\n        # If posterior probabilities are shorter than the number of observations, make self.X and self.y same length visa vis make posterier same as self.X and self.y length\n        if self.posterior.shape[1] &lt; self.X.shape[0]:\n            # Truncate self.X and self.y to match the length of self.posterior[i]\n            self.X = self.X[:self.posterior.shape[1]]\n            self.y = self.y[:self.posterior.shape[1]]\n        if self.posterior.shape[1] &gt; self.X.shape[0]:\n            # Truncate self.posterior to match the length of self.X and self.y\n            self.posterior = self.posterior[:, -self.X.shape[0]:]\n\n        coeffs = []\n        stds = []\n        X = self.X\n        for s in range(self.N):\n            # Add floor so state isn’t “killed”\n            w = self.posterior[s] + w_floor\n            w /= w.sum()\n            sw = np.sqrt(w)\n            Xw = X * sw[:, None]\n            yw = self.y * sw\n            XtX = Xw.T @ Xw + self.ridge*np.eye(X.shape[1])\n            Xty = Xw.T @ yw\n            beta_s = np.linalg.lstsq(XtX, Xty, rcond=None)[0]\n            coeffs.append(beta_s)\n            resid = self.y - X @ beta_s\n            var_s = (w * resid**2).sum() / max((w.sum()-beta_s.shape[0]), 1.0)\n            stds.append(np.sqrt(max(var_s, self.var_floor)))\n        self.coeffs = np.row_stack(coeffs)\n        self.stds = np.array(stds)\n\n\n# Hidden Markov Model with Vector Autoregressive (VAR)\n\n    def _log_emissions(self):\n        # logB[s,t] = log p(y_t | state s)\n        N, T = self.N, self.T\n        logB = np.empty((N, T))\n        self.fitted = np.empty((N, T))\n        # self.compute_coeffs()\n        for s in range(N):\n            mu = self.X @ self.coeffs[s]           # (T,)\n            self.fitted[s, :] = mu\n            logB[s, :] = norm.logpdf(self.y, loc=mu, scale=self.stds[s])\n        return logB\n\n    def _e_step_log(self):\n        N, T = self.N, self.T\n        logA  = np.log(self.A + 1e-300)\n        logpi = np.log(self.pi + 1e-300)\n        logB  = self._log_emissions()\n\n        # Forward\n        log_alpha = np.empty((N, T))\n        log_alpha[:, 0] = logpi + logB[:, 0]\n        for t in range(1, T):\n            # log_alpha[:,t] = logB[:,t] + logsumexp_i( log_alpha[i,t-1] + logA[i,:] )\n            log_alpha[:, t] = logB[:, t] + logsumexp(log_alpha[:, t-1][:, None] + logA, axis=0)\n\n        # Log-likelihood\n        loglik = logsumexp(log_alpha[:, -1])\n\n        # Backward\n        log_beta = np.full((N, T), 0.0)\n        for t in range(T-2, -1, -1):\n            # log_beta[:,t] = logsumexp_j( logA + logB[:,t+1] + log_beta[:,t+1] , axis=1 )\n            log_beta[:, t] = logsumexp(logA + (logB[:, t+1] + log_beta[:, t+1])[None, :], axis=1)\n\n        # Gamma\n        log_gamma = log_alpha + log_beta - loglik\n        # normalize per time to kill rounding; columns sum to 1 after exp\n        log_gamma -= logsumexp(log_gamma, axis=0)\n        gamma = np.exp(log_gamma)\n\n        # Xi\n        log_xi = np.empty((N, N, T-1))\n        for t in range(T-1):\n            tmp = log_alpha[:, t][:, None] + logA + (logB[:, t+1] + log_beta[:, t+1])[None, :]\n            tmp -= logsumexp(tmp)      # normalize this slice\n            log_xi[:, :, t] = tmp\n        xi = np.exp(log_xi)\n\n        # print(\"logB min/max:\", np.min(logB), np.max(logB))\n        # print(\"Any NaN in logB?\", np.any(np.isnan(logB)))\n        # print(\"Any Inf in logB?\", np.any(np.isinf(logB)))\n        # sanity checks (soft)\n        assert np.allclose(gamma.sum(axis=0), 1.0, atol=1e-8)\n        assert np.allclose(xi.sum(axis=(0,1)), 1.0, atol=1e-8)\n\n        self.log_forward  = log_alpha\n        self.log_backward = log_beta\n        self.posterior    = gamma\n        self.loglik       = loglik\n        return loglik, gamma, xi\n\n\n    def _m_step(self, gamma, xi):\n        numer = xi.sum(axis=2)                         # (N,N)\n        denom = gamma[:, :-1].sum(axis=1, keepdims=True)  # (N,1)\n        A = numer / (denom + 1e-12)\n        A = np.maximum(A, 1e-12)\n        A /= A.sum(axis=1, keepdims=True)\n        self.A = A\n        self.pi = gamma[:, 0] / gamma[:, 0].sum()\n        self.compute_coeffs() \n\n\n    def EM(self):\n        loglik, gamma, xi = self._e_step_log()\n        self._m_step(gamma, xi)\n        self.LL = loglik\n        # return loglik\n        \n    def fit_em(self, df_train):\n        \"\"\"\n        Run EM iterations until convergence (log-domain version).\n        \"\"\"\n\n        # Handle categorical variable encoding\n        if self.cat_variables is not None:\n            self.cat_var = {c: sorted(df_train[c].drop_duplicates().tolist(), key=lambda x: str(x))\n                            for c in self.cat_variables}\n            self.drop_categ = [sorted(df_train[col].drop_duplicates().tolist(), key=lambda x: str(x))[0]\n                               for col in self.cat_variables]\n        self.data_prep(df_train)\n\n        prev_ll = -np.inf\n        # store intermediate log-likelihoods\n        self.log_likelihoods = []\n        for it in range(self.iter):\n            # loglik, gamma, xi = self._e_step_log()\n            # self._m_step(gamma, xi)\n            self.EM()\n            if self.verb:\n                print(f\"Iter {it}: loglik={self.LL:.4f}\")\n            if it &gt; 10:\n                if abs(self.LL - prev_ll) &lt; self.tol:\n                    if self.verb:\n                        print(\"Converged.\")\n                    break\n            self.log_likelihoods.append(self.LL)\n            prev_ll = self.LL\n        return self.LL\n\n    def fit(self, df, n_iter=1):\n        \"\"\"\n        Refit the HMM regression model on new training data (log-domain version).\n        \"\"\"\n        if n_iter &lt; 1:\n            raise ValueError(\"n_iter must be at least 1.\")\n        \n        self.data_prep(df)\n        if n_iter &gt; 1:\n            prev_ll = self.LL\n            for _ in range(n_iter):\n                self.EM()\n                if abs(self.LL - prev_ll) &lt; self.tol:\n                    break\n                else:\n                    prev_ll = self.LL\n        else:\n            self.EM()\n\n        return self.LL\n    \n    def predict_states(self):\n        return np.argmax(self.posterior, axis=0)\n    def predict_proba(self):\n        return self.posterior\n    \n    # AIC computation    \n    @property\n    def AIC(self):\n        k = self.N * self.X.shape[1] + self.N ** 2 + self.N - 1\n        return 2 * k - 2 * self.LL\n\n    @property\n    def BIC(self):\n        \n        k = self.N * self.X.shape[1] + self.N ** 2 + self.N - 1\n        \n        n = self.T  # effective number of observations\n        return -2 * self.LL + k * np.log(n)\n\n    def copy(self):\n        return copy.deepcopy(self)\n\n    def forecast(self, H, exog=None):\n        \"\"\"\n        Forecast H periods ahead using fitted HMM regression (log-domain version), with advanced post-processing.\n\n        Handles:\n        - Trend re-adjustment (linear/ETS)\n        - Seasonal differencing reversal\n        - Regular differencing reversal\n        - Box-Cox back-transform\n        - Exogenous variables\n        - Lag transformations\n        \"\"\"\n        y_list = self.y.tolist()\n        forecasts_ = []\n        N = self.N\n\n        # Prepare exogenous future regressors if provided\n        if exog is not None:\n            if self.cons:\n                if exog.shape[0] == 1:\n                    exog.insert(0, 'const', 1)\n                else:\n                    exog = sm.add_constant(exog)\n            exog = np.array(self.data_prep(exog))\n\n\n        # Init with last forward distribution (in log)\n        log_forward_last = self.log_forward[:, -1]\n        logA = np.log(self.A + 1e-300)\n\n        # Forward\n        log_alpha = np.empty((N, H))\n        log_alpha[:, 0] = logsumexp(log_forward_last[:, None] + logA, axis=0)\n        for t in range(1, H):\n            # log_alpha[:,t] = logB[:,t] + logsumexp_i( log_alpha[i,t-1] + logA[i,:] )\n            log_alpha[:, t] = logsumexp(log_alpha[:, t-1][:, None] + logA, axis=0)\n\n        log_alpha -= logsumexp(log_alpha, axis=0)\n        self.forecast_forward = np.exp(log_alpha)\n        self.state_forecasts = np.argmax(self.forecast_forward, axis=0)\n        #per state forecasts\n        self.forecast_ps = np.zeros((N, H))\n\n        # Prepare for trend adjustment\n        # This assumes you stored original target (pre-trend removal) in self.target_orig\n        if hasattr(self, 'target_orig') and self.trend is not None:\n            if self.trend == \"linear\":\n                # future_time = np.arange(len(self.target_orig), len(self.target_orig) + H).reshape(-1, 1)\n                # trend_forecast = np.array(self.lr_model.predict(future_time))\n                trend_forecast= forecast_trend(model = self.lr_model, H=H, start=self.len, breakpoints=self.cps)\n            elif self.trend == \"ets\":\n                trend_forecast = np.array(self.ets_model_fit.forecast(H))\n\n        for t in range(H):\n            if exog is not None:\n                exo_inp = exog[t].tolist()\n            else:\n                exo_inp = [1] if self.cons else []\n            lags = [y_list[-l] for l in self.lags]\n            transform_lag = []\n            if self.lag_transform is not None:\n                series_array = np.array(y_list)\n                for func in self.lag_transform:\n                    transform_lag.append(func(series_array, is_forecast=True).to_numpy()[-1])\n            inp = np.array(exo_inp + lags + transform_lag)\n\n            state_preds = np.zeros(N)\n            for j in range(N):\n                mu = np.dot(self.coeffs[j], inp)\n                state_preds[j] = mu\n            self.forecast_ps[:, t] = state_preds\n\n            # normalize to probabilities\n\n            # normalize to probabilities\n            pred_w = np.sum(self.forecast_forward[:, t] * state_preds)\n            forecasts_.append(pred_w)\n            y_list.append(pred_w)\n\n            # log_forward_last = log_f_t.copy()\n\n        forecasts = np.array(forecasts_)\n\n        if self.trend is not None:\n            forecasts += trend_forecast\n            self.forecast_ps += trend_forecast\n\n        # --- Revert seasonal differencing if applied ---\n        if self.season_diff is not None:\n            forecasts = invert_seasonal_diff(self.orig_d, forecasts, self.season_diff)\n            # Also revert seasonal differencing for per state forecasts\n            for s in range(self.N):\n                self.forecast_ps[s] = invert_seasonal_diff(self.orig_d, self.forecast_ps[s], self.season_diff)\n\n        # --- Revert regular differencing if applied ---\n        if self.diff is not None:\n            forecasts = undiff_ts(self.orig, forecasts, self.diff)\n            # Also revert differencing for per state forecasts\n            for s in range(self.N):\n                self.forecast_ps[s] = undiff_ts(self.orig, self.forecast_ps[s], self.diff)\n\n        # --- Box-Cox back-transform if applied ---\n        if self.box_cox:\n            forecasts = back_box_cox_transform(\n                y_pred=forecasts, lmda=self.lamda,\n                shift=self.is_zero, box_cox_biasadj=self.biasadj\n            )\n            for s in range(self.N):\n                self.forecast_ps[s] = back_box_cox_transform(\n                    y_pred=self.forecast_ps[s], lmda=self.lamda,\n                    shift=self.is_zero, box_cox_biasadj=self.biasadj\n                )\n\n        return forecasts\n\n\nk = best_ward_ets_forecastk['ward_0'][1]\nets_paramshmm = best_ward_ets_forecastk['ward_0'][0]\nhmm_lags = sorted(hmm_best_forward_lags['ward_0'][0][\"best_lags\"])\ns = best_hmm_ets_tks['ward_0'][2]\n\ndf_ = data_prep_f(ward='ward_0', fourier_k=k)\n\ntrain_size = len(occup_train_clean)\nfit_df = df_[:train_size] # fit on train data only to avoid data leakage\ntest_df = df_[train_size:train_size+30]\n\nhm_model = MsHmmRegression(n_components=s, target_col=\"ward_0\", cat_variables=cat_col_f, lags=hmm_lags,\n                    trend=\"ets\", ets_params=ets_paramshmm, random_state=42, var_floor=1e-1,\n                      n_iter=300, tol=1e-3)\nhm_model.fit_em(fit_df)\n\nnp.float64(-2944.4438506327806)\n\n\n\nhmm_cross_validate(model=hm_model, df=df_, cv_split=30, test_size=30, step_size=13, n_iter=100, metrics=[SRMSE, MAE, RMSE, MASE])\n\n\n\n\n\n\n\n\neval_metric\nscore\n\n\n\n\n0\nSRMSE\n0.270739\n\n\n1\nMAE\n2.757161\n\n\n2\nRMSE\n3.208337\n\n\n3\nMASE\n3.311781\n\n\n\n\n\n\n\n\nhmm_cross_validate(model=hm_model, df=df_, cv_split=30, test_size=30, step_size=13, n_iter=100, metrics=[SRMSE, MAE, RMSE, MASE])\n\n\n\n\n\n\n\n\neval_metric\nscore\n\n\n\n\n0\nSRMSE\n0.271404\n\n\n1\nMAE\n2.766366\n\n\n2\nRMSE\n3.215240\n\n\n3\nMASE\n3.323389\n\n\n\n\n\n\n\n\nhm_model.stds\n\narray([1.39380279, 0.1       ])\n\n\n\n# perf_results = {\"avg\": [], \"state_0\": [], \"state_1\": []}\n# for i in range(1, 120):\n#     dft = df_[:train_size+i] # fit on train data only to avoid data leakage\n#     test_dft = df_[train_size+i:train_size+i+30]\n#     hm_model.fit(dft, n_iter=100)\n#     avg_forecast = hm_model.forecast(30, test_dft.drop(columns=[\"ward_0\"]))\n#     s0_forecast = hm_model.forecast_ps[0]\n#     s1_forecast = hm_model.forecast_ps[1]\n#     SRMSE_avg = SRMSE(np.array(test_dft[\"ward_0\"]), avg_forecast, np.array(dft[\"ward_0\"]))\n#     SRMSE_s0 = SRMSE(np.array(test_dft[\"ward_0\"]), s0_forecast, np.array(dft[\"ward_0\"]))\n#     SRMSE_s1 = SRMSE(np.array(test_dft[\"ward_0\"]), s1_forecast, np.array(dft[\"ward_0\"]))\n#     perf_results[\"avg\"].append(SRMSE_avg)\n#     perf_results[\"state_0\"].append(SRMSE_s0)\n#     perf_results[\"state_1\"].append(SRMSE_s1)\n#     # print(f\"Completed rolling forecast and evaluation for rol {i}\")\n# pd.DataFrame(perf_results).mean()\n\n\n# fit_df[\"ward_0\"][-700:].hist(bins=15, figsize=(10, 5))\n# plt.title(\"Distribution of ward_0 occupancy - last 700 days in training data\")\n# plt.xlabel(\"Occupancy\")\n# plt.ylabel(\"Frequency\")\n# plt.show()\n\n\n# with open('quarto_result/ward_0_hmm_model.pkl', 'wb') as f:\n#     pickle.dump(ward_0_hmm_model, f)\n\n# Load from file\nwith open('quarto_result/ward_0_hmm_model.pkl', 'rb') as f:\n    ward_0_hmm_model = pickle.load(f)\n\n\ntp = pd.DataFrame(hm_model.A).round(3)\ntp.index = [f\"State {i+1}\" for i in range(tp.shape[0])]\ntp.columns = [f\"State {i+1}\" for i in range(tp.shape[1])]\ntp\n\n\n\n\n\n\n\n\nState 1\nState 2\n\n\n\n\nState 1\n0.666\n0.334\n\n\nState 2\n0.487\n0.513\n\n\n\n\n\n\n\n\ntp.reset_index()\n\n\n\n\n\n\n\n\nindex\nState 1\nState 2\n\n\n\n\n0\nState 1\n0.666\n0.334\n\n\n1\nState 2\n0.487\n0.513\n\n\n\n\n\n\n\n\nstds = pd.DataFrame(hm_model.stds).round(2).T\n# change row name to standard deviation\nstds = stds.rename(index={0: \"standard deviations\"})\nstds.columns = [\"State 0\", \"State 1\"]\n\n\nstds\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[30], line 1\n----&gt; 1 stds\n\nNameError: name 'stds' is not defined\n\n\n\n\nstate_probs_train = hm_model.predict_proba()\nstate_predict_train = hm_model.predict_states()\n\n\npd.Series(state_predict_train[-365:]).value_counts(normalize=True)\n\n0    0.578082\n1    0.421918\nName: proportion, dtype: float64\n\n\n\nstate_predict_train[-10:]\n\narray([0, 1, 0, 1, 0, 1, 1, 1, 0, 1])\n\n\n\n# Load from file\nwith open('quarto_result/ward_0_hmm_model.pkl', 'rb') as f:\n    ward_0_hmm_model = pickle.load(f)\n\n\nimport matplotlib.pyplot as plt\nmy_series = fit_df[-2074:][\"ward_0\"]\n\nfig, ax1 = plt.subplots(figsize=(14, 6))\n\n# Plot the time series\nax1.plot(my_series.index[-60:], my_series.values[-60:], label=\"Occupancy\", color=\"C0\")\n# ax1.plot(fit_df.index, fit_df[\"forecast\"], label=\"Occupancy\", color=\"C3\", linestyle='--')\n# ax1.plot(vis_data.index, vis_data[\"Discharges\"], label=\"Discharges\", color=\"C1\")\nax1.set_ylabel(\"Occupancy level\")\nax1.set_xlabel(\"Date\")\nax1.legend(loc=\"upper left\")\n\n# Overlay probabilities as stacked area\nax2 = ax1.twinx()\nax2.stackplot(my_series.index[-60:],\n              state_probs_train[0][-60:], \n              state_probs_train[1][-60:],\n              labels=['State 0', 'State 1', 'State 2'], \n              alpha=0.25, colors=['C2','C3', 'C4'])\n\nax2.set_ylabel(\"State Probability\")\nax2.set_ylim(0, 1)\nax2.legend(loc=\"upper right\")\n\nplt.title(\"Occupancy with Hidden State Probabilities\")\nplt.setp(ax1.get_xticklabels(), rotation=30, ha=\"right\")  # &lt;--- Add this line\nplt.show()\n\n\n\n\n\n\n\n\n\n# Load from file\nwith open('results/hmm_conforms.pkl', 'rb') as f:\n    hmm_conforms = pickle.load(f)\n\n\ndf_ = data_prep_f('ward_0', best_hmm_ets_tks[\"ward_0\"][1])\ndfml = data_prep('ward_0')\n\ntrain_size = len(occup_train_clean)\ntrain_df = df_[:train_size] # fit on train data only to avoid data leakage\ntest_df = df_[train_size:train_size+30]\n\ntrain_dfml = dfml[:train_size] # fit on train data only to avoid data leakage\ntest_dfml = dfml[train_size:train_size+30]\n\npi_hmm = hmm_conforms[\"ward_0\"].generate_prediction_intervals(train_df, test_df.drop(columns=[\"ward_0\"]))\n\n\nplt.figure(figsize=(10, 6))\nhmm_conforms[\"ward_0\"].dist[['h_1', 'h_5', 'h_10', 'h_15', 'h_30']].boxplot()\nplt.title(\"Forecast Distribution for ward_0 HMM\")\nplt.ylabel(\"Value\")\nplt.show()\n\n\n\n\n\n\n\n\n\nconf_cols =[hmm_conforms[\"ward_0\"].dist.columns[-i] for i in range(1, 31)]\n\n\nfrom joypy import joyplot\ndfj = hmm_conforms[\"ward_0\"].dist\nconf_cols =[hmm_conforms[\"ward_0\"].dist.columns[-i] for i in range(1, 31)]\njoyplot(\n    data=dfj, \n    column=conf_cols,  # columns to plot\n    overlap=0.6,  # overlap between plots\n    figsize=(13, 10),\n    fade = True,\n    linecolor = \"white\"\n)\nplt.yticks(fontsize=8)\nplt.xticks(fontsize=12)\nplt.title(\"Forecast Distribution for ward_0 HMM\", fontsize=20)\nplt.xlabel(\"Forecast Value\")\nplt.ylabel(\"Horizon\")\nplt.show();\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(22, 11))\npalette = sns.color_palette(\"Blues_r\", n_colors=len(hmm_conforms[\"ward_0\"].dist.columns))\nsns.swarmplot(data=hmm_conforms[\"ward_0\"].dist, color = \"C0\")\nplt.xticks(rotation=90, fontsize=12)\nplt.title(\"Forecast Distribution for ward_0 HMM\")\nplt.ylabel(\"Value\")\nplt.show()\n\n\n\n\n\n\n\n\n\ntrain_size = len(occup_train_clean)\noccup_test\nforecasts = []\nstate_probs_list = []\nstate_forecasts = []\nfit_df = df_[:train_size]\ntest_target = df_[\"ward_0\"][train_size:].to_frame()\ntest_df = df_[train_size:train_size+30].drop(columns=[\"ward_0\"])\nhm_model.fit(fit_df, n_iter=100)\ny_forecast = hm_model.forecast(30, test_df).tolist()\n\nforecasts.extend(y_forecast)\nstate_seq = hm_model.state_forecasts.tolist()\nstate_forecasts.extend(state_seq)\n\nlog_alpha = hm_model.forecast_forward\nstate_probs_list.append(log_alpha)\n\nH = 30\ntrain_size += H\n\n\nwhile train_size &lt; len(df_):\n    # Prepare fit (training) data\n    fit_df = df_[:train_size]\n    \n    # Determine test (forecast) window, handle edge case for final window\n    end_idx = min(train_size + H, len(df_))\n    test_df = df_[train_size:end_idx].drop(columns=[\"ward_0\"])\n    current_H = end_idx - train_size  # May be less than 30 at the end\n\n    # Fit and forecast\n    hm_model.fit(fit_df, n_iter=100)\n    y_forecast = hm_model.forecast(current_H, test_df).tolist()\n    forecasts.extend(y_forecast)\n\n    state_seq = hm_model.state_forecasts.tolist()\n    state_forecasts.extend(state_seq)\n\n    log_alpha = hm_model.forecast_forward\n    state_probs_list.append(log_alpha)\n\n    # Advance the training window\n    train_size += H\n\n\nstate_probs = np.concatenate(state_probs_list, axis=1) \n\n\npoint = hm_model.forecast(30, test_df.drop(columns=[\"ward_0\"]))\nhm_model.forecast_ps[0]\n\narray([12.7747301 , 12.42390154, 12.08441653, 11.58121551, 11.28040762,\n       11.91556   , 12.42665751, 11.88207655, 11.81470847, 11.49832214,\n       11.24935295, 11.02136809, 11.65642169, 12.26621221, 11.68740357,\n       11.4643657 , 11.30317984, 10.82195942, 10.74693816, 11.26028151,\n       11.95867959, 11.31297098, 11.05599206, 11.20779446, 10.7636878 ,\n       10.47003598, 11.21341562, 11.64605447, 11.04578202, 10.8698587 ])\n\n\n\nfs = hm_model.forecast_forward\n\n\nimport matplotlib.pyplot as plt\nmy_series = test_df[\"ward_0\"]\n\nfig, ax1 = plt.subplots(figsize=(14, 6))\n\n# Plot the time series\nax1.plot(test_df.index, my_series.values, label=\"Occupancy\", color=\"C0\")\nax1.plot(test_df.index, point, label=\"mean\", color=\"C1\", linestyle='--')\nax1.plot(test_df.index, hm_model.forecast_ps[0], label=\"forecast_s0\", color=\"C2\", linestyle='--')\nax1.plot(test_df.index, hm_model.forecast_ps[1], label=\"forecast_s1\", color=\"C3\", linestyle='--')\n# ax1.plot(vis_data.index, vis_data[\"Discharges\"], label=\"Discharges\", color=\"C1\")\nax1.set_ylabel(\"Occupancy\")\nax1.set_xlabel(\"Date\")\nax1.legend(loc=\"upper left\")\n\n# Overlay probabilities as stacked area\nax2 = ax1.twinx()\nax2.stackplot(my_series.index,\n              fs[0], \n              fs[1],\n              labels=['State 0', 'State 1'], \n              alpha=0.25, colors=['C4','C5'])\n\nax2.set_ylabel(\"State Probability\")\nax2.set_ylim(0, 1)\nax2.legend(loc=\"upper right\")\n\nplt.title(\"occupancy with Hidden State Probabilities\")\nplt.show()\n\n\n\n\n\n\n\n\n\ntest_target[\"forecast\"] = forecasts\n\n\ntest_target\n\n\n\n\n\n\n\n\nward_0\n\n\nDate\n\n\n\n\n\n2024-04-30\n14.0\n\n\n2024-05-01\n12.0\n\n\n2024-05-02\n11.0\n\n\n2024-05-03\n11.0\n\n\n2024-05-04\n11.0\n\n\n...\n...\n\n\n2025-04-20\n19.0\n\n\n2025-04-21\n20.0\n\n\n2025-04-22\n18.0\n\n\n2025-04-23\n18.0\n\n\n2025-04-24\n18.0\n\n\n\n\n360 rows × 1 columns\n\n\n\n\ntest_target.plot(figsize=(12, 6))\nplt.title(\"HMM Forecast vs Actual for ward_0\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Occupancy\")\nplt.legend([\"Actual\", \"HMM Forecast\"])\nplt.show()  \n\n\n\n\n\n\n\n\n\ntest_target.mean()\n\nward_0      16.041667\nforecast    15.542527\ndtype: float64"
  },
  {
    "objectID": "talks/wgsss/occupancy_nb.html#decomposition",
    "href": "talks/wgsss/occupancy_nb.html#decomposition",
    "title": "Add date features to data",
    "section": "Decomposition",
    "text": "Decomposition\n\noccup_train_clean\n\n\ndecomposition = MSTL(occup_train_clean[\"ward_1\"], periods=[7, 365]).fit()  # 7=day, 365=year\nfig, ax = plt.subplots(5, 1, figsize=(30, 20))\ndecomposition.resid['2024':].plot(title='Residuals of MSTL Decomposition for ward_0', ax=ax[0])\ndecomposition.trend['2024':].plot(title='Trend of MSTL Decomposition for ward_0', ax=ax[1])\ndecomposition.seasonal['2024':].iloc[:, 0].plot(title='Seasonal 1 of MSTL Decomposition for ward_0', ax=ax[2])\ndecomposition.seasonal['2024':].iloc[:, 1].plot(title='Seasonal 2 of MSTL Decomposition for ward_0', ax=ax[3])\ndecomposition.observed['2024':].plot(title='Observed of MSTL Decomposition for ward_0', ax=ax[4])\n# increase distance between subplots\nplt.subplots_adjust(hspace=1)\nplt.show()"
  },
  {
    "objectID": "learn/monte_carlo/mc_notebook.html",
    "href": "learn/monte_carlo/mc_notebook.html",
    "title": "Monte Carlo Integration",
    "section": "",
    "text": "Monte Carlo integration is a statistical technique that allows us to approximate complex integrals using random sampling. The basic idea is to use random points in the domain of the integrand to estimate the average value of the function, which can then be scaled by the volume of the domain to obtain an estimate of the integral.\nThe steps for performing Monte Carlo integration are as follows:\nMonte Carlo integration is particularly useful for high-dimensional integrals or when the integrand is difficult to evaluate analytically. It can also be applied to problems in physics, finance, and other fields where uncertainty and randomness play a significant role."
  },
  {
    "objectID": "learn/monte_carlo/mc_notebook.html#basic-monte-carlo-integration",
    "href": "learn/monte_carlo/mc_notebook.html#basic-monte-carlo-integration",
    "title": "Monte Carlo Integration",
    "section": "Basic Monte Carlo Integration",
    "text": "Basic Monte Carlo Integration\nSuppose we want to evaluate the integral\n\\[\nI = \\int_0^1 h(x) \\, dx\n\\]\nfor some function \\(h(x)\\).\nIf \\(h\\) is complicated there may be no known closed form expression for \\(I\\). In such cases, we can use Monte Carlo integration to obtain an approximate value for the integral.\nLet us begin by writing\n\\[\nI = \\int_a^b h(x) \\, dx = \\int_a^b w(x)f(x) \\, dx\n\\]\nwhere \\(w(x) = h(x)(b-a)\\) and \\(f(x) = 1/(b-a)\\). Notice that \\(f\\) is the probability density for a uniform random variable over \\((a, b)\\). Hence\n\\[\nI = \\int_a^b w(x)f(x) \\, dx = \\mathbb{E}_f[w(X)]\n\\]\nwhere \\(X\\) is a random variable uniformly distributed over \\((a, b)\\).\nIf we generate \\(N\\) independent samples \\(X_1, X_2, \\ldots, X_N\\) from the distribution of \\(X\\), we can approximate the integral using the sample average, which satisfies law of large numbers:\n\\[\n\\hat{I} \\approx \\frac{1}{N} \\sum_{i=1}^N w(X_i)\n\\]\nThis gives us a Monte Carlo estimate of the integral.\nWe can also compute the standard error of the estimate:\n\\[\n\\text{SE} = \\frac{1}{\\sqrt{N}} \\sigma_w\n\\]\nwhere\n\\[\n\\sigma_w = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (w(X_i) - \\hat{I})^2}\n\\]\n\nExample 1\nLet \\(h(x) = x^3\\). We want to estimate the integral\n\\[\nI = \\int_0^1 x^3 \\, dx\n\\]\nUsing the Monte Carlo method, we can approximate this integral by generating random samples from the uniform distribution over \\((0, 1)\\) which is \\((a, b)\\) in the formula.\nThe goal is the compute \\[\n\\hat{I} = \\int_0^1 x^3 \\, dx\n\\]\nAnalytical answer: \\(I = 1/4\\) (easy to compute)\nMonte Carlo estimate:\n\nSimulation steps\n\n\nGenerate \\(10,000\\) random samples \\(X_1, X_2, \\ldots, X_{10,000}\\) from the uniform distribution over \\((0, 1)\\).\nCompute for each sample \\(X_i\\) the value \\(h(X_i) = X_i^3\\).\nEstimate the integral as the average of these values: \\[\n\\hat{I} \\approx \\frac{1}{10,000} \\sum_{i=1}^{10,000} (X_i^3)\n\\]\n\nWe get \\(\\hat{I} \\approx 0.243\\).\nMonte Carlo method generates an answer that is close to the analytical solution, which is \\(0.25\\), but not exact. The accuracy of the Monte Carlo estimate improves with more samples.\n\nPython implementation\n\n# python implementation\nimport numpy as np\nnp.random.seed(42) # for reproducibility\n\n# calculate for N = 100 and increase by 50 untill 10000 to see the convergence to the true value which is 0.25\nhistory_ex1 = []\nfor N in range(100, 10001, 50):\n    X = np.random.uniform(0, 1, N)\n    H = X**3\n    I_hat = np.mean(H)\n    history_ex1.append([N, I_hat])\nhistory_ex1 = np.array(history_ex1)\n\n\n#plot history against number of samples to observe convergence\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.plot(history_ex1[:, 0], history_ex1[:, 1], label='Monte Carlo Estimate')\nplt.axhline(y=0.25, color='r', linestyle='--', label='True Value')\nplt.xlabel('Number of Samples (N)')\nplt.ylabel('Estimated Integral')\nplt.title('Monte Carlo Integration Convergence')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nExample 2\nLet\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}\n\\]\nbe the standard Normal pdf. Suppose we want to compute the cdf at some point \\(x\\):\n\\[\nI = \\int_{-\\infty}^x f(s) ds = \\Phi(x).\n\\]\nWrite\n\\[\nI = \\int h(s) f(s) ds\n\\]\nwhere\n\\[\nh(s) = \\begin{cases}\n1 & s &lt; x \\\\\n0 & s \\geq x\n\\end{cases}\n\\]\nNow we generate \\(X_1, \\ldots, X_N \\sim N(0,1)\\) and set\n\\[\n\\hat{I} = \\frac{1}{N} \\sum_i h(X_i) = \\frac{\\text{number of observations } \\leq x}{N}.\n\\]\nFor example, with \\(x = 2\\), the true answer is \\(\\Phi(2) = .9772\\) and the Monte Carlo estimate with \\(N = 10,\\!000\\) yields \\(.9751\\). Using \\(N = 100,\\!000\\) we get \\(.9771\\).\n\n# Python implementation\n\nimport numpy as np\nfrom scipy.stats import norm\n\nx = 2\n\n# True value\nI_true = norm.cdf(x)\n# calculate for N = 100 and increase by 50 untill 10000 to see the convergence to the true value which is 0.9772\nhistory_ex2 = []\nfor N in range(100, 10001, 50):\n    X = np.random.normal(0, 1, N)\n    h = (X &lt; x).astype(int)\n    I_hat = np.mean(h)\n    history_ex2.append([N, I_hat])\nhistory_ex2 = np.array(history_ex2)\n\n\n#plot history against number of samples to observe convergence\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.plot(history_ex2[:, 0], history_ex2[:, 1], label='Monte Carlo Estimate')\nplt.axhline(y=I_true, color='r', linestyle='--', label='True Value')\nplt.xlabel('Number of Samples (N)')\nplt.ylabel('Estimated Integral')\nplt.title('Monte Carlo Integration Convergence')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "learn/prob_stats/dists_index.html",
    "href": "learn/prob_stats/dists_index.html",
    "title": "Random Variables",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "learn/prob_stats/cond_index.html",
    "href": "learn/prob_stats/cond_index.html",
    "title": "Conditional Probability and Conditional Expectation",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "workshops/trainings/rl_index.html",
    "href": "workshops/trainings/rl_index.html",
    "title": "Introduction to Reinforcement Learning",
    "section": "",
    "text": "Slides  Code\nDate: May 29, 2025 10:00 AM – 13:00 PM\nEvent: Data Lab for Social Good Training Session\nLocation: Aberconway Building Meeting Room, Cardiff University, UK"
  },
  {
    "objectID": "workshops/trainings/rl_index.html#who-is-the-course-for",
    "href": "workshops/trainings/rl_index.html#who-is-the-course-for",
    "title": "Introduction to Reinforcement Learning",
    "section": "Who is the course for?",
    "text": "Who is the course for?\nThis course is designed for researchers and postgraduate students who want to apply Reinforcement Learning (RL) to real-world decision-making in interdisciplinary settings, including supply chains, healthcare, and operational contexts across diverse industries. It is especially relevant for those working on problems involving sequential decision-making under uncertainty.\nYou don’t need prior RL knowledge, but you should be comfortable with basic probability and Python programming."
  },
  {
    "objectID": "workshops/trainings/rl_index.html#learning-objectives",
    "href": "workshops/trainings/rl_index.html#learning-objectives",
    "title": "Introduction to Reinforcement Learning",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this workshop, participants will be able to:\n\nUnderstand the core ideas of RL in the context of healthcare decision-making\nModel sequential sequential decisions using Markov Decision Processes (MDPs)\nApply key RL algorithms (Monte Carlo, SARSA, Q-learning) to learn optimal policies from experience\nInterpret learned policies and value functions in a healthcare context (e.g. when to treat, when to wait)\nLink RL insights to practical policy decisions like cost control, clinical risk, or stockout mitigation"
  },
  {
    "objectID": "workshops/trainings/rl_index.html#prerequisites",
    "href": "workshops/trainings/rl_index.html#prerequisites",
    "title": "Introduction to Reinforcement Learning",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nFamiliarity with basic probability and statistics (e.g., expected value, random variables)\nSome experience with Python, particularly with lists, dictionaries, and basic control flow\nNo prior knowledge of RL or advanced machine learning is required"
  },
  {
    "objectID": "workshops/index.html",
    "href": "workshops/index.html",
    "title": "MUSTAFA ASLAN",
    "section": "",
    "text": "Introduction to Reinforcement Learning\n\n\nWorkshop\n\n\n\n\n\nMay 22, 2025\n\n\nMustafa Aslan\n\n\n\n\n\n\n\n\n\n\n\n\nBasics of Python and Introduction to Quarto in Visual Studio Code\n\n\nWorkshop\n\n\n\n\n\nMar 3, 2025\n\n\nMustafa Aslan\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "workshops/rl/notes_ides.html",
    "href": "workshops/rl/notes_ides.html",
    "title": "The difference between on-policy and off-policy",
    "section": "",
    "text": "Learns about the policy it is currently using to make decisions.\nThe same policy is used for both:\n\nActing (exploring the environment)\nLearning (updating value functions)\n\nExample: SARSA\n\nUpdates Q-values using the action that was actually taken.\nLearns the value of the \\(\\varepsilon\\)-greedy policy it follows."
  },
  {
    "objectID": "workshops/rl/notes_ides.html#example",
    "href": "workshops/rl/notes_ides.html#example",
    "title": "The difference between on-policy and off-policy",
    "section": "Example:",
    "text": "Example:\n🎯 Imagine you’re learning to play chess.\n🧑 On-policy (SARSA-like):\n\nYou’re learning by watching yourself play, including your mistakes and explorations.\nYou play games using a mix of good and random moves (e.g., trying new strategies).\nYou update your knowledge based on what you actually did in each game.\nYou get better at playing the way you currently play, gradually improving it.\n\n✅ On-policy:\nYou learn the value of the policy you’re following.\n👀 Off-policy (Q-learning-like):\n\nYou’re learning how a grandmaster would play, while still playing your own games.\nYou still explore (e.g., try risky moves), but…\nWhen updating your knowledge, you ask what the best move would have been, not the one you actually chose.\nYou get better at playing like the grandmaster, even though you’re not playing like them yet.\n\n✅ This is off-policy:\nYou learn the value of a different, better policy (often greedy), while following a more exploratory one."
  },
  {
    "objectID": "workshops/rl/notes_ides.html#notes",
    "href": "workshops/rl/notes_ides.html#notes",
    "title": "The difference between on-policy and off-policy",
    "section": "Notes",
    "text": "Notes\n\nReinforcement learning is a framework for learning how to interact with environment from experience.\nAgent takes actions to interact with environment\nThe big challange in RL Design a policy of what actions to take given a state s to maximize a future reward\nQ(s,a) tells us what is the quality of being in state s and taking action a. Then once I find myself in a state s, I just have to look across all of the actions and pick the one that gives the best quality. If I do that in future I will maximize my value."
  },
  {
    "objectID": "workshops/rl/temp/rl_python_revenue_max.html",
    "href": "workshops/rl/temp/rl_python_revenue_max.html",
    "title": "A backward dynamic programming algorithm.",
    "section": "",
    "text": "Scenario:\nAn NGO sells a single handcrafted backpack produced by under-represented artisan groups. Each month, over a 120-month horizon, they reach one of four audience-size tiers—40, 60, 80, or 100 customers—and choose a campaign label (marketing message) highlighting a different artisan community. Each label yields a unit profit per customer as follows:\n\n\n\n\n\n\n\n\nLabel\nArtisan Group Highlighted\nProfit/Customer (£)\n\n\n\n\nEcoNomad\nWomen-led cooperatives\n15\n\n\nTrailBlazer\nDisability-led artisan studios\n9\n\n\nClassicCarry\nMinority-owned craft collectives\n12\n\n\n\nTheir goal is to maximize total expected profit over the 120 months by selecting the best label each month.\n\nR = [15, 9, 12]\n\n\nimport numpy as np\n\ndef create__trans_prob(num_states, num_actions):\n    \"\"\"\n    Create a transition probability matrix for a reinforcement learning problem.\n    \n    Parameters:\n    - num_states: Number of states in the environment.\n    - num_actions: Number of actions available in the environment.\n    \n    Returns:\n    - P: Transition probability matrix of shape (num_states, num_actions, num_states) using Dirichlet.\n    \"\"\"\n    alpha = np.ones(num_states)  # Dirichlet distribution parameter\n    P = np.zeros((num_states, num_actions, num_states))\n    for s in range(num_states):\n        for a in range(num_actions):\n            P[s, a, :] = np.random.dirichlet(alpha)  # transition probabilities for each state-action pair\n    return P\n\n\n# initialize value function values\n\nimport numpy as np\n\ndef create__trans_prob(num_states, num_actions):\n    \"\"\"\n    Create a transition probability matrix for a reinforcement learning problem.\n    \n    Parameters:\n    - num_states: Number of states in the environment.\n    - num_actions: Number of actions available in the environment.\n    \n    Returns:\n    - P: Transition probability matrix of shape (num_states, num_actions, num_states) using Dirichlet.\n    \"\"\"\n    alpha = np.ones(num_states)  # Dirichlet distribution parameter\n    P = np.zeros((num_states, num_actions, num_states))\n    for s in range(num_states):\n        for a in range(num_actions):\n            P[s, a, :] = np.random.dirichlet(alpha)  # transition probabilities for each state-action pair\n    return P\n\n\ndef backward_dynamic_programming(T, states, actions, R, P):\n    \"\"\"\n    Perform backward dynamic programming to compute the value function and action values.\n    Parameters:\n    - T: Number of time steps.\n    - states: List of states in the environment.\n    - actions: Actions available in the environment.\n    - P: Transition probability matrix of shape (num_states, num_actions, num_states).\n    \n    Returns:\n    - V: Value function for each state at each time step.\n    - A: Best action for each state at each time step.\n    \"\"\"\n\n    num_states = len(states)  # number of states\n    num_actions = len(actions)  # number of actions\n\n    V = np.zeros((T+1, num_states)) # initial value function for 80 time steps and 4 states\n    A = np.zeros((T, num_states)) # action values for 80 time steps and 4 states\n\n    for t in reversed(range(T)):\n        for i in range(num_states): # loop over states\n            # initialize best value and action\n            best_val = -np.inf  \n            best_action = None \n            # loop over actions to find the best action for the current state\n            for a in range(num_actions): # loop over actions\n                q_ia = 0 # initialize action value\n                # calculate the action value q(s, a) using the Bellman equation\n                for j in range(num_states): # loop over next states\n                    # r = np.random.normal(50, 40) # sample a reward from a normal distribution\n                    r = R[a]*states[i]  # reward based on the action and state\n                    q_ia = + r + 0.1*P[i][a][j]*V[t+1, j]  # reward + discounted value of next state\n                if q_ia &gt; best_val: # if the action value is better than the best value found so far\n                    best_val = q_ia # update best value\n                    best_action = a # update best action\n            V[t][i] = best_val # store the best value for the current state at time t\n            A[t][i] = best_action # store the best action for the current state at time t\n    return V, A            \n\n\n\nT = 120\nstates = list(range(40, 61))  # example states\nactions = [0, 1, 2]  # example actions\nnum_states = len(states)  # number of states\nnum_actions = len(actions)  # number of actions\nR = [15, 9, 12]  # rewards for each action\nP = create__trans_prob(num_states, num_actions) # create transition probability matrix\nVs, As = backward_dynamic_programming(T, states, actions, R, P)  # perform backward dynamic programming\n\n\nVs.shape\n\n(121, 21)\n\n\n\nimport pandas as pd\npd.DataFrame(Vs, columns=states, index=range(T+1)).iloc[:, 0:3].plot()\n\n\n\n\n\n\n\n\nConsider a network of two products: product 1 (bags) has 10 units in inventory, while product 2 (raincoats) has 20 units in inventory. Customers may buy product 1, product 2, or both products. The price is £15 for product 1, £20 for product 2, and £30 if a customer wants to buy both products 1 and 2. The products can be sold over a time horizon of T = 90 periods. The arrival probabilities in each time period are constant over time: λ₁ = 0.2, λ₂ = 0.15, and λ₃ = 0.1. With a probability of λ₀ = 0.55, there will be no arrival in a given period. The objective of the NGO manager is to decide, whenever a shopping request arrives, whether to accept the demand such that the total expected profit is maximized.\n\nimport numpy as np\n\ndef backward_dp_two_products(T, Imax, Jmax, lambdas, prices):\n    \"\"\"\n    Backward dynamic programming for 2‐product booking control.\n    \n    Args:\n        T       : int, horizon (e.g. 90)\n        Imax    : int, max bags inventory (e.g. 10)\n        Jmax    : int, max raincoats inventory (e.g. 20)\n        lambdas : list of arrival probabilities [λ0, λ1, λ2, ...]\n        prices   : dict, prices for each product\n                 {1: price_bag, 2: price_raincoat, 3: price_both}\n        \n    Returns:\n        V      : np.array shape (T+1, Imax+1, Jmax+1)\n                 value function\n        policy : np.array shape (T, Imax+1, Jmax+1, 4)\n                 policy[t,i,j,k] = 0 (reject) or 1 (accept) for arrival k\n    \"\"\"\n    V = np.zeros((T+1, Imax+1, Jmax+1))\n    policy = np.zeros((T, Imax+1, Jmax+1, 3), dtype=int)\n    \n    for t in range(T-1, -1, -1):\n        for i in range(Imax+1):\n            for j in range(Jmax+1):\n                # calculate the value function for each state (i, j) at time t following the Bellman equation\n                reward = 0\n                for p in range(2):\n                    if i == 0 | j == 0 # No capacity to accept this booking\n                        reject = V[t+1, i, j]\n                        policy[t, i, j, p] = 0  # Reject\n                    else:\n\n\n                          \n\n    \n    \n    return V, policy\n\n# --- Usage ---\nT = 90\nImax, Jmax = 10, 20\nlambdas = [0.55, 0.2, 0.15, 0.1]\nfares = {1:15, 2:20, 3:30}\n\nV, policy = backward_dp_two_products(T, Imax, Jmax, lambdas, fares)\n\n\nlist(range(1, 2))\n\n[0, 1]"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#outline",
    "href": "workshops/rl/RL_presentation.html#outline",
    "title": "MUSTAFA ASLAN",
    "section": "Outline",
    "text": "Outline\n\nWhat is Reinforcement Learning\nMarkov Decision Processes\nDynamic Programming\nMonte Carlo Methods\nTemporal Difference Learning\n\nSarsa\nQ-learning\n\nMore advanced approximation methods"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#reinforcement-learning",
    "href": "workshops/rl/RL_presentation.html#reinforcement-learning",
    "title": "MUSTAFA ASLAN",
    "section": "Reinforcement Learning",
    "text": "Reinforcement Learning\n\nReinforcement learning is learning what to do—how to map situations to actions to maximize a numerical reward signal.\nThe learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them."
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#why-use-reinforcement-learning",
    "href": "workshops/rl/RL_presentation.html#why-use-reinforcement-learning",
    "title": "MUSTAFA ASLAN",
    "section": "Why use Reinforcement Learning?",
    "text": "Why use Reinforcement Learning?\n\n\n\n\n\n\n\n\n\nModel-Free Adaptation: Learns optimal behavior directly from experience\nLong-Term Planning: Optimizes decisions over long time horizons\nData-Driven Policies: Automatically refines its policy as more data becomes available\nScalable to Complex Tasks: Can handle very large state-action spaces via function approximation."
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#core-concepts",
    "href": "workshops/rl/RL_presentation.html#core-concepts",
    "title": "MUSTAFA ASLAN",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nAgent is the learner and decision maker.\n\nIt interacts with the environment continually to select actions\n\nThe environment responds to these actions and presents new situations to the agent.\n\nA policy defines the learning agent’s way of behaving at a given time.\n\nA mapping from states of the environment to actions to be taken when in those states.\n\nThe reward signal indicates what is good immediately after each decision time.\nA value function specifies what is good in the long run.\n\nThe value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state."
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#core-definitions",
    "href": "workshops/rl/RL_presentation.html#core-definitions",
    "title": "MUSTAFA ASLAN",
    "section": "Core definitions",
    "text": "Core definitions\n\nMDPs are a classical formalization of sequential decision making or reinforcement learning problem\nActions taken now influence not only immediate contributions (e.g. reward, cost, profit), but also future situations (states), and consequently, future contributions\nMDPs involve delayed contribution and the need to trade off immediate and delayed contributions"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#the-agentenvironment-interaction-in-a-decision-process",
    "href": "workshops/rl/RL_presentation.html#the-agentenvironment-interaction-in-a-decision-process",
    "title": "MUSTAFA ASLAN",
    "section": "The agent–environment interaction in a decision process",
    "text": "The agent–environment interaction in a decision process\n\nThe agent–environment interaction in a decision processAgent gives rise to a sequence that begins like this\n\\[\nS_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \\dots  S_t, A_t, R_{t+1}\n\\]"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#the-agentenvironment-interaction-in-a-decision-process-continued",
    "href": "workshops/rl/RL_presentation.html#the-agentenvironment-interaction-in-a-decision-process-continued",
    "title": "MUSTAFA ASLAN",
    "section": "The agent–environment interaction in a decision process (continued)",
    "text": "The agent–environment interaction in a decision process (continued)\n\nAn MDP can be described by the tuple \\((S, A, p, r(a,s), \\pi)\\), where:\n\n\\(S\\) is a finite set of states,\n\\(A\\) is a finite set of actions,\n\\(p(s', r \\mid s, a)\\) is the probability of transitioning to state \\(s' \\in S\\) and receiving reward \\(r(a,s)\\) when taking action \\(a \\in A\\) in state \\(s \\in S\\),\n\\(r(s, a)\\) is the expected immediate reward received after taking action \\(a\\) in state \\(s\\),\n\\(\\pi\\) is the policy, a mapping from states to probabilities of selecting each possible action.\n\nThe \\(p(s', \\mid s, a)\\) defines the dynamics of the MDP and \\(p\\) specifies a probability distribution for each choice of \\(s\\) and \\(a\\): \\[\n\\sum_{s'\\in S}p(s' \\mid s, a) = 1, \\text{ for all } s \\in S, a \\in A(s)\n\\]"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#the-goal-of-agent",
    "href": "workshops/rl/RL_presentation.html#the-goal-of-agent",
    "title": "MUSTAFA ASLAN",
    "section": "The Goal of Agent",
    "text": "The Goal of Agent\n\nAt each time step, the reward is a simple number, \\(R_t \\in \\mathbb{R}\\) passing from the environment to the agent.\nThe agent’s goal is to maximize the total (cumulative) amount of reward it receives in the long run.\nThe aim is to maximize the expected return, \\(G_t\\), which is defined as: \\[\nG_t \\doteq R_{t+1} + R_{t+2} + R_{t+3}+ \\dots+R_T\n\\]\n\nDiscounting technique is used to prioritize immediate rewards over future rewards.\n\nThe idea is to multiply future rewards by a discount factor \\(\\gamma \\in (0,1]\\)\nThis makes future rewards worth less than immediate rewards.\nThe return \\(G_t\\) with discounting is defined as:\n\n\\[\nG_t \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n\\]"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#policies-and-value-functions",
    "href": "workshops/rl/RL_presentation.html#policies-and-value-functions",
    "title": "MUSTAFA ASLAN",
    "section": "Policies and Value Functions",
    "text": "Policies and Value Functions\n\nA policy is a mapping from states to probabilities of selecting each possible action. If the agent is following policy \\(\\pi\\) at time \\(t\\), then \\(\\pi(a \\mid s)\\) is the probability that \\(A_t = a\\) if \\(S_t = s\\)\nValue functions—functions of states estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state).\nThe value of a state \\(s\\) under a policy \\(\\pi\\), denoted \\(v_\\pi\\) is expressed as \\[\nv_\\pi(s) \\doteq \\mathbb{E}_\\pi[G_t | S_t=s] = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t=s\\right], \\text{ for all } s \\in S,\n\\]\n\nwhere \\(\\mathbb{E}[\\cdot]\\) denotes the expected value of a random variable given that the agent follows policy \\(\\pi\\), and \\(t\\) is any time step."
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#bellman-equation",
    "href": "workshops/rl/RL_presentation.html#bellman-equation",
    "title": "MUSTAFA ASLAN",
    "section": "Bellman Equation",
    "text": "Bellman Equation\nTo solve stochastic sequential decision problems\n\nWe have to model the fact that new information becomes available after we make the decision \\(a_t\\).\nThe result can be uncertainty in both the contribution earned, and in the determination of the next state we visit, \\(S_{t+1}\\).\n\nWe estimate the expected future contributions (cumulative rewards) and add the immediate contribution received from the action taken in the current state following a given policy. This relationship is expressed by the Bellman equation, defined as:\n\n\\[\nV_\\pi(S_t) = \\max_{a_t \\in A_t} \\left(r_{t+1}(S_t, a_t) +\\\n\\gamma \\mathbb{E}[V_{t+1}(S_{t+1}) \\mid S_t, a_t]\n\\right)\n\\]\nThis equation can be solved backward or forward recursively. You get the value functions and at the same time an optimal policy at time \\(t\\)\n\\[\na_t^*(S_t) = \\arg\\max_{a_t \\in A_t} \\left(r_{t+1}(S_t, a_t) + \\gamma [V_{t+1}(S_{t+1}) \\mid S_t, a_t] \\right),\n\\]"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#exercise-solve-the-car-rental-problem",
    "href": "workshops/rl/RL_presentation.html#exercise-solve-the-car-rental-problem",
    "title": "MUSTAFA ASLAN",
    "section": "Exercise: Solve the Car Rental Problem",
    "text": "Exercise: Solve the Car Rental Problem\n\n\nBooking Process:\n\nExclusively 1-day rentals, must booked the day before.\nBookings arrive randomly in two batches (Morning, Afternoon) of previous day\nEach batch has one of the four possible demand (# Large requests, # Small requests)\n\nBooking Controls:\n\nAccept a booking request\nReject a booking request\nUpgrade: Small car can be met by the large car offered at £30\n\n\nDemand Forecast\n\n\n(# Large requests, # Small Requests)\n(0,0)\n(0,1)\n(1,0)\n(1,1)\n\n\n\n\nArrival Probability\n3%\n10%\n20%\n67%\n\n\n\n\nRental Rate\n\n\nSmall Car\nLarge car\n\n\n\n\n£30\n£40"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#a-working-example-car-rental-booking-control",
    "href": "workshops/rl/RL_presentation.html#a-working-example-car-rental-booking-control",
    "title": "MUSTAFA ASLAN",
    "section": "A Working Example: Car Rental Booking Control",
    "text": "A Working Example: Car Rental Booking Control\nGoal: Find a booking control policy that maximises the firm’s expected revenues\nWhat is the value of \\(v_1(1,1)\\)"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#what-if",
    "href": "workshops/rl/RL_presentation.html#what-if",
    "title": "MUSTAFA ASLAN",
    "section": "What if…",
    "text": "What if…\n\n\n\nCars can be rented for more than one day\nLonger booking periods\nMore types of cars\nLarger fleet\nA network of rental stations\nAdvanced booking\n\n\n\nThe Bellman equation could not be solved anymore.\n\nCurse of Dimensionality\n\nThat is where we need Approximate Dynamic Programming and Reinforcement Learning\n\nTo obtain good policies without the need to solve the Bellman equation directly"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#policy-evalulation-prediction",
    "href": "workshops/rl/RL_presentation.html#policy-evalulation-prediction",
    "title": "MUSTAFA ASLAN",
    "section": "Policy Evalulation (Prediction)",
    "text": "Policy Evalulation (Prediction)\n\nPolicy evaluation is the computation of the state-value function \\(v_\\pi\\) for an arbitrary policy \\(\\pi\\).\nConsider a sequence of approximate value functions \\(v_0, v_1, v_2,\\dots,\\). Each successive approximation is obtained by using the Bellman equation for \\(v_\\pi\\) as an update rule:\n\n\\[\n\\begin{aligned}\nv_{k+1}(s) &\\doteq \\mathbb{E}_\\pi \\big[G_t \\mid S_t = s \\big] \\\\\n&= \\mathbb{E}_\\pi \\big[r_{t+1} + \\gamma G_{t+1} \\mid S_t = s \\big] \\\\\n&\\doteq \\mathbb{E}_\\pi \\big[r_{t+1} + \\gamma v_k(S_{t+1}) \\mid S_t = s \\big] \\\\\n         &= \\sum_a \\pi(a \\mid s) \\sum_{s',r} p(s', r \\mid s, a) \\big[ r + \\gamma v_k(s') \\big].\n\\end{aligned}\n\\]\nwhere \\(k\\) refers to iteration number."
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#policy-evalulation-prediction-1",
    "href": "workshops/rl/RL_presentation.html#policy-evalulation-prediction-1",
    "title": "MUSTAFA ASLAN",
    "section": "Policy Evalulation (Prediction)",
    "text": "Policy Evalulation (Prediction)\nThe sequence \\({v_k}\\) can be shown in general to converge to \\(v_\\pi\\) as \\(k \\rightarrow \\infty\\). This algorithm is called iterative policy evaluation.\n\n\n\nIterative Policy Evaluation, for estimating \\(V \\approx v_\\pi\\)\n\n\nInput:\n\\(\\pi\\), the policy to be evaluated\nAlgorithm parameter: a small threshold \\(\\theta &gt; 0\\) determining accuracy of estimation\nInitialize \\(V(s)\\) arbitrarily, for \\(s \\in S\\), and \\(V(\\text{terminal}) = 0\\)\nLoop:\n\\(\\Delta \\leftarrow 0\\)\nLoop for each \\(s \\in S\\):\n \\(v \\leftarrow V(s)\\)\n \\(V(s) \\leftarrow \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r | s, a) [r + \\gamma V(s')]\\)\n \\(\\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)\\)\nuntil \\(\\Delta &lt; \\theta\\)"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#policy-improvement",
    "href": "workshops/rl/RL_presentation.html#policy-improvement",
    "title": "MUSTAFA ASLAN",
    "section": "Policy Improvement",
    "text": "Policy Improvement\n\nWe know how good it is to follow the current policy from \\(s\\)—that is \\(v_\\pi(s)\\)\nBut would it be better or worse to change to the new policy?\n\nOnce a policy, \\(\\pi\\), has been improved using \\(v_\\pi\\) to yield a better policy, \\(\\pi^{'}\\), we can then compute \\(v_{\\pi^{'}}\\) and improve it again to yield an even better \\(\\pi^{''}\\).\n\\[\n\\pi_0 \\xrightarrow E v_{\\pi_0} \\xrightarrow I \\pi_1 \\xrightarrow E v_{\\pi_1}\\xrightarrow I \\pi_2,\\dots \\xrightarrow I \\pi_* \\xrightarrow E v_*\n\\]\nwhere \\(\\xrightarrow E\\) denotes a policy evaluation and \\(\\xrightarrow I\\) denotes a policy improvement. This way of finding an optimal policy is called policy iteration."
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#policy-iteration",
    "href": "workshops/rl/RL_presentation.html#policy-iteration",
    "title": "MUSTAFA ASLAN",
    "section": "Policy Iteration",
    "text": "Policy Iteration\nA complete policy iteration algorithm\n\n\n\nPolicy Iteration (using iterative policy evaluation) for estimating \\(\\pi \\approx \\pi_*\\)\n\n\n1. Initialization\n\\(V(s) \\in \\mathbb{R}\\) and \\(\\pi(s) \\in A(s)\\) arbitrarily for all \\(s \\in S\\)\n\\(V(\\text{terminal}) \\doteq 0\\)\n2. Policy Evaluation\nLoop:\n \\(\\Delta \\leftarrow 0\\)\n Loop for each \\(s \\in S\\):\n  \\(v \\leftarrow V(s)\\)\n  \\(V(s) \\leftarrow \\sum_{s', r} p(s', r \\mid s, \\pi(s)) \\left[r + \\gamma V(s')\\right]\\)\n  \\(\\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)\\)\nUntil \\(\\Delta &lt; \\theta\\) (a small positive number determining the accuracy of estimation)\n3. Policy Improvement\npolicy-stable \\(\\leftarrow\\) true\nFor each \\(s \\in S\\):\n old-action \\(\\leftarrow \\pi(s)\\)\n \\(\\pi(s) \\leftarrow \\arg\\max_a \\sum_{s', r} p(s', r \\mid s, a)\\left[r + \\gamma V(s')\\right]\\)\n If old-action \\(\\ne \\pi(s)\\), then policy-stable \\(\\leftarrow\\) false\nIf policy-stable, then stop and return \\(V \\approx v_*\\) and \\(\\pi \\approx \\pi_*\\);\nElse go to step 2"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#value-iteration",
    "href": "workshops/rl/RL_presentation.html#value-iteration",
    "title": "MUSTAFA ASLAN",
    "section": "Value Iteration",
    "text": "Value Iteration\n\nOne drawback to policy iteration is that each of its iterations involves policy evaluation, which may need a huge iterative computation requiring multiple sweeps through the state set.\nValue iteration is a special case of policy iteration where the policy evaluation step is truncated to just one sweep.\nThis algorithm combines the policy improvement and truncated policy evaluation steps into a single update operation:\n\n\\[\n\\begin{aligned}\nv_{k+1}(s) &= \\mathbb{E} \\big[R_{t+1} + \\gamma v_k(S_{t+1}) \\mid S_t=s, A_t = a \\big] \\\\\n       &=\\max_a \\sum_{s', r} p(s', r \\mid s, a) \\big[ r + \\gamma v_k(s') \\big]\n\\end{aligned}\n\\]\nfor all \\(s \\in S\\)."
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#value-iteration-1",
    "href": "workshops/rl/RL_presentation.html#value-iteration-1",
    "title": "MUSTAFA ASLAN",
    "section": "Value Iteration",
    "text": "Value Iteration\n\nValue iteration is also obtained simply by turning the Bellman optimality equation into an update rule.\nAlso note how the value iteration update is identical to the policy evaluation update except that it requires the maximum to be taken over all actions.\n\n\n\n\nValue Iteration, for estimating \\(\\pi \\approx \\pi_*\\)\n\n\nAlgorithm parameter:\nA small threshold \\(\\theta &gt; 0\\) determining the accuracy of estimation\nInitialization:\nInitialize \\(V(s)\\) arbitrarily for all \\(s \\in S^+\\), except that \\(V(\\text{terminal}) = 0\\)\nLoop:\n \\(\\Delta \\leftarrow 0\\)\n Loop for each \\(s \\in S\\):\n  \\(v \\leftarrow V(s)\\)\n  \\(V(s) \\leftarrow \\max_a \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma V(s') \\right]\\)\n  \\(\\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)\\)\nuntil \\(\\Delta &lt; \\theta\\)\nOutput:\nA deterministic policy, \\(\\pi \\approx \\pi_*\\), such that\n \\(\\pi(s) = \\arg\\max_a \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma V(s') \\right]\\)"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#monte-carlo-mc-methods",
    "href": "workshops/rl/RL_presentation.html#monte-carlo-mc-methods",
    "title": "MUSTAFA ASLAN",
    "section": "Monte Carlo (MC) Methods",
    "text": "Monte Carlo (MC) Methods\n\nThe term “Monte Carlo” is often used more broadly for any estimation method whose operation involves a significant random component.\nMC methods solve reinforcement learning problems by averaging results (returns) from sampled experiences sequences of states, actions, and rewards.\nThey do not require knowledge of the environment’s dynamics, making them powerful for learning from real or simulated experiences."
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#monte-carlo-prediction",
    "href": "workshops/rl/RL_presentation.html#monte-carlo-prediction",
    "title": "MUSTAFA ASLAN",
    "section": "Monte Carlo Prediction",
    "text": "Monte Carlo Prediction\nSuppose we wish to estimate \\(v_{\\pi}(s)\\), the values of a state \\(s\\) under policy \\(\\pi\\), given a set of episodes obtained by following \\(\\pi\\) and passing through \\(s\\).\n\n\n\nFirst-Visit Monte Carlo Prediction (for estimating \\(V \\approx v_\\pi\\))\n\n\nInput:\nA policy \\(\\pi\\) to be evaluated\nInitialize:\n \\(V(s) \\in \\mathbb{R}\\) arbitrarily, for all \\(s \\in S\\)\n \\(\\text{Returns}(s) \\leftarrow\\) an empty list, for all \\(s \\in S\\)\nLoop forever (for each episode):\n Generate an episode following \\(\\pi\\): \\(S_0, A_0, R_1, S_1, A_1, R_2, \\dots, S_{T-1}, A_{T-1}, R_T\\)\n \\(G \\leftarrow 0\\)\n Loop for each step of the episode, \\(t = T-1, T-2, \\dots, 0\\):\n  \\(G \\leftarrow \\gamma G + R_{t+1}\\)\n  Unless \\(S_t\\) appears in \\(S_0, S_1, \\dots, S_{t-1}\\):\n   Append \\(G\\) to \\(\\text{Returns}(S_t)\\)\n   \\(V(S_t) \\leftarrow \\text{average}(\\text{Returns}(S_t))\\)"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#monte-carlo-control",
    "href": "workshops/rl/RL_presentation.html#monte-carlo-control",
    "title": "MUSTAFA ASLAN",
    "section": "Monte Carlo Control",
    "text": "Monte Carlo Control\nAlternating complete steps of policy evaluation and policy improvement are performed, beginning with an arbitrary policy \\(\\pi_0\\) and ending with the optimal policy and optimal action-value function:\n\\[\n\\pi_0 \\xrightarrow E q_{\\pi_0} \\xrightarrow I \\pi_1 \\xrightarrow E q_{\\pi_1}\\xrightarrow I \\pi_2,\\dots,\\xrightarrow I \\pi_* \\xrightarrow E q_*\n\\]\n\n\n\nMonte Carlo ES (Exploring Starts), for estimating \\(\\pi \\approx \\pi_*\\)\n\n\nInitialize:\n\\(\\pi(s) \\in A(s)\\) arbitrarily, for all \\(s \\in S\\)\n\\(Q(s, a) \\in \\mathbb{R}\\) arbitrarily, for all \\(s \\in S, a \\in A(s)\\)\n\\(\\text{Returns}(s, a) \\leftarrow\\) empty list, for all \\(s \\in S, a \\in A(s)\\)\nLoop forever (for each episode):\n Choose \\(S_0 \\in S, A_0 \\in A(S_0)\\) randomly, such that all pairs have probability \\(&gt; 0\\)\n Generate an episode from \\(S_0, A_0\\), following \\(\\pi\\):\n  \\(S_0, A_0, R_1, \\dots, S_{T-1}, A_{T-1}, R_T\\)\n \\(G \\leftarrow 0\\)\n Loop for each step of episode, \\(t = T-1, T-2, \\dots, 0\\):\n  \\(G \\leftarrow \\gamma G + R_{t+1}\\)\n  Unless the pair \\((S_t, A_t)\\) appears in\n   \\(S_0, A_0, S_1, A_1, \\dots, S_{t-1}, A_{t-1}\\):\n   Append \\(G\\) to \\(\\text{Returns}(S_t, A_t)\\)\n   \\(Q(S_t, A_t) \\leftarrow \\text{average}(\\text{Returns}(S_t, A_t))\\)\n   \\(\\pi(S_t) \\leftarrow \\arg\\max_a Q(S_t, a)\\)"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#monte-carlo-control-without-exploring-starts",
    "href": "workshops/rl/RL_presentation.html#monte-carlo-control-without-exploring-starts",
    "title": "MUSTAFA ASLAN",
    "section": "Monte Carlo Control without Exploring Starts",
    "text": "Monte Carlo Control without Exploring Starts\n\nIn on-policy control methods the policy is generally soft, meaning that \\(\\pi(a\\mid s)&gt;0\\) for all \\(s \\in S\\) and all \\(a \\in A(s)\\), but gradually shifted closer and closer to a deterministic policy.\nThe on-policy method we present in this section uses \\(\\epsilon\\)-greedy policies, meaning that most of the time they choose an action that has maximal estimated action value, but with probability \\(\\epsilon\\) they instead select an action at random. That is, all nongreedy actions are given the minimal probability of selection, \\(\\frac{\\epsilon}{|A(s)|}\\), and the remaining bulk of the probability \\(1-\\epsilon+\\frac{\\epsilon}{|A(s)|}\\) is given to the greedy action.\n\\(\\epsilon\\)-greedy policies are examples of \\(\\epsilon-soft\\) policies, definied as policies for which \\(\\pi(a \\mid s) \\geq \\frac{\\epsilon}{|A(s)|}\\) for all states and actions, for some \\(\\epsilon &gt; 0\\). Among \\(\\epsilon\\)-soft policies, \\(\\epsilon\\)-greedy policies are in some sense those that are closest to greedy."
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#monte-carlo-control-without-exploring-starts-1",
    "href": "workshops/rl/RL_presentation.html#monte-carlo-control-without-exploring-starts-1",
    "title": "MUSTAFA ASLAN",
    "section": "Monte Carlo Control without Exploring Starts",
    "text": "Monte Carlo Control without Exploring Starts\n\n\n\nOn-policy First-Visit MC Control (for \\(\\varepsilon\\)-soft policies), estimates \\(\\pi \\approx \\pi_*\\)\n\n\nAlgorithm parameter:\nSmall \\(\\varepsilon &gt; 0\\)\nInitialize:\n\\(\\pi \\leftarrow\\) an arbitrary \\(\\varepsilon\\)-soft policy\n\\(Q(s, a) \\in \\mathbb{R}\\) arbitrarily, for all \\(s \\in S, a \\in A(s)\\)\n\\(\\text{Returns}(s, a) \\leftarrow\\) empty list, for all \\(s \\in S, a \\in A(s)\\)\nRepeat forever (for each episode):\n Generate an episode following \\(\\pi\\): \\(S_0, A_0, R_1, \\dots, S_{T-1}, A_{T-1}, R_T\\)\n \\(G \\leftarrow 0\\)\n Loop for each step of episode, \\(t = T-1, T-2, \\dots, 0\\):\n  \\(G \\leftarrow \\gamma G + R_{t+1}\\)\n  Unless the pair \\((S_t, A_t)\\) appears in \\(S_0, A_0, \\dots, S_{t-1}, A_{t-1}\\):\n   Append \\(G\\) to \\(\\text{Returns}(S_t, A_t)\\)\n   \\(Q(S_t, A_t) \\leftarrow \\text{average}(\\text{Returns}(S_t, A_t))\\)\n   \\(A^* \\leftarrow \\arg\\max_a Q(S_t, a)\\) (ties broken arbitrarily)\n   For all \\(a \\in A(S_t)\\):\n    \\(\\pi(a \\mid S_t) \\leftarrow \\begin{cases}\n1 - \\varepsilon + \\varepsilon / |A(S_t)| & \\text{if } a = A^* \\\\\n\\varepsilon / |A(S_t)| & \\text{if } a \\ne A^*\n\\end{cases}\\)\n\n\n\nwhere \\(|A(s)|\\) is the number of actions available in state \\(s\\).\nThe \\(\\epsilon\\)-greedy policy ensures that all actions are tried, but actions with higher value estimates are tried more frequently. This balances exploration (trying new actions) and exploitation (choosing the best-known action)."
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#what-is-td-learning",
    "href": "workshops/rl/RL_presentation.html#what-is-td-learning",
    "title": "MUSTAFA ASLAN",
    "section": "What is TD Learning",
    "text": "What is TD Learning\n\nTD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas.\nLike Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment’s dynamics.\nLike DP, TD methods update estimates without waiting for a final outcome.\nWhereas Monte Carlo methods must wait until the end of the episode to determine the increment to \\(V(St)\\)\n\nThe simplest TD method updates \\(v_(S_t)\\) as follow:\n\\[\nv(S_t) \\leftarrow v(S_t) + \\alpha \\big[r_{t+1} + \\gamma v(S_{t+1})-v(S_t)  \\big]\n\\]"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#td-prediction",
    "href": "workshops/rl/RL_presentation.html#td-prediction",
    "title": "MUSTAFA ASLAN",
    "section": "TD Prediction",
    "text": "TD Prediction\n\n\n\nTabular TD(0) for Estimating \\(v_\\pi\\)\n\n\nInput:\nThe policy \\(\\pi\\) to be evaluated\nAlgorithm parameter:\nStep size \\(\\alpha \\in (0, 1]\\)\nInitialize:\n\\(V(s)\\) arbitrarily for all \\(s \\in S^+\\), except that \\(V(\\text{terminal}) = 0\\)\nLoop for each episode:\n Initialize \\(S\\)\n Loop for each step of episode:\n  \\(A \\leftarrow\\) action given by \\(\\pi\\) for \\(S\\)\n  Take action \\(A\\), observe \\(R\\), \\(S'\\)\n  \\(V(S) \\leftarrow V(S) + \\alpha \\left[ R + \\gamma V(S') - V(S) \\right]\\)\n  \\(S \\leftarrow S'\\)\n Until \\(S\\) is terminal"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#sarsa-on-policy-td-control",
    "href": "workshops/rl/RL_presentation.html#sarsa-on-policy-td-control",
    "title": "MUSTAFA ASLAN",
    "section": "Sarsa: On-policy TD Control",
    "text": "Sarsa: On-policy TD Control\nWe consider transitions from state–action pair to state–action pair, and learn the values of state–action pairs.\n\\[\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\big[R_{t+1}+\\gamma Q(S_{t+1}, A_{t+1})-Q(S_t, A_t) \\big]\n\\]\n\n\n\nSARSA (On-Policy TD Control), for estimating \\(Q \\approx q_*\\)\n\n\nAlgorithm parameters:\nStep size \\(\\alpha \\in (0, 1]\\), small \\(\\varepsilon &gt; 0\\)\nInitialize:\n\\(Q(s, a)\\) arbitrarily, for all \\(s \\in S^+\\), \\(a \\in A(s)\\)\n\\(Q(\\text{terminal}, \\cdot) = 0\\)\nLoop for each episode:\n Initialize \\(S\\)\n Choose \\(A\\) from \\(S\\) using a policy derived from \\(Q\\) (e.g., \\(\\varepsilon\\)-greedy)\n Loop for each step of episode:\n  Take action \\(A\\), observe \\(R\\), \\(S'\\)\n  Choose \\(A'\\) from \\(S'\\) using a policy derived from \\(Q\\) (e.g., \\(\\varepsilon\\)-greedy)\n  \\(Q(S, A) \\leftarrow Q(S, A) + \\alpha \\left[ R + \\gamma Q(S', A') - Q(S, A) \\right]\\)\n  \\(S \\leftarrow S';\\quad A \\leftarrow A'\\)\nUntil \\(S\\) is terminal"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#q-learning-off-policy-td-control",
    "href": "workshops/rl/RL_presentation.html#q-learning-off-policy-td-control",
    "title": "MUSTAFA ASLAN",
    "section": "Q-learning: Off-policy TD Control",
    "text": "Q-learning: Off-policy TD Control\nQ-learning is defined by\n\\[\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t)+ \\alpha \\big[R_{t+1}+\\gamma \\max_aQ(S_{t+1}, a)-Q(S_t, A_t)  \\big]\n\\]\nThe Q-learning algorithm is shown below in procedural form.\n\n\n\nQ-learning (Off-Policy TD Control), for estimating \\(\\pi \\approx \\pi_*\\)\n\n\nAlgorithm parameters:\nStep size \\(\\alpha \\in (0, 1]\\), small \\(\\varepsilon &gt; 0\\)\nInitialize:\n\\(Q(s, a)\\) arbitrarily for all \\(s \\in S^+\\), \\(a \\in S(s)\\),\nexcept that \\(Q(\\text{terminal}, \\cdot) = 0\\)\nLoop for each episode:\n Initialize \\(S\\)\n Loop for each step of episode:\n  Choose \\(A\\) from \\(S\\) using a policy derived from \\(Q\\) (e.g., \\(\\varepsilon\\)-greedy)\n  Take action \\(A\\), observe \\(R\\), \\(S'\\)\n  \\(Q(S, A) \\leftarrow Q(S, A) + \\alpha \\left[ R + \\gamma \\max_a Q(S', a) - Q(S, A) \\right]\\)\n  \\(S \\leftarrow S'\\)\nUntil \\(S\\) is terminal"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#model-based-vs-model-free-methods",
    "href": "workshops/rl/RL_presentation.html#model-based-vs-model-free-methods",
    "title": "MUSTAFA ASLAN",
    "section": "Model-based vs Model-free Methods",
    "text": "Model-based vs Model-free Methods\n\n\nModel-based\n\n\nThe agent knows/learns the model of the environment\nThey then compute the policy using the ADP methods or the model-free methods on simulated data\n\nPros:\n\nSample efficient\nSafer exploration\n\nCons:\n\nProne to the model errors\nLearning a model is challenging\n\n\n\nModel-free\n\n\nThe agent does not know the model of the environment\nThey learn the values or policies from trial-and-error interactions with the environment\n\nPros:\n\nDo not need a model\nFlexible\n\nCons:\n\nSample inefficient: requires a lot of interactions with the environment\nSlow convergence"
  },
  {
    "objectID": "workshops/rl/RL_presentation.html#recommended-materials",
    "href": "workshops/rl/RL_presentation.html#recommended-materials",
    "title": "MUSTAFA ASLAN",
    "section": "Recommended materials",
    "text": "Recommended materials\nReadings:\n\nReinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto\nReinforcement Learning and Stochastic Optimization by Warren B. Powell\n\nTutoriols by Steve Brunton:"
  },
  {
    "objectID": "workshops/rl/note.html",
    "href": "workshops/rl/note.html",
    "title": "Notes for presentation",
    "section": "",
    "text": "page 2 - Model-Free Adaptation: Learns optimal behavior directly from experience, without needing an explicit model of the environment.\n\nLong-Term Planning: Optimizes decisions over long time horizons by balancing immediate vs. future rewards.\n\nScalable to Complex Tasks\nCan handle very large state-action spaces via function approximation (e.g., neural networks).\n\nInteractive Settings\nIdeal when you have an agent interacting with an environment (games, robotics, resource allocation).\n\nData-Driven Policies\nAutomatically refines its policy as more data becomes available, making it robust to non-stationarity."
  },
  {
    "objectID": "workshops/rl/rl.html",
    "href": "workshops/rl/rl.html",
    "title": "1. Python implementation of the Value Iteration algorithm",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef value_iteration(env, gamma=0.99, theta=1e-6):\n    \"\"\"\n    Performs value iteration to compute an optimal policy.\n\n    Args:\n        env: An environment that provides:\n            - env.states: a list of all states\n            - env.actions(s): a list of actions available in state s\n            - env.transitions(s, a): a list of (probability, next_state, reward) tuples\n            - env.is_terminal(s): a function to check if a state is terminal\n        gamma: Discount factor.\n        theta: Small threshold for convergence.\n\n    Returns:\n        A tuple (V, pi) where:\n            - V is a dict of state -&gt; value\n            - pi is a dict of state -&gt; optimal action\n    \"\"\"\n    V = {s: 0.0 for s in env.states}\n    history = {s: [0] for s in env.states}\n    for s in env.states:\n        if env.is_terminal(s):\n            V[s] = 0.0  # terminal states have zero value\n\n    while True:\n        delta = 0\n        for s in env.states:\n            if env.is_terminal(s):\n                continue\n            v = V[s]\n            action_values = []\n            for a in env.actions(s):\n                q = sum(\n                    prob * (reward + gamma * V[next_state])\n                    for prob, next_state, reward in env.transitions(s, a)\n                )\n                action_values.append(q)\n            V[s] = max(action_values)\n            history[s].append(V[s])\n            delta = max(delta, abs(v - V[s]))\n        if delta &lt; theta:\n            break\n\n    # Derive the policy\n    policy = {}\n    for s in env.states:\n        if env.is_terminal(s):\n            policy[s] = None\n            continue\n        best_action = max(\n            env.actions(s),\n            key=lambda a: sum(\n                prob * (reward + gamma * V[next_state])\n                for prob, next_state, reward in env.transitions(s, a)\n            )\n        )\n        policy[s] = best_action\n\n    return V, policy, history"
  },
  {
    "objectID": "workshops/rl/rl.html#example-usage--environment-for-diabetes-care",
    "href": "workshops/rl/rl.html#example-usage--environment-for-diabetes-care",
    "title": "1. Python implementation of the Value Iteration algorithm",
    "section": "1.1 Example usage- Environment for Diabetes Care",
    "text": "1.1 Example usage- Environment for Diabetes Care\n\nThis environment simulates a diabetes care scenario with different states and actions.\nThe states represent the health status of a patient, and the actions represent different care strategies.\nThe transitions define the probabilities of moving from one state to another based on the action taken.\nThe rewards are based on the health outcomes of the actions taken.\n\n\n\nclass DiabetesCareEnv:\n    def __init__(self):\n        # all possible states\n        self.states = [\"stable\", \"moderate\", \"critical\", \"dead\"] \n        self._terminal_states = {\"dead\"}\n\n    def is_terminal(self, s):\n        return s in self._terminal_states\n\n    def actions(self, s):\n        if s == \"dead\":\n            return []\n        return [\"lifestyle\", \"medicate\", \"intensive_care\"]\n\n    def transitions(self, s, a):\n        \"\"\"\n        Returns a list of (probability, next_state, reward) tuples.\n        Each action from a state leads to all possible states.\n        \"\"\"\n\n        if s == \"stable\":\n            if a == \"lifestyle\":\n                return [\n                    (0.80, \"stable\", 8),\n                    (0.10, \"moderate\", 0),\n                    (0.05, \"critical\", -10),\n                    (0.05, \"dead\", -100),\n                ]\n            elif a == \"medicate\":\n                return [\n                    (0.85, \"stable\", 6),\n                    (0.10, \"moderate\", -3),\n                    (0.03, \"critical\", -10),\n                    (0.02, \"dead\", -100),\n                ]\n            elif a == \"intensive_care\":\n                return [\n                    (0.90, \"stable\", -5),\n                    (0.05, \"moderate\", -8),\n                    (0.03, \"critical\", -15),\n                    (0.02, \"dead\", -100),\n                ]\n\n        elif s == \"moderate\":\n            if a == \"lifestyle\":\n                return [\n                    (0.50, \"stable\", 5),\n                    (0.30, \"moderate\", -1),\n                    (0.15, \"critical\", -6),\n                    (0.05, \"dead\", -50),\n                ]\n            elif a == \"medicate\":\n                return [\n                    (0.60, \"stable\", 6),\n                    (0.25, \"moderate\", 0),\n                    (0.10, \"critical\", -5),\n                    (0.05, \"dead\", -50),\n                ]\n            elif a == \"intensive_care\":\n                return [\n                    (0.70, \"stable\", 3),\n                    (0.20, \"moderate\", -2),\n                    (0.05, \"critical\", -10),\n                    (0.05, \"dead\", -80),\n                ]\n\n        elif s == \"critical\":\n            if a == \"lifestyle\":\n                return [\n                    (0.05, \"stable\", 2),\n                    (0.10, \"moderate\", -3),\n                    (0.50, \"critical\", -12),\n                    (0.35, \"dead\", -100),\n                ]\n            elif a == \"medicate\":\n                return [\n                    (0.10, \"stable\", 3),\n                    (0.30, \"moderate\", -2),\n                    (0.50, \"critical\", -8),\n                    (0.20, \"dead\", -100),\n                ]\n            elif a == \"intensive_care\":\n                return [\n                    (0.30, \"stable\", 5),\n                    (0.30, \"moderate\", 0),\n                    (0.30, \"critical\", -3),\n                    (0.10, \"dead\", -90),\n                ]\n\n        elif s == \"dead\":\n            return [(1.0, \"dead\", 0)]\n\n        return []\n\n\nenv = DiabetesCareEnv()\nV, policy, hist = value_iteration(env, gamma=0.95, theta=1e-4)\n\nfor state in env.states:\n    print(f\"State: {state:10s} | Value: {V[state]:6.2f} | Best Action: {policy[state]}\")\n\nState: stable     | Value:  22.73 | Best Action: medicate\nState: moderate   | Value:  18.36 | Best Action: medicate\nState: critical   | Value:   4.63 | Best Action: intensive_care\nState: dead       | Value:   0.00 | Best Action: None\n\n\n\nplt.figure(figsize=(12, 8))\nfor state in env.states:\n    if state == \"dead\":\n        continue\n    plt.plot(hist[state], label=state)\nplt.title(\"Value Iteration Convergence\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Value\")\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "workshops/rl/rl.html#interpretation",
    "href": "workshops/rl/rl.html#interpretation",
    "title": "1. Python implementation of the Value Iteration algorithm",
    "section": "1.2 Interpretation",
    "text": "1.2 Interpretation\n\n\\(\\pi(s)\\) = lifestyle: Suggests conservative management is best.\n\\(\\pi(s)\\) = medicate: Indicates medical intervention is effective and worth the cost.\n\\(\\pi(s)\\) = intensive_care: Signals high urgency; aggressive action justified.\n\n\ninterp = {\n    'state': ['stable', 'moderate', 'critical', 'dead'],\n    'best_action': ['lifestyle', 'medicate', 'intensive_care', \"none\"],\n    'interpretation': [\n        'Encourage self-care',\n        'Best to intervene with medication',\n        'Aggressive care needed to avoid death.',\n        'No further action needed.'\n    ]}\ninterp_df = pd.DataFrame(interp)\ninterp_df\n\n\n\n\n\n\n\n\nstate\nbest_action\ninterpretation\n\n\n\n\n0\nstable\nlifestyle\nEncourage self-care\n\n\n1\nmoderate\nmedicate\nBest to intervene with medication\n\n\n2\ncritical\nintensive_care\nAggressive care needed to avoid death.\n\n\n3\ndead\nnone\nNo further action needed."
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#agenda",
    "href": "workshops/rl/temp/RL_presentation_v0.html#agenda",
    "title": "MUSTAFA ASLAN",
    "section": "Agenda",
    "text": "Agenda\n\nWhat is Reinforcement Learning\nElements of Reinforcement Learning\nCategories of RL methods\n\nModel-based methods \\(\\approx\\) Dynamic Programming\nModel-Free Methods\n\nMonte Carlo\nSarsa\nQ-learning"
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#reinforcement-learning",
    "href": "workshops/rl/temp/RL_presentation_v0.html#reinforcement-learning",
    "title": "MUSTAFA ASLAN",
    "section": "Reinforcement Learning",
    "text": "Reinforcement Learning\n\nReinforcement learning is learning what to do—how to map situations to actions to maximize a numerical reward signal.\nThe learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them."
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#elements-of-reinforcement-learning-1",
    "href": "workshops/rl/temp/RL_presentation_v0.html#elements-of-reinforcement-learning-1",
    "title": "MUSTAFA ASLAN",
    "section": "Elements of Reinforcement Learning",
    "text": "Elements of Reinforcement Learning\n\nAgent is the learner and decision maker.\n\nIt interacts with the environment (comprising everything outside the agent) continually to select actions\n\nThe environment responds to these actions and presents new situations to the agent.\n\nA policy defines the learning agent’s way of behaving at a given time.\n\nA mapping from states of the environment to actions to be taken when in those states.\n\nThe reward signal indicates what is good immediately after each decision time.\nA value function specifies what is good in the long run.\n\nThe value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state."
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#elements-of-reinforcement-learning-2",
    "href": "workshops/rl/temp/RL_presentation_v0.html#elements-of-reinforcement-learning-2",
    "title": "MUSTAFA ASLAN",
    "section": "Elements of Reinforcement Learning",
    "text": "Elements of Reinforcement Learning\n\nThe agent–environment interaction in a decision processAgent gives rise to a sequence that begins like this\n\\[\nS_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \\dots  S_t, A_t, R_{t+1}\n\\]"
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#goals-and-rewards",
    "href": "workshops/rl/temp/RL_presentation_v0.html#goals-and-rewards",
    "title": "MUSTAFA ASLAN",
    "section": "Goals and Rewards",
    "text": "Goals and Rewards\n\nAt each time step, the reward is a simple number, \\(R_t \\in \\mathbb{R}\\) passing from the environment to the agent.\nThe agent’s goal is to maximize the total (cumulative) amount of reward it receives in the long run.\nThe aim is to maximize the expected return, \\(G_t\\), which is defined as: \\[\nG_t \\doteq R_{t+1} + R_{t+2} + R_{t+3}+ \\dots+R_T\n\\]\n\nDiscounting technique is used to prioritize immediate rewards over future rewards.\n\nThe idea is to multiply future rewards by a discount factor \\(\\gamma \\in (0,1]\\)\nThis makes future rewards worth less than immediate rewards.\nThe return \\(G_t\\) with discounting is defined as:\n\n\\[\nG_t \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n\\]"
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#policies-and-value-functions",
    "href": "workshops/rl/temp/RL_presentation_v0.html#policies-and-value-functions",
    "title": "MUSTAFA ASLAN",
    "section": "Policies and Value Functions",
    "text": "Policies and Value Functions\n\nA policy is a mapping from states to probabilities of selecting each possible action. If the agent is following policy \\(\\pi\\) at time \\(t\\), then \\(\\pi(a \\mid s)\\) is the probability that \\(A_t = a\\) if \\(S_t = s\\)\nValue functions—functions of states (or of state–action pairs) estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state)\n\nThe value function of a state \\(s\\) under a policy \\(\\pi\\), denoted \\(v_\\pi(s)\\) is: \\[\nv_\\pi(s) \\doteq \\mathbb{E}_\\pi[G_t | S_t=s] = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t=s\\right], \\text{ for all } s \\in \\mathbf{S},\n\\]\n\n\nwhere \\(\\mathbb{E}[\\cdot]\\) denotes the expected value of a random variable given that the agent follows policy \\(\\pi\\), and \\(t\\) is any time step."
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#categories-of-reinforcement-learning-methods",
    "href": "workshops/rl/temp/RL_presentation_v0.html#categories-of-reinforcement-learning-methods",
    "title": "MUSTAFA ASLAN",
    "section": "Categories of reinforcement learning methods",
    "text": "Categories of reinforcement learning methods\n\n\nModel-based\n\n\nThe agent knows/learns the model of the environment\nThey then compute the policy using the ADP methods or the model-free methods on simulated data\n\nPros:\n\nSample efficient\nSafer exploration\n\nCons:\n\nProne to the model errors\nLearning a model is challenging\n\n\n\nModel-free\n\n\nThe agent does not know the model of the environment\nThey learn the values or policies from trial-and-error interactions with the environment\n\nPros:\n\nDo not need a model\nFlexible\n\nCons:\n\nSample inefficient: requires a lot of interactions with the environment\nSlow convergence"
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#model-based-methods-dynamic-programming",
    "href": "workshops/rl/temp/RL_presentation_v0.html#model-based-methods-dynamic-programming",
    "title": "MUSTAFA ASLAN",
    "section": "Model-based methods (Dynamic Programming)",
    "text": "Model-based methods (Dynamic Programming)\nConditional probability of the next state given the current state and action taken \\[\np(s' \\mid s, a) \\doteq \\Pr \\{ S_t = s' \\mid S_{t-1} = s, A_{t-1} = a \\}, \\text{ for all } s',s \\in S, \\text{and} a \\in A(s)\n\\]\nThe function \\(p\\) defines the dynamics of the MDP\n\\(p(s', \\mid s, a)\\) is a dynamics function and \\(p\\) specifies a probability distribution for each choice of \\(s\\) and \\(a\\), that is: \\[\n\\sum_{s'\\in S}p(s'\\mid s, a) = 1, \\text{ for all } s \\in S, a \\in A(s)\n\\]"
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#model-based-methods-dynamic-programming-1",
    "href": "workshops/rl/temp/RL_presentation_v0.html#model-based-methods-dynamic-programming-1",
    "title": "MUSTAFA ASLAN",
    "section": "Model-based methods (Dynamic Programming)",
    "text": "Model-based methods (Dynamic Programming)\n\nThey are used to derive optimal policies and value functions in Markov Decision Processes (MDPs) and reinforcement learning.\n\nBellman Equation\n\nThe Bellman equations express the relationship between the value of a state and the values of its successor states. For the state-value function \\(v_\\pi\\), the Bellman equation is:\n\n\\[\n\\begin{aligned}\nv_\\pi(s) &\\doteq \\mathbb{E}_\\pi \\big[G_t \\mid S_t = s \\big] \\\\\n         &= \\mathbb{E}_\\pi \\big[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s \\big] \\\\\n         &= \\mathbb{E}_\\pi \\big[R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s \\big] \\\\\n         &= \\sum_a \\pi(a \\mid s) \\sum_{s',r} p(s', r \\mid s, a) \\big[ r + \\gamma v_\\pi(s') \\big].\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#model-based-methods-dynamic-programming-2",
    "href": "workshops/rl/temp/RL_presentation_v0.html#model-based-methods-dynamic-programming-2",
    "title": "MUSTAFA ASLAN",
    "section": "Model-based methods (Dynamic Programming)",
    "text": "Model-based methods (Dynamic Programming)\n\nAn optimal policy is a policy that achieves the maximum expected return from any initial state.\nThe optimal state-value function \\(v_*\\) is the maximum value function over all policies: \\[\nv_*(s) = \\max_\\pi v_\\pi(s), \\text{ for all } s \\in S\n\\]\nThe optimal action-value function \\(q_*\\) is the maximum action-value function over all policies: \\[\nq_*(s, a) = \\max_\\pi q_\\pi(s, a), \\text{ for all } s \\in S, a \\in A(s)\n\\]\n\nWe use Dynamic Programming (DP) to leverage value functions in the search for good policies.\n\nThe Bellman optimality equation for \\(v_*\\) is: \\[\n\\begin{aligned}\nv_*(s) &= \\max_a \\mathbb{E} \\big[R_{t+1} + \\gamma v_*(S_{t+1}) \\mid S_{t} = s, A_{t} = a  \\big] \\\\\n    &= \\max_a \\sum_{s',r}p(s',r|s,a) \\big[r+\\gamma v_*(s')],\n\\end{aligned}\n\\]\nThe Bellman optimality equation for \\(q_*\\) is: \\[\nq_*(s, a) = \\sum_{s', r} p(s', r \\mid s, a) \\big[ r + \\gamma \\max_{a'} q_*(s', a') \\big]\n\\]"
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#policy-evalulation-prediction",
    "href": "workshops/rl/temp/RL_presentation_v0.html#policy-evalulation-prediction",
    "title": "MUSTAFA ASLAN",
    "section": "Policy Evalulation (Prediction)",
    "text": "Policy Evalulation (Prediction)\nPolicy evaluation is the computation of the state-value function \\(v_\\pi\\) for an arbitrary policy \\(\\pi\\). We also refer to it as the *prediction problem. \\[\n\\begin{aligned}\nv_\\pi(s) &\\doteq \\mathbb{E}_\\pi \\big[G_t \\mid S_t = s \\big] \\\\\n         &= \\mathbb{E}_\\pi \\big[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s \\big] \\\\\n         &= \\mathbb{E}_\\pi \\big[R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s \\big] \\\\\n         &= \\sum_a \\pi(a \\mid s) \\sum_{s',r} p(s', r \\mid s, a) \\big[ r + \\gamma v_\\pi(s') \\big].\n\\end{aligned}\n\\]\nwhere \\(\\pi(a|s)\\) is the probability of taking action \\(\\alpha\\) in state \\(s\\) under policy \\(\\pi\\), and the expectations are subscripted by \\(\\pi\\) to indicate that they are conditional on \\(\\pi\\) being followed.\nConsider a sequence of approximate value functions \\(v_0, v_1, v_2,\\dots,\\). Each successive approximation is obtained by using the Bellman equation for \\(v_\\pi\\) as an update rule:\n\\[\n\\begin{aligned}\nv_{k+1}(s) &\\doteq \\mathbb{E}_\\pi \\big[R_{t+1} + \\gamma v_k(S_{t+1}) \\mid S_t = s \\big] \\\\\n         &= \\sum_a \\pi(a \\mid s) \\sum_{s',r} p(s', r \\mid s, a) \\big[ r + \\gamma v_k(s') \\big].\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#policy-evalulation-prediction-1",
    "href": "workshops/rl/temp/RL_presentation_v0.html#policy-evalulation-prediction-1",
    "title": "MUSTAFA ASLAN",
    "section": "Policy Evalulation (Prediction)",
    "text": "Policy Evalulation (Prediction)\nThe sequence \\({v_k}\\) can be shown in general to converge to \\(v_\\pi\\) as \\(k \\rightarrow \\infty\\) under the same conditions that guarantee the existence of \\(v_\\pi\\). This algorithm is called iterative policy evaluation.\n\n\n\nIterative Policy Evaluation, for estimating \\(V \\approx v_\\pi\\)\n\n\nInput:\n\\(\\pi\\), the policy to be evaluated\nAlgorithm parameter: a small threshold \\(\\theta &gt; 0\\) determining accuracy of estimation\nInitialize \\(V(s)\\) arbitrarily, for \\(s \\in S\\), and \\(V(\\text{terminal}) = 0\\)\nLoop:\n\\(\\Delta \\leftarrow 0\\)\nLoop for each \\(s \\in S\\):\n \\(v \\leftarrow V(s)\\)\n \\(V(s) \\leftarrow \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r | s, a) [r + \\gamma V(s')]\\)\n \\(\\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)\\)\nuntil \\(\\Delta &lt; \\theta\\)"
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#policy-improvement",
    "href": "workshops/rl/temp/RL_presentation_v0.html#policy-improvement",
    "title": "MUSTAFA ASLAN",
    "section": "Policy Improvement",
    "text": "Policy Improvement\n\n\n\nWe know how good it is to follow the current policy from \\(s\\)—that is \\(v_\\pi(s)\\)—but would it be better or worse to change to the new policy?\n\nOne way to answer this question is to consider selecting \\(a\\) in \\(s\\) and thereafter following the existing policy \\(\\pi\\).\nThis leads to the definition of the q-value of a state-action pair:\n\\[\n\\begin{aligned}\nq_\\pi(s, a) &\\doteq \\mathbb{E} \\big[ R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s, A_t = a \\big] \\\\\n     &= \\sum_{s', r} p(s', r \\mid s, a) \\big[ r + \\gamma v_\\pi(s') \\big].\n\\end{aligned}\n\\]\n\nThe policy improvement theorem states that if we improve the policy by acting greedily with respect to \\(q_\\pi\\), the new policy \\(\\pi'\\) will be at least as good as \\(\\pi\\).\nFormally, if\n\\[\n\\begin{aligned}\n\\pi'(s) &= \\arg\\max_a q_\\pi(s, a) \\\\\n&= \\arg \\max_a \\mathbb{E} \\big[R_{t+1}+\\gamma v_\\pi(S_{t+1}) \\mid S_t=s, A_t=a \\big] \\\\\n&= \\arg \\max_a \\sum_{s',r}p(s',r \\mid s, a) \\big[r+\\gamma v_\\pi(s') \\big]\n\\end{aligned}\n\\]\nthen\n\\[\nv_{\\pi'}(s) \\geq v_\\pi(s).\n\\]\nfor all \\(s \\in S\\)."
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#policy-iteration",
    "href": "workshops/rl/temp/RL_presentation_v0.html#policy-iteration",
    "title": "MUSTAFA ASLAN",
    "section": "Policy Iteration",
    "text": "Policy Iteration\nOnce a policy, \\(\\pi\\), has been improved using \\(v_\\pi\\) to yield a better policy, \\(\\pi^{'}\\), we can then compute \\(v_{\\pi^{'}}\\) and improve it again to yield an even better \\(\\pi^{''}\\).\n\\[\n\\pi_0 \\xrightarrow E v_{\\pi_0} \\xrightarrow I \\pi_1 \\xrightarrow E v_{\\pi_1}\\xrightarrow I \\pi_2,\\dots \\xrightarrow I \\pi_* \\xrightarrow E v_*\n\\]\nwhere \\(\\xrightarrow E\\) denotes a policy evaluation and \\(\\xrightarrow I\\) denotes a policy improvement. This way of finding an optimal policy is called policy iteration."
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#policy-iteration-1",
    "href": "workshops/rl/temp/RL_presentation_v0.html#policy-iteration-1",
    "title": "MUSTAFA ASLAN",
    "section": "Policy Iteration",
    "text": "Policy Iteration\nA complete policy iteration algorithm\n\n\n\nPolicy Iteration (using iterative policy evaluation) for estimating \\(\\pi \\approx \\pi_*\\)\n\n\n1. Initialization\n\\(V(s) \\in \\mathbb{R}\\) and \\(\\pi(s) \\in A(s)\\) arbitrarily for all \\(s \\in S\\)\n\\(V(\\text{terminal}) \\doteq 0\\)\n2. Policy Evaluation\nLoop:\n \\(\\Delta \\leftarrow 0\\)\n Loop for each \\(s \\in S\\):\n  \\(v \\leftarrow V(s)\\)\n  \\(V(s) \\leftarrow \\sum_{s', r} p(s', r \\mid s, \\pi(s)) \\left[r + \\gamma V(s')\\right]\\)\n  \\(\\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)\\)\nUntil \\(\\Delta &lt; \\theta\\) (a small positive number determining the accuracy of estimation)\n3. Policy Improvement\npolicy-stable \\(\\leftarrow\\) true\nFor each \\(s \\in S\\):\n old-action \\(\\leftarrow \\pi(s)\\)\n \\(\\pi(s) \\leftarrow \\arg\\max_a \\sum_{s', r} p(s', r \\mid s, a)\\left[r + \\gamma V(s')\\right]\\)\n If old-action \\(\\ne \\pi(s)\\), then policy-stable \\(\\leftarrow\\) false\nIf policy-stable, then stop and return \\(V \\approx v_*\\) and \\(\\pi \\approx \\pi_*\\);\nElse go to step 2"
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#value-iteration",
    "href": "workshops/rl/temp/RL_presentation_v0.html#value-iteration",
    "title": "MUSTAFA ASLAN",
    "section": "Value Iteration",
    "text": "Value Iteration\n\nOne drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set.\nValue iteration is a special case of policy iteration where the policy evaluation step is truncated to just one sweep.\nThis algorithm combines the policy improvement and truncated policy evaluation steps into a single update operation:\n\n\\[\n\\begin{aligned}\nv_{k+1}(s) &= \\mathbb{E} \\big[R_{t+1} + \\gamma v_k(S_{t+1}) \\mid S_t=s, A_t = a \\big] \\\\\n       &=\\max_a \\sum_{s', r} p(s', r \\mid s, a) \\big[ r + \\gamma v_k(s') \\big]\n\\end{aligned}\n\\]\nfor all \\(s \\in S\\)."
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#value-iteration-1",
    "href": "workshops/rl/temp/RL_presentation_v0.html#value-iteration-1",
    "title": "MUSTAFA ASLAN",
    "section": "Value Iteration",
    "text": "Value Iteration\n\nValue iteration is obtained simply by turning the Bellman optimality equation into an update rule.\nAlso note how the value iteration update is identical to the policy evaluation update except that it requires the maximum to be taken over all actions.\n\n\n\n\nValue Iteration, for estimating \\(\\pi \\approx \\pi_*\\)\n\n\nAlgorithm parameter:\nA small threshold \\(\\theta &gt; 0\\) determining the accuracy of estimation\nInitialization:\nInitialize \\(V(s)\\) arbitrarily for all \\(s \\in S^+\\), except that \\(V(\\text{terminal}) = 0\\)\nLoop:\n \\(\\Delta \\leftarrow 0\\)\n Loop for each \\(s \\in S\\):\n  \\(v \\leftarrow V(s)\\)\n  \\(V(s) \\leftarrow \\max_a \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma V(s') \\right]\\)\n  \\(\\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)\\)\nuntil \\(\\Delta &lt; \\theta\\)\nOutput:\nA deterministic policy, \\(\\pi \\approx \\pi_*\\), such that\n \\(\\pi(s) = \\arg\\max_a \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma V(s') \\right]\\)"
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#generalized-policy-iteration",
    "href": "workshops/rl/temp/RL_presentation_v0.html#generalized-policy-iteration",
    "title": "MUSTAFA ASLAN",
    "section": "Generalized Policy Iteration",
    "text": "Generalized Policy Iteration\n\n\n\nGPI is a general idea that describes how two processes — evaluating a policy and improving it — work together and influence each other.\nMost reinforcement learning algorithms use a value function to judge how good a policy is, and then update the policy based on that judgment.\nIf both value estimation and policy improvement stabilize (i.e., stop changing), then the policy must be the best possible one for that value function—meaning the policy is optimal."
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#monte-carlo-mc-methods",
    "href": "workshops/rl/temp/RL_presentation_v0.html#monte-carlo-mc-methods",
    "title": "MUSTAFA ASLAN",
    "section": "Monte Carlo (MC) Methods",
    "text": "Monte Carlo (MC) Methods\n\nThe term “Monte Carlo” is often used more broadly for any estimation method whose operation involves a significant random component.\nMC methods solve reinforcement learning problems by averaging results (returns) from sampled experiences sequences of states, actions, and rewards.\nThey do not require knowledge of the environment’s dynamics, making them powerful for learning from real or simulated experiences."
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#monte-carlo-prediction",
    "href": "workshops/rl/temp/RL_presentation_v0.html#monte-carlo-prediction",
    "title": "MUSTAFA ASLAN",
    "section": "Monte Carlo Prediction",
    "text": "Monte Carlo Prediction\n\nSuppose we wish to estimate \\(v_{\\pi}(s)\\), the values of a state \\(s\\) under policy \\(\\pi\\), given a set of episodes obtained by following \\(\\pi\\) and passing through \\(s\\).\n\n\n\n\nFirst-Visit Monte Carlo Prediction (for estimating \\(V \\approx v_\\pi\\))\n\n\nInput:\nA policy \\(\\pi\\) to be evaluated\nInitialize:\n \\(V(s) \\in \\mathbb{R}\\) arbitrarily, for all \\(s \\in S\\)\n \\(\\text{Returns}(s) \\leftarrow\\) an empty list, for all \\(s \\in S\\)\nLoop forever (for each episode):\n Generate an episode following \\(\\pi\\): \\(S_0, A_0, R_1, S_1, A_1, R_2, \\dots, S_{T-1}, A_{T-1}, R_T\\)\n \\(G \\leftarrow 0\\)\n Loop for each step of the episode, \\(t = T-1, T-2, \\dots, 0\\):\n  \\(G \\leftarrow \\gamma G + R_{t+1}\\)\n  Unless \\(S_t\\) appears in \\(S_0, S_1, \\dots, S_{t-1}\\):\n   Append \\(G\\) to \\(\\text{Returns}(S_t)\\)\n   \\(V(S_t) \\leftarrow \\text{average}(\\text{Returns}(S_t))\\)"
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#monte-carlo-control",
    "href": "workshops/rl/temp/RL_presentation_v0.html#monte-carlo-control",
    "title": "MUSTAFA ASLAN",
    "section": "Monte Carlo Control",
    "text": "Monte Carlo Control\nAlternating complete steps of policy evaluation and policy improvement are performed, beginning with an arbitrary policy \\(\\pi_0\\) and ending with the optimal policy and optimal action-value function:\n\\[\n\\pi_0 \\xrightarrow E q_{\\pi_0} \\xrightarrow I \\pi_1 \\xrightarrow E q_{\\pi_1}\\xrightarrow I \\pi_2,\\dots,\\xrightarrow I \\pi_* \\xrightarrow E q_*\n\\]\n\n\n\nMonte Carlo ES (Exploring Starts), for estimating \\(\\pi \\approx \\pi_*\\)\n\n\nInitialize:\n\\(\\pi(s) \\in A(s)\\) arbitrarily, for all \\(s \\in S\\)\n\\(Q(s, a) \\in \\mathbb{R}\\) arbitrarily, for all \\(s \\in S, a \\in A(s)\\)\n\\(\\text{Returns}(s, a) \\leftarrow\\) empty list, for all \\(s \\in S, a \\in A(s)\\)\nLoop forever (for each episode):\n Choose \\(S_0 \\in S, A_0 \\in A(S_0)\\) randomly, such that all pairs have probability \\(&gt; 0\\)\n Generate an episode from \\(S_0, A_0\\), following \\(\\pi\\):\n  \\(S_0, A_0, R_1, \\dots, S_{T-1}, A_{T-1}, R_T\\)\n \\(G \\leftarrow 0\\)\n Loop for each step of episode, \\(t = T-1, T-2, \\dots, 0\\):\n  \\(G \\leftarrow \\gamma G + R_{t+1}\\)\n  Unless the pair \\((S_t, A_t)\\) appears in\n   \\(S_0, A_0, S_1, A_1, \\dots, S_{t-1}, A_{t-1}\\):\n   Append \\(G\\) to \\(\\text{Returns}(S_t, A_t)\\)\n   \\(Q(S_t, A_t) \\leftarrow \\text{average}(\\text{Returns}(S_t, A_t))\\)\n   \\(\\pi(S_t) \\leftarrow \\arg\\max_a Q(S_t, a)\\)"
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#monte-carlo-control-without-exploring-starts",
    "href": "workshops/rl/temp/RL_presentation_v0.html#monte-carlo-control-without-exploring-starts",
    "title": "MUSTAFA ASLAN",
    "section": "Monte Carlo Control without Exploring Starts",
    "text": "Monte Carlo Control without Exploring Starts\n\nIn on-policy control methods the policy is generally soft, meaning that \\(\\pi(a\\mid s)&gt;0\\) for all \\(s \\in S\\) and all \\(a \\in A(s)\\), but gradually shifted closer and closer to a deterministic policy.\nThe on-policy method we present in this section uses \\(\\epsilon\\)-greedy policies, meaning that most of the time they choose an action that has maximal estimated action value, but with probability \\(\\epsilon\\) they instead select an action at random. That is, all nongreedy actions are given the minimal probability of selection, \\(\\frac{\\epsilon}{|A(s)|}\\), and the remaining bulk of the probability \\(1-\\epsilon+\\frac{\\epsilon}{|A(s)|}\\) is given to the greedy action.\n\\(\\epsilon\\)-greedy policies are examples of \\(\\epsilon-soft\\) policies, definied as policies for which \\(\\pi(a \\mid s) \\geq \\frac{\\epsilon}{|A(s)|}\\) for all states and actions, for some \\(\\epsilon &gt; 0\\). Among \\(\\epsilon\\)-soft policies, \\(\\epsilon\\)-greedy policies are in some sense those that are closest to greedy."
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#monte-carlo-control-without-exploring-starts-1",
    "href": "workshops/rl/temp/RL_presentation_v0.html#monte-carlo-control-without-exploring-starts-1",
    "title": "MUSTAFA ASLAN",
    "section": "Monte Carlo Control without Exploring Starts",
    "text": "Monte Carlo Control without Exploring Starts\n\n\n\nOn-policy First-Visit MC Control (for \\(\\varepsilon\\)-soft policies), estimates \\(\\pi \\approx \\pi_*\\)\n\n\nAlgorithm parameter:\nSmall \\(\\varepsilon &gt; 0\\)\nInitialize:\n\\(\\pi \\leftarrow\\) an arbitrary \\(\\varepsilon\\)-soft policy\n\\(Q(s, a) \\in \\mathbb{R}\\) arbitrarily, for all \\(s \\in S, a \\in A(s)\\)\n\\(\\text{Returns}(s, a) \\leftarrow\\) empty list, for all \\(s \\in S, a \\in A(s)\\)\nRepeat forever (for each episode):\n Generate an episode following \\(\\pi\\): \\(S_0, A_0, R_1, \\dots, S_{T-1}, A_{T-1}, R_T\\)\n \\(G \\leftarrow 0\\)\n Loop for each step of episode, \\(t = T-1, T-2, \\dots, 0\\):\n  \\(G \\leftarrow \\gamma G + R_{t+1}\\)\n  Unless the pair \\((S_t, A_t)\\) appears in \\(S_0, A_0, \\dots, S_{t-1}, A_{t-1}\\):\n   Append \\(G\\) to \\(\\text{Returns}(S_t, A_t)\\)\n   \\(Q(S_t, A_t) \\leftarrow \\text{average}(\\text{Returns}(S_t, A_t))\\)\n   \\(A^* \\leftarrow \\arg\\max_a Q(S_t, a)\\) (ties broken arbitrarily)\n   For all \\(a \\in A(S_t)\\):\n    \\(\\pi(a \\mid S_t) \\leftarrow \\begin{cases}\n1 - \\varepsilon + \\varepsilon / |A(S_t)| & \\text{if } a = A^* \\\\\n\\varepsilon / |A(S_t)| & \\text{if } a \\ne A^*\n\\end{cases}\\)\n\n\n\nwhere \\(|A(s)|\\) is the number of actions available in state \\(s\\).\nThe \\(\\epsilon\\)-greedy policy ensures that all actions are tried, but actions with higher value estimates are tried more frequently. This balances exploration (trying new actions) and exploitation (choosing the best-known action)."
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#temporal-difference-learning-1",
    "href": "workshops/rl/temp/RL_presentation_v0.html#temporal-difference-learning-1",
    "title": "MUSTAFA ASLAN",
    "section": "Temporal-Difference Learning",
    "text": "Temporal-Difference Learning\n\nTD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas.\nLike Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment’s dynamics.\nLike DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap)."
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#td-prediction",
    "href": "workshops/rl/temp/RL_presentation_v0.html#td-prediction",
    "title": "MUSTAFA ASLAN",
    "section": "TD Prediction",
    "text": "TD Prediction\nWhereas Monte Carlo methods must wait until the end of the episode to determine the increment to \\(V(St)\\) (only then is \\(G_t\\) known), TD methods need to wait only until the next time step. At time t + 1 they immediately form a target and make a useful update using the observed reward \\(R_{t+1}\\) and the estimate \\(V_{S_{t+1}}\\). The simplest TD method makes the update:\n\\[\nV(S_t) \\leftarrow V(S_t) + \\alpha \\big[ R_{t+1} + \\gamma V(S_{t+1})-V(S_t)  \\big]\n\\]\nimmediately on transition to \\(S_{t+1}\\) and receiving \\(R_{t+1}\\). In effect, the target for the Monte Carlo update is \\(G_t\\), whereas the target for the TD update is \\(R_{t+1} + \\gamma V(S_{t+1})\\). This TD method is called TD(0), or one-step TD."
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#td-prediction-1",
    "href": "workshops/rl/temp/RL_presentation_v0.html#td-prediction-1",
    "title": "MUSTAFA ASLAN",
    "section": "TD Prediction",
    "text": "TD Prediction\n\n\n\nTabular TD(0) for Estimating \\(v_\\pi\\)\n\n\nInput:\nThe policy \\(\\pi\\) to be evaluated\nAlgorithm parameter:\nStep size \\(\\alpha \\in (0, 1]\\)\nInitialize:\n\\(V(s)\\) arbitrarily for all \\(s \\in S^+\\), except that \\(V(\\text{terminal}) = 0\\)\nLoop for each episode:\n Initialize \\(S\\)\n Loop for each step of episode:\n  \\(A \\leftarrow\\) action given by \\(\\pi\\) for \\(S\\)\n  Take action \\(A\\), observe \\(R\\), \\(S'\\)\n  \\(V(S) \\leftarrow V(S) + \\alpha \\left[ R + \\gamma V(S') - V(S) \\right]\\)\n  \\(S \\leftarrow S'\\)\n Until \\(S\\) is terminal"
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#sarsa-on-policy-td-control",
    "href": "workshops/rl/temp/RL_presentation_v0.html#sarsa-on-policy-td-control",
    "title": "MUSTAFA ASLAN",
    "section": "Sarsa: On-policy TD Control",
    "text": "Sarsa: On-policy TD Control\nwe consider transitions from state–action pair to state–action pair, and learn the values of state–action pairs.\n\\[\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\big[R_{t+1}+\\gamma Q(S_{t+1}, A_{t+1})-Q(S_t, A_t) \\big]\n\\]\n\n\n\nSARSA (On-Policy TD Control), for estimating \\(Q \\approx q_*\\)\n\n\nAlgorithm parameters:\nStep size \\(\\alpha \\in (0, 1]\\), small \\(\\varepsilon &gt; 0\\)\nInitialize:\n\\(Q(s, a)\\) arbitrarily, for all \\(s \\in S^+\\), \\(a \\in A(s)\\)\n\\(Q(\\text{terminal}, \\cdot) = 0\\)\nLoop for each episode:\n Initialize \\(S\\)\n Choose \\(A\\) from \\(S\\) using a policy derived from \\(Q\\) (e.g., \\(\\varepsilon\\)-greedy)\n Loop for each step of episode:\n  Take action \\(A\\), observe \\(R\\), \\(S'\\)\n  Choose \\(A'\\) from \\(S'\\) using a policy derived from \\(Q\\) (e.g., \\(\\varepsilon\\)-greedy)\n  \\(Q(S, A) \\leftarrow Q(S, A) + \\alpha \\left[ R + \\gamma Q(S', A') - Q(S, A) \\right]\\)\n  \\(S \\leftarrow S';\\quad A \\leftarrow A'\\)\nUntil \\(S\\) is terminal"
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#q-learning-off-policy-td-control",
    "href": "workshops/rl/temp/RL_presentation_v0.html#q-learning-off-policy-td-control",
    "title": "MUSTAFA ASLAN",
    "section": "Q-learning: Off-policy TD Control",
    "text": "Q-learning: Off-policy TD Control\nQ-learning is defined by\n\\[\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t)+ \\alpha \\big[R_{t+1}+\\gamma \\max_aQ(S_{t+1}, a)-Q(S_t, A_t)  \\big]\n\\]\nThe Q-learning algorithm is shown below in procedural form.\n\n\n\nQ-learning (Off-Policy TD Control), for estimating \\(\\pi \\approx \\pi_*\\)\n\n\nAlgorithm parameters:\nStep size \\(\\alpha \\in (0, 1]\\), small \\(\\varepsilon &gt; 0\\)\nInitialize:\n\\(Q(s, a)\\) arbitrarily for all \\(s \\in S^+\\), \\(a \\in S(s)\\),\nexcept that \\(Q(\\text{terminal}, \\cdot) = 0\\)\nLoop for each episode:\n Initialize \\(S\\)\n Loop for each step of episode:\n  Choose \\(A\\) from \\(S\\) using a policy derived from \\(Q\\) (e.g., \\(\\varepsilon\\)-greedy)\n  Take action \\(A\\), observe \\(R\\), \\(S'\\)\n  \\(Q(S, A) \\leftarrow Q(S, A) + \\alpha \\left[ R + \\gamma \\max_a Q(S', a) - Q(S, A) \\right]\\)\n  \\(S \\leftarrow S'\\)\nUntil \\(S\\) is terminal"
  },
  {
    "objectID": "workshops/rl/temp/RL_presentation_v0.html#recommended-materials",
    "href": "workshops/rl/temp/RL_presentation_v0.html#recommended-materials",
    "title": "MUSTAFA ASLAN",
    "section": "Recommended materials",
    "text": "Recommended materials\nReadings:\n\nReinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto\nReinforcement Learning and Stochastic Optimization by Warren B. Powell\n\nTutoriols:"
  },
  {
    "objectID": "workshops/intro_python/intro_to_python_part_3.html#outline",
    "href": "workshops/intro_python/intro_to_python_part_3.html#outline",
    "title": "MUSTAFA ASLAN",
    "section": "Outline",
    "text": "Outline\n\nIntroduction to VS\nInstalling and Importing Libraries in Python\nGetting Help in Python\nIntroduction to Quarto"
  },
  {
    "objectID": "workshops/intro_python/intro_to_python_part_3.html#introduction-to-vs-code-1",
    "href": "workshops/intro_python/intro_to_python_part_3.html#introduction-to-vs-code-1",
    "title": "MUSTAFA ASLAN",
    "section": "Introduction to VS Code",
    "text": "Introduction to VS Code\n\n\nWhat is VS Code\n\nA free, and open-source code editor developed by Microsoft.\nSupports multiple programming languages and frameworks.\n\n\nKey Features\n\nIntegrated Terminal: Access a terminal directly in VS Code for running commands.\nExtensions: Add functionality for specific languages, tools, or frameworks (e.g., Quarto, Python, R).\nSource Control: Built-in Git integration for version control.\nDebugging: Powerful debugging tools for various languages.\nCustomizable Interface: Themes, layout, and key bindings to suit user preferences."
  },
  {
    "objectID": "workshops/intro_python/intro_to_python_part_3.html#what-is-library",
    "href": "workshops/intro_python/intro_to_python_part_3.html#what-is-library",
    "title": "MUSTAFA ASLAN",
    "section": "What is library?",
    "text": "What is library?\n\nA collection of pre-written code that contains functions and modules that developers can use to solve programming tasks\nPython libraries cover libraries for a wide range of tasks, including:\n\npandas and numpy for data analysis, cleaning, exploration, and efficient numerical computations.\nscikit-learn, TensorFlow for machine learning tasks such as prediction, clustering, and forecasting.\nstatsforecast, darts and skforecast for forecasting tasks"
  },
  {
    "objectID": "workshops/intro_python/intro_to_python_part_3.html#what-is-library-1",
    "href": "workshops/intro_python/intro_to_python_part_3.html#what-is-library-1",
    "title": "MUSTAFA ASLAN",
    "section": "What is library?",
    "text": "What is library?\n\n\nWhat is PIP?\n\nPIP is a package manager for Python packages or modules\nPIP is used to install and manage Python libraries\n\n\nInstalling a Library\n\n\nOpen a terminal or command prompt.\n\n\nUse the command pip install &lt;library-name&gt;.\n\n\nExample: pip install numpy\n\n\nAlternatively, run the following code in a notebook to install libraries directly: python       !pip install numpy"
  },
  {
    "objectID": "workshops/intro_python/intro_to_python_part_3.html#importing-libraries-or-modules",
    "href": "workshops/intro_python/intro_to_python_part_3.html#importing-libraries-or-modules",
    "title": "MUSTAFA ASLAN",
    "section": "Importing libraries or modules",
    "text": "Importing libraries or modules\n\nOnce a library is installed, you need to import it into your Python script:\n\nExample:\nimport numpy\n\nUse as to create an alias for convenience (e.g., import pandas as pd or import numpy as np).\nUse from … import … to call specific functions from libraries or modules :\n\nExample:\nfrom math import sqrt\nprint(sqrt(16))  # Output: 4.0"
  },
  {
    "objectID": "workshops/intro_python/intro_to_python_part_3.html#namespaces",
    "href": "workshops/intro_python/intro_to_python_part_3.html#namespaces",
    "title": "MUSTAFA ASLAN",
    "section": "Namespaces",
    "text": "Namespaces\n\n\n\nA namespace is a system that has a unique name for each and every object in Python.\nAn object might be a variable or a method.\nSome functions like print(), id() are always present, these are built-in namespaces.\nWhen a user creates a module, a global namespace gets created, later the creation of local functions creates the local namespace.\n\n\nExample:\nx = \"global\"\n\ndef outer_function():\n    # Enclosing namespace\n    x = \"enclosing\"\n    \n    def inner_function():\n        # Local namespace\n        x = \"local\"\n        print(\"Inner:\", x)  # Prints: Inner: local\n\n    inner_function()\n    print(\"Outer:\", x)  # Prints: Outer: enclosing\n\nouter_function()\nprint(\"Global:\", x)  # Prints: Global: global"
  },
  {
    "objectID": "workshops/intro_python/intro_to_python_part_3.html#using-built-in-help-functions",
    "href": "workshops/intro_python/intro_to_python_part_3.html#using-built-in-help-functions",
    "title": "MUSTAFA ASLAN",
    "section": "Using built-in help functions",
    "text": "Using built-in help functions\n\nPython provides built-in tools to explore libraries and and functions\nSome of them are:\n\nhelp(): Displays documentation for an object.\ndir(): Lists the attributes and methods of an object\ntype(): Displays the type of the object"
  },
  {
    "objectID": "workshops/intro_python/intro_to_python_part_3.html#using-built-in-help-functions-1",
    "href": "workshops/intro_python/intro_to_python_part_3.html#using-built-in-help-functions-1",
    "title": "MUSTAFA ASLAN",
    "section": "Using built-in help functions",
    "text": "Using built-in help functions\nExamples:\n\n\nExample -1:\n\nprint(help(len))\n# output:\n\nHelp on built-in function len in module builtins:\n\nlen(obj, /)\n    Return the number of items in a container.\n\nNone\n\n\n\nExample -2:\n\nprint(dir(list))\n# output:\n\n['__add__', '__class__', '__class_getitem__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']\n\n\n\nExample -3:\n\nx = \"cardiff\"\ny = 5\nz = [1,2,3,4,5]\n\nprint(type(x))\nprint(type(y))\nprint(type(z))\n# output:\n\n&lt;class 'str'&gt;\n&lt;class 'int'&gt;\n&lt;class 'list'&gt;"
  },
  {
    "objectID": "workshops/intro_python/intro_to_python_part_3.html#three-main-tasks-using-quarto",
    "href": "workshops/intro_python/intro_to_python_part_3.html#three-main-tasks-using-quarto",
    "title": "MUSTAFA ASLAN",
    "section": "Three Main Tasks Using Quarto",
    "text": "Three Main Tasks Using Quarto\nQuarto help us publish reproducible, production quality articles, presentations, dashboards, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more.\n\nWriting: Combine Markdown, code, and visualizations in .qmd files.\nRender: Generate output formats like HTML, PDF, or slides.\nPublish: Share documents or host on platforms like GitHub."
  },
  {
    "objectID": "workshops/intro_python/intro_to_python_part_3.html#how-to-use-quarto-in-vs-code",
    "href": "workshops/intro_python/intro_to_python_part_3.html#how-to-use-quarto-in-vs-code",
    "title": "MUSTAFA ASLAN",
    "section": "How to Use Quarto in VS Code",
    "text": "How to Use Quarto in VS Code\n\n\n1. Install the Quarto Extension\n\nOpen VS Code.\nGo to the Extensions view by clicking on the Extensions icon in the Activity Bar on the side of the window or press Ctrl+Shift+X.\nSearch for “Quarto”.\nInstall the Quarto extension on you vs code\n\n\n2. Create a Quarto Document\n\nGo to File &gt; New File.\nSave the file with a .qmd extension (e.g., example.qmd).\nStart writing your Quarto content.\n\n\n3. Render a Quarto Document\n\nTo render and preview, execute the Quarto: “Preview” command.\nIf you want to preview a different format, use the Quarto: Preview Format command and select the format you want to render to (e.g., HTML, PDF, DOCX).\nQuarto will render the document, and the output file will be saved in the same directory."
  },
  {
    "objectID": "workshops/intro_python/intro_to_python_part_3.html#example--1",
    "href": "workshops/intro_python/intro_to_python_part_3.html#example--1",
    "title": "MUSTAFA ASLAN",
    "section": "Example -1",
    "text": "Example -1\nHere’s an example of creating a bar plot using Seaborn package with a sample dataset:\n\n\n# Import required libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Sample data\ndata = {\n    \"Category\": [\"A\", \"B\", \"C\", \"D\", \"E\"],\n    \"Values\": [23, 45, 56, 78, 12]\n}\n\n# Convert to a DataFrame\nimport pandas as pd\ndf = pd.DataFrame(data).sort_values(by = \"Values\", ascending=False)\n\n# Create a bar plot\nsns.barplot(x=\"Category\", y=\"Values\", data=df, palette=\"viridis\")\n\n# Add a title and labels\nplt.title(\"Sample Bar Plot\")\nplt.xlabel(\"Category\")\nplt.ylabel(\"Values\")\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "workshops/intro_python/intro_to_python_part_3.html#example--2",
    "href": "workshops/intro_python/intro_to_python_part_3.html#example--2",
    "title": "MUSTAFA ASLAN",
    "section": "Example -2",
    "text": "Example -2\nAn example of creating a line graph using plotly package\n\nimport plotly.express as px\n\ndf = px.data.gapminder().query(\"country=='Canada'\")\nfig = px.line(df, x=\"year\", y=\"lifeExp\", title='Life expectancy in Canada')\nfig.show()"
  },
  {
    "objectID": "workshops/intro_python/intro_to_python_part_3.html#exploring-python-documentation-and-community-forums",
    "href": "workshops/intro_python/intro_to_python_part_3.html#exploring-python-documentation-and-community-forums",
    "title": "MUSTAFA ASLAN",
    "section": "Exploring Python documentation and community forums",
    "text": "Exploring Python documentation and community forums\n\n\nWebsites\n\nw3schools\ndatacamp\ncodeacademy\nlearnpython\n\n\nForums\n\nStack Overflow\nReal Python"
  },
  {
    "objectID": "workshops/trainings/py_index.html",
    "href": "workshops/trainings/py_index.html",
    "title": "Basics of Python and Introduction to Quarto in Visual Studio Code",
    "section": "",
    "text": "Slides\nDate: Mar 3 2025 11:00 AM – 13:00 PM\nEvent: Data Lab for Social Good Training Session\nLocation: Aberconway Building Meeting Room, Cardiff University, UK"
  },
  {
    "objectID": "learn/distributions/dists_notebook.html",
    "href": "learn/distributions/dists_notebook.html",
    "title": "Random Variables",
    "section": "",
    "text": "For a discrete random variable \\(X\\), we define the probability mass function \\(p(\\alpha)\\) of \\(X\\) by\n\\[\np(\\alpha) = P(X = \\alpha)\n\\]\nThe probability mass function \\(p(\\alpha)\\) is positive for at most a countable number of values of \\(a\\). That is, if \\(X\\) must assume one of the values \\(x_1, x_2,\\dots ,\\) then\n\\[\np(x_i) \\geq 0 \\text{ for } i = 1, 2, \\dots\n\\]\nand\n\\[\n\\sum_{i=1}^{\\infty} p(x_i) = 1\n\\]\nThe cumulative distribution function \\(F(x)\\) can be expressed in terms of \\(p(\\alpha)\\) by\n\\[\nF(\\alpha) = P(X \\leq \\alpha) = \\sum_{x_i \\leq \\alpha} p(x_i)\n\\]\n\n\nIf \\(X\\) is a discrete random variable having a probability mass function \\(p(x)\\), then the expected value of \\(X\\) is defined by\n\\[\nE(X) = \\sum_{x_i} x_i p(x_i)\n\\]\n\n\n\nA Bernoulli random variable is a discrete random variable that takes the value 1 with probability \\(p\\) and the value 0 with probability \\(1-p\\). The probability mass function of a Bernoulli random variable \\(X\\) is given by \\[\np(x) = \\begin{cases}\n1-p & \\text{if } x = 0 \\\\\np & \\text{if } x = 1\n\\end{cases}\n\\]\nThe expected value of a Bernoulli random variable is given by \\[\nE(X) = 0 \\cdot (1-p) + 1 \\cdot p = p\n\\]\nThe variance of a Bernoulli random variable is given by \\[\nVar(X) = E(X^2) - (E(X))^2 = p - p^2 = p(1-p)\n\\]\n\n# Necessary packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# from scipy.stats import poisson\n\n\ndef bernoulli(p):\n    \"\"\"\n    Bernoulli distribution with parameter p.\n    \"\"\"\n    return np.random.binomial(1, p)\ndef binomial(n, p):\n    \"\"\"\n    Binomial distribution with parameters n and p.\n    \"\"\"\n    return np.random.binomial(n, p)\ndef geometric(p):\n    \"\"\"\n    Geometric distribution with parameter p.\n    \"\"\"\n    return np.random.geometric(p)\ndef negative_binomial(n, p):\n    \"\"\"\n    Negative binomial distribution with parameters n and p.\n    \"\"\"\n    return np.random.negative_binomial(n, p)\n\n\n\n\nSuppose that \\(n\\) independent trials, each of which results in a “success” with probability \\(p\\) and in a “failure” with probability \\(1−p\\), are to be performed. If \\(X\\) represents the number of successes that occur in the \\(n\\) trials, then \\(X\\) is said to be a binomial random variable with parameters \\(n, p\\). The probability mass function of a binomial random variable having parameters \\((n, p)\\) is given by\n\\[\np(x) = \\binom{n}{x} p^x (1-p)^{n-x}, \\quad x = 0, 1, \\dots, n\n\\]\nwhere \\(\\binom{n}{x} = \\frac{n!}{x!(n-x)!}\\) is the binomial coefficient, which counts the number of ways to choose \\(x\\) successes from \\(n\\) trials.\n\n\nIf \\(X\\) is a binomial random variable with parameters \\(n\\) and \\(p\\), then the expected value of \\(X\\) is given by \\[\nE(X) = np\n\\] ### Variance of a Binomial Random Variable If \\(X\\) is a binomial random variable with parameters \\(n\\) and \\(p\\), then the variance of \\(X\\) is given by \\[\nVar(X) = np(1-p)\n\\]\n\n\n\n\nA random variable \\(X\\) taking on one of the values \\(0, 1, 2, \\dots,\\) is said to be a Poisson random variable with parameter \\(\\lambda\\), if for some \\(\\lambda &gt; 0\\),\n\\[\np(i) = P \\{ X = i \\} = e^{-\\lambda} \\frac{\\lambda^i}{i!}, \\quad i = 0, 1, \\ldots\n\\]\n\n\nIf \\(X\\) is a Poisson random variable with parameter \\(\\lambda\\), then the expected value of \\(X\\) is given by\n\\[\nE(X) = \\sum_{i=0}^{\\infty} i e^{-\\lambda} \\frac{\\lambda^i}{i!}\n= e^{-\\lambda} \\sum_{i=1}^{\\infty} \\frac{\\lambda^i}{(i-1)!}\n= e^{-\\lambda} \\lambda \\sum_{i=0}^{\\infty} \\frac{\\lambda^i}{i!}\n= e^{-\\lambda} \\lambda e^{\\lambda}\n= \\lambda\n\\]\nwhere we have used the identity \\(\\sum_{i=0}^{\\infty} \\frac{\\lambda^i}{i!} = e^{\\lambda}\\).\n\ndef poisson(m, k):\n    \"\"\"\n    Calculate the Poisson probability of observing k events in an interval\n    given the average rate of occurrence n.\n\n    :param m: Average rate of occurrence (lambda)\n    :param k: Number of events\n    :return: Probability of observing k events\n    \"\"\"\n    from math import exp, factorial\n\n    return (m ** k) * exp(-m) / factorial(k)\n\n\n# Example usage\nx = np.arange(0, 20) # Average rate of occurrence\nlambdas = [1, 3, 5, 7, 10, 12,15] # Different values of lambda\n\n           \nplt.figure(figsize=(8, 6))\n\nfor lam in lambdas:\n    pmf = [poisson(lam, k) for k in x]\n    # Using scipy's poisson distribution for comparison\n    # pmf_scy = poisson.pmf(x, lam)\n    plt.plot(x, pmf, marker='o', label=f'λ = {lam}')\n\nplt.xlabel('Number of events (k)')\nplt.ylabel('Probability')\nplt.title('Poisson Distribution')\nplt.grid(axis=  \"y\" , linestyle='--')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "learn/distributions/dists_notebook.html#bernoulli-random-variable",
    "href": "learn/distributions/dists_notebook.html#bernoulli-random-variable",
    "title": "Random Variables",
    "section": "",
    "text": "A Bernoulli random variable is a discrete random variable that takes the value 1 with probability \\(p\\) and the value 0 with probability \\(1-p\\). The probability mass function of a Bernoulli random variable \\(X\\) is given by \\[\np(x) = \\begin{cases}\n1-p & \\text{if } x = 0 \\\\\np & \\text{if } x = 1\n\\end{cases}\n\\]\nThe expected value of a Bernoulli random variable is given by \\[\nE(X) = 0 \\cdot (1-p) + 1 \\cdot p = p\n\\]\nThe variance of a Bernoulli random variable is given by \\[\nVar(X) = E(X^2) - (E(X))^2 = p - p^2 = p(1-p)\n\\]\n\n# Necessary packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# from scipy.stats import poisson\n\n\ndef bernoulli(p):\n    \"\"\"\n    Bernoulli distribution with parameter p.\n    \"\"\"\n    return np.random.binomial(1, p)\ndef binomial(n, p):\n    \"\"\"\n    Binomial distribution with parameters n and p.\n    \"\"\"\n    return np.random.binomial(n, p)\ndef geometric(p):\n    \"\"\"\n    Geometric distribution with parameter p.\n    \"\"\"\n    return np.random.geometric(p)\ndef negative_binomial(n, p):\n    \"\"\"\n    Negative binomial distribution with parameters n and p.\n    \"\"\"\n    return np.random.negative_binomial(n, p)"
  },
  {
    "objectID": "learn/distributions/dists_notebook.html#binomial-random-variable",
    "href": "learn/distributions/dists_notebook.html#binomial-random-variable",
    "title": "Random Variables",
    "section": "",
    "text": "Suppose that \\(n\\) independent trials, each of which results in a “success” with probability \\(p\\) and in a “failure” with probability \\(1−p\\), are to be performed. If \\(X\\) represents the number of successes that occur in the \\(n\\) trials, then \\(X\\) is said to be a binomial random variable with parameters \\(n, p\\). The probability mass function of a binomial random variable having parameters \\((n, p)\\) is given by\n\\[\np(x) = \\binom{n}{x} p^x (1-p)^{n-x}, \\quad x = 0, 1, \\dots, n\n\\]\nwhere \\(\\binom{n}{x} = \\frac{n!}{x!(n-x)!}\\) is the binomial coefficient, which counts the number of ways to choose \\(x\\) successes from \\(n\\) trials.\n\n\nIf \\(X\\) is a binomial random variable with parameters \\(n\\) and \\(p\\), then the expected value of \\(X\\) is given by \\[\nE(X) = np\n\\] ### Variance of a Binomial Random Variable If \\(X\\) is a binomial random variable with parameters \\(n\\) and \\(p\\), then the variance of \\(X\\) is given by \\[\nVar(X) = np(1-p)\n\\]"
  },
  {
    "objectID": "learn/distributions/dists_notebook.html#the-poisson-random-variable",
    "href": "learn/distributions/dists_notebook.html#the-poisson-random-variable",
    "title": "Random Variables",
    "section": "",
    "text": "A random variable \\(X\\) taking on one of the values \\(0, 1, 2, \\dots,\\) is said to be a Poisson random variable with parameter \\(\\lambda\\), if for some \\(\\lambda &gt; 0\\),\n\\[\np(i) = P \\{ X = i \\} = e^{-\\lambda} \\frac{\\lambda^i}{i!}, \\quad i = 0, 1, \\ldots\n\\]\n\n\nIf \\(X\\) is a Poisson random variable with parameter \\(\\lambda\\), then the expected value of \\(X\\) is given by\n\\[\nE(X) = \\sum_{i=0}^{\\infty} i e^{-\\lambda} \\frac{\\lambda^i}{i!}\n= e^{-\\lambda} \\sum_{i=1}^{\\infty} \\frac{\\lambda^i}{(i-1)!}\n= e^{-\\lambda} \\lambda \\sum_{i=0}^{\\infty} \\frac{\\lambda^i}{i!}\n= e^{-\\lambda} \\lambda e^{\\lambda}\n= \\lambda\n\\]\nwhere we have used the identity \\(\\sum_{i=0}^{\\infty} \\frac{\\lambda^i}{i!} = e^{\\lambda}\\).\n\ndef poisson(m, k):\n    \"\"\"\n    Calculate the Poisson probability of observing k events in an interval\n    given the average rate of occurrence n.\n\n    :param m: Average rate of occurrence (lambda)\n    :param k: Number of events\n    :return: Probability of observing k events\n    \"\"\"\n    from math import exp, factorial\n\n    return (m ** k) * exp(-m) / factorial(k)\n\n\n# Example usage\nx = np.arange(0, 20) # Average rate of occurrence\nlambdas = [1, 3, 5, 7, 10, 12,15] # Different values of lambda\n\n           \nplt.figure(figsize=(8, 6))\n\nfor lam in lambdas:\n    pmf = [poisson(lam, k) for k in x]\n    # Using scipy's poisson distribution for comparison\n    # pmf_scy = poisson.pmf(x, lam)\n    plt.plot(x, pmf, marker='o', label=f'λ = {lam}')\n\nplt.xlabel('Number of events (k)')\nplt.ylabel('Probability')\nplt.title('Poisson Distribution')\nplt.grid(axis=  \"y\" , linestyle='--')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "learn/distributions/dists_notebook.html#exponential-random-variables",
    "href": "learn/distributions/dists_notebook.html#exponential-random-variables",
    "title": "Random Variables",
    "section": "2.1 Exponential Random Variables",
    "text": "2.1 Exponential Random Variables\nA continuous random variable whose probability density function is given, for some \\(\\lambda &gt; 0\\), by: \\[\nf(x) = \\begin{cases}\n\\lambda e^{-\\lambda x} & x \\geq 0 \\\\\n0 & x &lt; 0\n\\end{cases}\n\\]\nis said to be an exponential random variable with parameter \\(\\lambda\\), which is the mean of the distribution. The cumulative distribution function is given by\n\\[\nF(\\alpha) =  \\int_0^\\alpha \\lambda e^{-\\lambda x} \\, dx, = 1-e^{-\\lambda \\alpha}  \\quad \\alpha \\geq 0\n\\]\nNote that \\(F(\\infty) = \\int_0^\\infty \\lambda e^{-\\lambda x} \\, dx = 1\\).\n\nExpectation of an Exponential Random Variable\nIf \\(X\\) is an exponential random variable with parameter \\(\\lambda\\), then the expected value of \\(X\\) is given by \\[\nE(X) = \\int_0^\\infty x \\lambda e^{-\\lambda x} \\, dx\n\\]\nIntegration by parts (\\(dv = \\lambda e^{-\\lambda x}, u = x\\)) yields \\[\nE(X) = \\left[ -\\frac{x}{\\lambda} e^{-\\lambda x} \\right]_0^\\infty + \\frac{1}{\\lambda} \\int_0^\\infty e^{-\\lambda x} \\, dx\n= 0 + \\frac{1}{\\lambda} \\cdot \\frac{1}{\\lambda} = \\frac{1}{\\lambda}\n\\]\n\ndef exponential(x, m):\n    \"\"\"\n    Calculate the Exponential probability density function (PDF) for a given x and rate parameter m.\n\n    :param x: Value at which to evaluate the PDF\n    :param m: Rate parameter (1/lambda)\n    :return: Probability density at x\n    \"\"\"\n    from math import exp\n\n    return m * exp(-m * x) if x &gt;= 0 else 0\n\n\nx = np.linspace(0, 10, 50)\nlamdas = [0.1, 0.5, 1, 3, 5] # Different values of lambda\n\nplt.figure(figsize=(8, 6))# Plotting the Exponential distribution\nfor lam in lamdas:\n    pdf = [exponential(i, lam) for i in x]\n    plt.plot(x, pdf, marker='o', label=f'λ = {lam}')\nplt.xlabel('x')\nplt.ylabel('Probability Density')\nplt.title('Exponential Distribution')\nplt.grid(axis=  \"y\" , linestyle='--')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "learn/distributions/dists_notebook.html#gamma-random-variables",
    "href": "learn/distributions/dists_notebook.html#gamma-random-variables",
    "title": "Random Variables",
    "section": "2.2 Gamma Random Variables",
    "text": "2.2 Gamma Random Variables\nA continuous random variable whose density is given by: \\[\nf(x) = \\begin{cases}\n\\frac{\\lambda e^{- \\lambda x} {(\\lambda x)}^{\\alpha-1}}{\\Gamma (\\alpha)} & x \\geq 0 \\\\\n0 & x &lt; 0\n\\end{cases}\n\\]\nfor some \\(\\lambda&gt;0\\) and \\(\\alpha&gt;0\\) is said to be a gamma random variable with parameters \\(\\alpha, \\lambda\\).\nThe quantity \\(\\Gamma (\\alpha)\\) is called the gamma function and is defined by\n\\[\n\\Gamma (\\alpha) = \\int_0^\\infty e^{-x} x^{\\alpha-1} \\, dx = (\\alpha-1)!\n\\]\n\nExpectation of a Gamma Random Variable\nIf \\(X\\) is a gamma random variable with parameters \\(\\alpha\\) and \\(\\lambda\\), then the expected value of \\(X\\) is given by \\[\nE(X) = \\int_0^\\infty x \\frac{\\lambda e^{- \\lambda x} {(\\lambda x)}^{\\alpha-1}}{\\Gamma (\\alpha)} \\, dx\n= \\frac{1}{\\lambda} \\int_0^\\infty e^{-x} x^{\\alpha} \\, dx\n= \\frac{1}{\\lambda} \\cdot \\Gamma (\\alpha + 1)\n= \\frac{1}{\\lambda} \\cdot \\alpha \\Gamma (\\alpha)\n= \\frac{\\alpha}{\\lambda}\n\\]\n\ndef gamma(x, alpha, lamd):\n    \"\"\"\n    Calculate the Gamma probability density function (PDF) for a given x, shape parameter alpha, and scale parameter lamd.\n\n    :param x: Value at which to evaluate the PDF\n    :param alpha: Shape parameter (alpha)\n    :param lamd: Scale parameter (lamda)\n    :return: Probability density at x\n    \"\"\"\n    from math import exp, factorial\n\n    return (lamd*exp(-lamd*x)*((lamd*x)**(alpha-1)))/factorial(alpha) if x &gt;= 0 else 0\n\n\nx = np.linspace(0, 10, 50) # Range of x values for the gamma distribution\nalpha_values = [3, 2] # Different values of alpha (shape parameter)\nlambdas = [1.5,  2] # Different values of lambda (scale parameter)\nplt.figure(figsize=(8, 6)) # Plotting the Gamma distribution\nfor alpha in alpha_values:\n    for lam in lambdas:\n        pdf = [gamma(i, alpha, lam) for i in x]\n        plt.plot(x, pdf, marker='o', label=f'α = {alpha}, λ = {lam}')\nplt.xlabel('x')\nplt.ylabel('Probability Density')\nplt.title('Gamma Distribution')\nplt.grid(axis=  \"y\" , linestyle='--')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "learn/distributions/dists_notebook.html#normal-random-variables",
    "href": "learn/distributions/dists_notebook.html#normal-random-variables",
    "title": "Random Variables",
    "section": "2.3 Normal Random Variables",
    "text": "2.3 Normal Random Variables\nWe say that \\(X\\) is a normal random variable (or simply that \\(X\\) is normally distributed) with parameters \\(\\mu\\) and \\(\\sigma ^2\\) if the density of X is given by\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-(x-\\mu)/ 2\\sigma^2} \\quad -\\infty &lt; x &lt; \\infty\n\\]\nThis density function is a bell-shaped curve that is symmetric around \\(\\mu\\).\nThe mean of the normal distribution is \\(\\mu\\) and the variance is \\(\\sigma^2\\). The cumulative distribution function is given by\n\\[\nF_X(\\alpha) = P(X \\leq \\alpha) = \\int_{-\\infty}^ \\alpha \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-(x-\\mu)/ 2\\sigma^2} \\, dx\n\\]\n\nExpectation of a Normal Random Variable\nIf \\(X\\) is a normal random variable with parameters \\(\\mu\\) and \\(\\sigma^2\\), then the expected value of \\(X\\) is given by \\[\nE(X) = \\int_{-\\infty}^\\infty x \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-(x-\\mu)/ 2\\sigma^2} \\, dx\n= \\mu \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-(x-\\mu)/ 2\\sigma^2} \\, dx\n= \\mu \\cdot 1\n= \\mu\n\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\ndef normal(x, mu, sigma):\n    \"\"\"\n    Calculate the Normal probability density function (PDF) for a given x, mean mu, and standard deviation sigma.\n\n    :param x: Value at which to evaluate the PDF\n    :param mu: Mean of the distribution\n    :param sigma: Standard deviation of the distribution\n    :return: Probability density at x\n    \"\"\"\n    from math import exp, pi\n\n    return (1 / (sigma * (2 * pi) ** 0.5)) * exp(-0.5 * ((x - mu) / sigma) ** 2)\n    \n\n\n# Example usage\nmu =50 # Mean of the distribution\nstd_devs = [5, 10, 20] # Different standard deviations\n\n# X range for plotting\nx = np.linspace(0, 100, 100) \n\n# Plot each distribution\nfor sigma in std_devs:\n    # y = norm.pdf(x, loc=mu, scale=sigma)\n    y = [normal(i, mu, sigma) for i in x]\n    plt.plot(x, y, label=f'std = {sigma}')\n\nplt.title('Normal Distributions with Different Standard Deviations')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "learn/distributions/dists_notebook.html#expectation-of-a-function-of-a-random-variable",
    "href": "learn/distributions/dists_notebook.html#expectation-of-a-function-of-a-random-variable",
    "title": "Random Variables",
    "section": "Expectation of a Function of a Random Variable",
    "text": "Expectation of a Function of a Random Variable\n\nIn Discrete Case\nIf \\(X\\) is a discrete random variable having a probability mass function \\(p(x)\\), and \\(g(x)\\) is a function of \\(x\\), then the expected value of \\(g(X)\\) is defined by \\[\nE(g(X)) = \\sum_{x_i} g(x_i) p(x_i)\n\\] ### In Continuous Case If \\(X\\) is a continuous random variable having a probability density function \\(f(x)\\), and \\(g(x)\\) is a function of \\(x\\), then the expected value of \\(g(X)\\) is defined by \\[\nE(g(X)) = \\int_{-\\infty}^{\\infty} g(x) f(x) \\, dx\n\\]"
  },
  {
    "objectID": "learn/distributions/dists_notebook.html#independent-random-variables",
    "href": "learn/distributions/dists_notebook.html#independent-random-variables",
    "title": "Random Variables",
    "section": "Independent Random Variables",
    "text": "Independent Random Variables\nThe random variables \\(X\\) and \\(Y\\) are said to be independent if, for all \\(a, b,\\) \\[\nP(X \\leq a, Y \\leq b) = P(X \\leq a) P(Y \\leq b)\n\\]\nIn terms of the joint cumulative distribution function, this means that \\[\nF(a,b) = F_X(a) F_Y(b)\n\\]\nWhen \\(X\\) and \\(Y\\) are discrete random variables, the independence condition can be expressed as \\[\np(x,y) = p_X(x) p_Y(y)\n\\]\nWhen \\(X\\) and \\(Y\\) are joint continuous, the independence reduces to \\[\nf(x,y) = f_X(x) f_Y(y)\n\\]\n\nProof for discrete case\nSuppose that the joint probability mass function \\(p(x,y)\\), then\n\\[\n\\begin{align*}\nP(X \\leq a, Y \\leq b) &= \\sum_{y \\leq b} \\sum_{x \\leq a} p(x,y) \\\\\n&= \\sum_{y \\leq b} \\sum_{x \\leq a} p_X(x) p_Y(y) \\\\\n&= \\left( \\sum_{x \\leq a} p_X(x) \\right) \\left( \\sum_{y \\leq b} p_Y(y) \\right) \\\\\n&= P(X \\leq a) P(Y \\leq b)\n\\end{align*}\n\\]\nand so \\(X\\) and \\(Y\\) are independent.\n\n\nProof for continuous case\nSuppose that the joint probability density function \\(f(x,y)\\), then \\[\n\\begin{align*}\nP(X \\leq a, Y \\leq b) &= \\int_{-\\infty}^a \\int_{-\\infty}^b f(x,y) \\, dx \\, dy \\\\\n&= \\int_{-\\infty}^a \\int_{-\\infty}^b f_X(x) f_Y(y) \\, dx \\, dy \\\\\n&= \\left( \\int_{-\\infty}^a f_X(x) \\, dx \\right) \\left( \\int_{-\\infty}^b f_Y(y) \\, dy \\right) \\\\\n&= P(X \\leq a) P(Y \\leq b)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "learn/distributions/dists_notebook.html#covariance-and-variance-of-sums-of-random-variables",
    "href": "learn/distributions/dists_notebook.html#covariance-and-variance-of-sums-of-random-variables",
    "title": "Random Variables",
    "section": "Covariance and Variance of Sums of Random Variables",
    "text": "Covariance and Variance of Sums of Random Variables\nThe covariance of any two random variables \\(X\\) and \\(Y\\), denoted by \\(Cov(X,Y)\\), is defined by \\[\nCov(X,Y) = E[(X - E(X))(Y - E(Y))] = E(XY) - E(X)E(Y)\n\\] The variance of a random variable \\(X\\), denoted by \\(Var(X)\\), is defined by \\[\nVar(X) = E[(X - E(X))^2] = E(X^2) - (E(X))^2\n\\]"
  },
  {
    "objectID": "learn/distributions/dists_notebook.html#moment-generating-functions",
    "href": "learn/distributions/dists_notebook.html#moment-generating-functions",
    "title": "Random Variables",
    "section": "Moment Generating Functions",
    "text": "Moment Generating Functions"
  },
  {
    "objectID": "learn/prob_stats/mc_integration.html",
    "href": "learn/prob_stats/mc_integration.html",
    "title": "Monte Carlo Integration",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "learn/cond_dists/cond_dist_notebook.html",
    "href": "learn/cond_dists/cond_dist_notebook.html",
    "title": "Conditional Probability and Conditional Expectation",
    "section": "",
    "text": "For any two events \\(E\\) and \\(F\\), the conditional probability of \\(E\\) given \\(F\\) is defined, as long as \\(P(F) &gt; 0\\), by\n\\[\nP(E|F) = \\frac{P(E  F)}{P(F)}.\n\\]\nHence, if \\(X\\) and \\(Y\\) are discrete random variables, then it is natural to define the conditional probability mass function of \\(X\\) given that \\(Y = y\\), by\n\\[\n\\begin{align*}\np_{X|Y}(x|y) &= P(X = x | Y = y) \\\\\n&= \\frac{P(X = x, Y = y)}{P(Y = y)} \\\\\n&= \\frac{p(x,y)}{p_Y(y)}.\n\\end{align*}\n\\]\nSimilarly, the conditional probability distribution function of \\(X\\) given that \\(Y=y\\) is defined, for all \\(y\\) such that \\(P(Y = y) &gt; 0\\), by\n\\[\n\\begin{align*}\nF_{X|Y}(x|y) &= P(X \\leq x | Y = y) \\\\\n&= \\sum_{a \\leq x} P_{X | Y}(a | y) \\\\\n\\end{align*}\n\\]\nThe conditional expectation of \\(X\\) given that \\(Y = y\\) is defined by \\[\nE[X | Y = y] = \\sum_{x} x P(X = x | Y = y) = \\sum_{x} x p_{X|Y}(x|y).\n\\]"
  },
  {
    "objectID": "learn/cond_dists/cond_dist_notebook.html#the-discrete-case",
    "href": "learn/cond_dists/cond_dist_notebook.html#the-discrete-case",
    "title": "Conditional Probability and Conditional Expectation",
    "section": "",
    "text": "For any two events \\(E\\) and \\(F\\), the conditional probability of \\(E\\) given \\(F\\) is defined, as long as \\(P(F) &gt; 0\\), by\n\\[\nP(E|F) = \\frac{P(E  F)}{P(F)}.\n\\]\nHence, if \\(X\\) and \\(Y\\) are discrete random variables, then it is natural to define the conditional probability mass function of \\(X\\) given that \\(Y = y\\), by\n\\[\n\\begin{align*}\np_{X|Y}(x|y) &= P(X = x | Y = y) \\\\\n&= \\frac{P(X = x, Y = y)}{P(Y = y)} \\\\\n&= \\frac{p(x,y)}{p_Y(y)}.\n\\end{align*}\n\\]\nSimilarly, the conditional probability distribution function of \\(X\\) given that \\(Y=y\\) is defined, for all \\(y\\) such that \\(P(Y = y) &gt; 0\\), by\n\\[\n\\begin{align*}\nF_{X|Y}(x|y) &= P(X \\leq x | Y = y) \\\\\n&= \\sum_{a \\leq x} P_{X | Y}(a | y) \\\\\n\\end{align*}\n\\]\nThe conditional expectation of \\(X\\) given that \\(Y = y\\) is defined by \\[\nE[X | Y = y] = \\sum_{x} x P(X = x | Y = y) = \\sum_{x} x p_{X|Y}(x|y).\n\\]"
  },
  {
    "objectID": "learn/cond_dists/cond_dist_notebook.html#the-continuous-case",
    "href": "learn/cond_dists/cond_dist_notebook.html#the-continuous-case",
    "title": "Conditional Probability and Conditional Expectation",
    "section": "The Continuous Case",
    "text": "The Continuous Case\nIf \\(X\\) and \\(Y\\) have a joint probability density function \\(f(x, y)\\), then the conditional probability density function of \\(X\\), given that \\(Y = y\\), is defined for all values of \\(y\\) such that \\(f_Y(y) &gt; 0\\), by\n\\[\n\\begin{align*}\nf_{X|Y}(x|y) &= \\frac{f(x,y)}{f_Y(y)} \\\\\n&= \\frac{f(x,y)}{\\int_{-\\infty}^{\\infty} f(x,y) dx}.\n\\end{align*}\n\\] The conditional distribution function of \\(X\\) given that \\(Y = y\\) is defined by \\[\n\\begin{align*}\nF_{X|Y}(x|y) &= P(X \\leq x | Y = y) \\\\\n&= \\int_{-\\infty}^{x} f_{X|Y}(a|y) da \\\\\n&= \\int_{-\\infty}^{x} \\frac{f(a,y)}{f_Y(y)} da \\\\\n&= \\frac{1}{f_Y(y)} \\int_{-\\infty}^{x} f(a,y) da.\n\\end{align*}\n\\]\nThe conditional expectation of \\(X\\) given that \\(Y = y\\) is defined or all values of \\(y\\) such that \\(f_Y(y) &gt; 0\\), by \\[\n\\begin{align*}\nE[X | Y = y] &= \\int_{-\\infty}^{\\infty} x f_{X|Y}(x|y) dx \\\\\n&= \\int_{-\\infty}^{\\infty} x \\frac{f(x,y)}{f_Y(y)} dx \\\\\n&= \\frac{1}{f_Y(y)} \\int_{-\\infty}^{\\infty} x f(x,y) dx.\n\\end{align*}\n\\]"
  },
  {
    "objectID": "learn/cond_dists/cond_dist_notebook.html#computing-expectations-by-conditioning",
    "href": "learn/cond_dists/cond_dist_notebook.html#computing-expectations-by-conditioning",
    "title": "Conditional Probability and Conditional Expectation",
    "section": "Computing Expectations by Conditioning",
    "text": "Computing Expectations by Conditioning\nLet us denote by \\(E[X|Y]\\) that function of the random variable \\(Y\\) whose value at \\(Y = y\\) is \\(E[X|Y = y]\\). Note that \\(E[X|Y]\\) is itself a random variable. An extremely important important property of conditional expectation is that for all random variables \\(X\\) and \\(Y\\) \\[\nE[X] = E[E[X|Y]]\n\\]\nIf \\(Y\\) is a discrete random variable, then \\[\nE[X] = \\sum_{y} E[X|Y = y] P(Y = y)\n\\]\nIf \\(Y\\) is continuous with density \\(f_Y(y)\\), then \\[\nE[X] = \\int_{-\\infty}^{\\infty} E[X|Y = y] f_Y(y) dy\n\\]"
  },
  {
    "objectID": "learn/index.html",
    "href": "learn/index.html",
    "title": "MUSTAFA ASLAN",
    "section": "",
    "text": "Monte Carlo Integration\n\n\nlearning material\n\n\n\n\n\nAug 9, 2025\n\n\nMustafa Aslan\n\n\n\n\n\n\n\n\n\n\n\n\nConditional Probability and Conditional Expectation\n\n\nlearning material\n\n\n\n\n\nApr 30, 2025\n\n\nMustafa Aslan\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Variables\n\n\nlearning material\n\n\n\n\n\nApr 30, 2025\n\n\nMustafa Aslan\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/wgsss/slides.html#outline",
    "href": "talks/wgsss/slides.html#outline",
    "title": "MUSTAFA ASLAN",
    "section": "Outline",
    "text": "Outline\n\nThe problem\nData and preliminary analysis\nExperiment design and Modelling framework\nResults\nKey findings\nNext steps"
  },
  {
    "objectID": "talks/wgsss/slides.html#current-systemic-issues-in-patient-flow",
    "href": "talks/wgsss/slides.html#current-systemic-issues-in-patient-flow",
    "title": "MUSTAFA ASLAN",
    "section": "Current Systemic Issues in Patient Flow",
    "text": "Current Systemic Issues in Patient Flow\n\n\n\n\n\nUnpredictable occupancy levels can result in redundant staffing and resource allocation, driving up costs, or in understaffing, which compromises patient care.\n\n\n\nSpikes in occupancy level drive overtime, sickness, and staff burnout.\n\n\n\nLack of analytical solutions that can capture the complexity and uncertainty of patient flow and bed availability limits proactive decision-making."
  },
  {
    "objectID": "talks/wgsss/slides.html#why-is-it-important",
    "href": "talks/wgsss/slides.html#why-is-it-important",
    "title": "MUSTAFA ASLAN",
    "section": "Why is it important?",
    "text": "Why is it important?\n\n\nWhy is an efficient bed management system crucial?\n\nHospital beds are one of most costly healthcare resources.\n\nNHS rates for acute mental health beds are generally between £760 and £850 per day in 2025\n\nBed shortages force some patients to wait in corridors, impacting care quality and safety.\nBed shortages increase staff pressure, burnout, and operational costs.\n\n\n\nWhy a reliable forecasting approach is needed?\n\nMental health demand is volatile and influenced by unpredictable crisis, by seasonality, epidemics, and unforeseen events.\nA reliable forecasting approach enables timely staffing, resource allocation, and improved patient care.\nAnticipating demand allows decision-makers to balance scarce resources while improving patient flow and safety.\nRobust forecasts enhance resilience to unexpected surges and reduce staff burnout."
  },
  {
    "objectID": "talks/wgsss/slides.html#what-do-we-forecast",
    "href": "talks/wgsss/slides.html#what-do-we-forecast",
    "title": "MUSTAFA ASLAN",
    "section": "What do we forecast?",
    "text": "What do we forecast?\n\nDaily hospital bed occupancy for 7 wards in a UK hospital.\nForecast horizon: 30 days ahead, supporting medium-term planning for bed allocation and staffing, generated at the end of each month for the following month.\nFocus on probabilistic forecasting to capture uncertainty, offering prediction intervals rather than single-point estimates using a reliable model."
  },
  {
    "objectID": "talks/wgsss/slides.html#key-research-questions",
    "href": "talks/wgsss/slides.html#key-research-questions",
    "title": "MUSTAFA ASLAN",
    "section": "Key Research Questions",
    "text": "Key Research Questions\n\nHow can we accurately forecast hospital bed occupancy to improve resource allocation and patient care?\nHow can probabilistic forecasting models be developed to account for the inherent uncertainty in patient flow and bed availability?\nWhat are the hidden factors influencing bed occupancy, and how can they be incorporated into forecasting models?"
  },
  {
    "objectID": "talks/wgsss/slides.html#dataset",
    "href": "talks/wgsss/slides.html#dataset",
    "title": "MUSTAFA ASLAN",
    "section": "Dataset",
    "text": "Dataset\nDaily hospital occupancy data from a UK hospital with 7 wards, spanning from July 2018 to April 2025. The dataset includes:\n\nDaily occupancy counts for each ward (daily number of patients staying in each ward)\nDate-related features: day of the week, month, year, day of the month, week of the year\nHoliday indicators"
  },
  {
    "objectID": "talks/wgsss/slides.html#data-characteristics",
    "href": "talks/wgsss/slides.html#data-characteristics",
    "title": "MUSTAFA ASLAN",
    "section": "Data characteristics",
    "text": "Data characteristics\n\nStrong upward trend in occupancy over time\nSpike after COVID-19 pandemic"
  },
  {
    "objectID": "talks/wgsss/slides.html#seasonal-patterns",
    "href": "talks/wgsss/slides.html#seasonal-patterns",
    "title": "MUSTAFA ASLAN",
    "section": "Seasonal patterns",
    "text": "Seasonal patterns\nDay of the week seasonality"
  },
  {
    "objectID": "talks/wgsss/slides.html#seasonal-patterns-1",
    "href": "talks/wgsss/slides.html#seasonal-patterns-1",
    "title": "MUSTAFA ASLAN",
    "section": "Seasonal patterns",
    "text": "Seasonal patterns\nMonthly seasonality"
  },
  {
    "objectID": "talks/wgsss/slides.html#effect-of-holidays",
    "href": "talks/wgsss/slides.html#effect-of-holidays",
    "title": "MUSTAFA ASLAN",
    "section": "Effect of holidays",
    "text": "Effect of holidays"
  },
  {
    "objectID": "talks/wgsss/slides.html#key-insights-from-data-analysis",
    "href": "talks/wgsss/slides.html#key-insights-from-data-analysis",
    "title": "MUSTAFA ASLAN",
    "section": "Key insights from data analysis",
    "text": "Key insights from data analysis\nAn important assumption to model: Stationarity\nOne approach to make the series stationary is to take the first difference of the series.\nOriginal series: \\(y_t, y_{t-1}, y_{t-2}, y_{t-3}, \\ldots\\)\nDifferenced series: \\(y_t - y_{t-1}, y_{t-1} - y_{t-2}, y_{t-2} - y_{t-3}, \\ldots\\)"
  },
  {
    "objectID": "talks/wgsss/slides.html#key-insights-from-data-analysis-1",
    "href": "talks/wgsss/slides.html#key-insights-from-data-analysis-1",
    "title": "MUSTAFA ASLAN",
    "section": "Key insights from data analysis",
    "text": "Key insights from data analysis\nInformation from ACF and PACF plots after differencing"
  },
  {
    "objectID": "talks/wgsss/slides.html#key-insights-from-data-analysis-2",
    "href": "talks/wgsss/slides.html#key-insights-from-data-analysis-2",
    "title": "MUSTAFA ASLAN",
    "section": "Key insights from data analysis",
    "text": "Key insights from data analysis\nAlternative approach to make the series stationary and keep the dependence structure"
  },
  {
    "objectID": "talks/wgsss/slides.html#an-important-assumption-to-model-stationarity-1",
    "href": "talks/wgsss/slides.html#an-important-assumption-to-model-stationarity-1",
    "title": "MUSTAFA ASLAN",
    "section": "An important assumption to model: Stationarity",
    "text": "An important assumption to model: Stationarity\nInformation from ACF and PACF plots after detrending"
  },
  {
    "objectID": "talks/wgsss/slides.html#regime-switching-autoregressive-hidden-markov-model-rs-arhmm",
    "href": "talks/wgsss/slides.html#regime-switching-autoregressive-hidden-markov-model-rs-arhmm",
    "title": "MUSTAFA ASLAN",
    "section": "Regime-Switching AutoRegressive Hidden Markov Model (RS-ARHMM)",
    "text": "Regime-Switching AutoRegressive Hidden Markov Model (RS-ARHMM)\nLet \\(y_t\\) be the observed value at time \\(t\\), modeled as a function of its \\(p\\) lagged values, the regime-specific parameters associated with the hidden state \\(s_t\\), and exogenous variables \\(\\mathbf{X}_t = (X_{t1}, \\ldots, X_{tM})\\).\nThe RS-ARHMM can be expressed as follows:\n\\[\ny_t^{(s)} = \\beta_{0}^{(s)} + \\sum_{i=1}^{p} \\beta_{i}^{(s)} \\, y_{t-i} + \\sum_{j=1}^{M} \\beta_{p+j}^{(s)} \\, X_{tj} + \\epsilon_t^{(s)},\n\\]\nwhere:\n\n\\(\\beta_{i}^{(s)}\\) are the coefficients for the lagged values for regime \\(s\\),\n\\(\\beta_{p+j}^{(s)}\\) are the coefficients for the exogenous variables for regime \\(s\\),\n\\(M\\) is the number of exogenous variables,\n\\(\\epsilon_t^{(s)}\\) is the error term for regime \\(s\\).\n\nThe Markov property: \\[\nP(s_t = k \\mid s_{1:t-1}) = P(s_t = k \\mid s_{t-1}), \\quad \\forall t \\geq 2.\n\\]\nA HMM has the following components:\n\n\\(S\\): The set of regimes, \\(\\mathbb{S} = \\{S_1, S_2, \\ldots, S_K\\}\\).\n\\(P\\): The transition probability matrix\n\\(p_{ij} = P(s_t = S_j \\mid s_{t-1} = S_i)\\) is the probability of transitioning from regime \\(S_i\\) to regime \\(S_j\\), s.t. \\(\\sum_{j=1}^{K} p_{ij} = 1\\).\n\nTransition matrix \\(P\\) is defined as:\n\\[\nP = \\begin{pmatrix}\np_{11} & \\cdots & p_{1K} \\\\\n\\vdots & \\ddots & \\vdots \\\\\np_{K1} & \\cdots & p_{KK}\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "talks/wgsss/slides.html#modeling-framework",
    "href": "talks/wgsss/slides.html#modeling-framework",
    "title": "MUSTAFA ASLAN",
    "section": "Modeling Framework",
    "text": "Modeling Framework\nForecasting with RS-ARHMM\n\nForecast Computation: The final forecast for each time step is a weighted average of the forecasts from each regime, weighted by the predicted regime probabilities:\n\n\\[\n\\hat{y}_{T+h} = \\sum_{j=1}^{K} P(s_{T+h} = S_j) \\, \\hat{y}_{T+h}^{(S_j)}\n\\]\n\nState Prediction: We predict the most probable state sequence for the next \\(h\\) time steps using the transition matrix \\(\\mathbf{P}\\). The probability of being in state \\(S_j\\) at time \\(T+h\\) is computed recursively as:\n\n\\[\nP(s_{T+h} = S_j) = \\sum_{i=1}^{K} P(s_{T+h-1} = S_i) \\, p_{ij}\n\\]\n\nRegime-specific forecasts: For each predicted regime, we utilize the corresponding regression model to forecast the observation at that time step. The predicted observation \\(\\hat{y}_{T+h}\\) for state \\(s_{T+h}\\) is computed as:\n\n\\[\n\\hat{y}_{T+h}^{s_{T+h}} = \\beta_{0}^{(s_{T+h})} + \\sum_{i=1}^{p} \\beta_{i}^{(s_{T+h})} \\, y_{T+h-i} + \\sum_{j=1}^{M} \\beta_{p+j}^{(s_{T+h})} \\, X_{T+h,j}\n\\]"
  },
  {
    "objectID": "talks/wgsss/slides.html#rs-arhmm-parameter-estimation",
    "href": "talks/wgsss/slides.html#rs-arhmm-parameter-estimation",
    "title": "MUSTAFA ASLAN",
    "section": "RS-ARHMM Parameter Estimation",
    "text": "RS-ARHMM Parameter Estimation\n\n\n\n\n\nEM (Expectation-Maximization) Algorithm\n\n\n\nIteratively estimates parameters \\(\\Theta = \\{\\beta^{(s)}, \\sigma^{2(s)}, P, \\pi\\}\\) to maximize likelihood.\nAlternates between two steps:\n\nE-Step: Estimate regime probabilities given current \\(\\Theta\\).\nM-Step: Update \\(\\Theta\\) using these probabilities.\n\n\n\n\n\n\n\n\n\n1. E-Step\n\n\n\nState probability:\n\\[\n\\gamma_t(i) = P(s_t = S_i \\mid y_{1:T}, \\Theta^{(k)})\n\\]\nTransition probability:\n\\[\n\\xi_t(i,j) = P(s_t = S_i, s_{t+1} = S_j \\mid y_{1:T}, \\Theta^{(k)})\n\\]\nComputed via Forward-Backward algorithm:\n\nForward:\n\n\\(\\alpha_t(i) = P(y_{1:t}, s_t = S_i \\mid \\Theta) = \\sum_{j=1}^{K} \\alpha_{t-1}(j) p_{ji} o_i(y_t)\\)\n\nBackward:\n\n\\(b_t(i) = P(y_{t+1:T} \\mid s_t = S_i, \\Theta) = \\sum_{j=1}^{K} p_{ij} o_j(y_{t+1}) b_{t+1}(j)\\)\n\nEmission (observation) likelihood:\n\\[\no_i(y_t) = \\frac{1}{\\sqrt{2\\pi \\sigma_i^2}} \\exp\\left( -\\frac{(y_t - \\mu_{i,t})^2}{2\\sigma_i^2} \\right)\n\\]\nwhere \\(\\mu_{i,t}\\) is AR + exogenous mean for state \\(i\\).\n\n\n\n\n\n\n\n2. M-Step\n\n\nUpdate parameters using weighted averages with \\(\\gamma_t(i)\\):\n\nTransition matrix:\n\\[\np_{ij}^{(k+1)} = \\frac{\\sum_{t=1}^{T-1} \\xi_t(i,j)}{\\sum_{t=1}^{T-1} \\gamma_t(i)}\n\\]\nInitial state:\n\\[\n\\pi_i^{(k+1)} = \\gamma_1(i)\n\\]\nRegression (AR/exog) coefficients:\n\\[\n\\beta^{(i,k+1)} = (\\mathbf{X}^T \\mathbf{W}_i \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W}_i \\mathbf{y}\n\\]\n\nWhere \\(\\mathbf{W}_i = \\text{diag}(\\gamma_1(i),\\dots,\\gamma_T(i))\\)\n\nVariance:\n\\[\n\\sigma_i^{2(k+1)} = \\frac{\\sum_{t=1}^T \\gamma_t(i)(y_t-\\mu_{i,t})^2}{\\sum_{t=1}^T \\gamma_t(i)}\n\\]\n\n\n\n\n\n\n\n\nRepeat E-M Steps until convergence:\n\n\n\\[|\\ell(\\Theta^{(k+1)}) - \\ell(\\Theta^{(k)})| &lt; \\epsilon\\]"
  },
  {
    "objectID": "talks/wgsss/slides.html#why-rs-arhmm",
    "href": "talks/wgsss/slides.html#why-rs-arhmm",
    "title": "MUSTAFA ASLAN",
    "section": "Why RS-ARHMM?",
    "text": "Why RS-ARHMM?\n\nCaptures regime changes in highly volatile time series data (e.g., healthcare), integrating them into the forecasting process, making it more accurate and reliable predictions.\nTraditional statistical and modern machine learning models fail to account for such regime shifts.\nModels complex dependencies and non-linear relationships with reduced risk of overfitting and less reliance on extensive hyperparameter tuning.\nProduces interpretable results through regime-specific parameters.\nProvides reliable forecasts that incorporate uncertainty arising from regime changes."
  },
  {
    "objectID": "talks/wgsss/slides.html#benchmark-models",
    "href": "talks/wgsss/slides.html#benchmark-models",
    "title": "MUSTAFA ASLAN",
    "section": "Benchmark models",
    "text": "Benchmark models\nStatistical models:\n\nExponential Smoothing (ETS): A state space time series model capturing level, trend, and seasonality.\nLinear Regression: A statistical model that estimates the linear relationship between predictors and a response variable.\nLasso Regression: A regression method with L1 regularization, useful for variable selection and preventing overfitting.\n\nMachine Learning models:\n\nXGBoost: An optimized gradient boosting library designed to be highly efficient and flexible. It uses level-wise tree growth, building trees level by level horizontally.\nLightGBM: A gradient boosting framework that uses tree-based learning algorithms, known for its speed and efficiency. It uses leaf-wise tree growth.\nRandom Forest: An ensemble learning method that builds multiple decision trees and merges them together to get a more accurate and stable prediction."
  },
  {
    "objectID": "talks/wgsss/slides.html#probabilistic-forecasting-using-conformal-prediction",
    "href": "talks/wgsss/slides.html#probabilistic-forecasting-using-conformal-prediction",
    "title": "MUSTAFA ASLAN",
    "section": "Probabilistic Forecasting using Conformal Prediction",
    "text": "Probabilistic Forecasting using Conformal Prediction\nConformal Prediction for Time Series\n\nA distribution-free method for constructing prediction intervals\nA way to quantify the uncertainty of point forecasts by generating prediction intervals\n\n\nSteps to Generate Prediction Intervals\n\n\n\n\n\nPoint Forecasting\n\n\nGenerate point forecasts \\(\\hat{y}_{t+h}\\) for the desired forecast horizon \\(h\\) using rolling-origin cross-validation on a calibration set of 500 observations per horizon.\n\n\n\n\nResidual Calculation\n\n\\[\nr_{t+h} = y_{t+h} - \\hat{y}_{t+h}\n\\]\nwhere \\(y_{t+h}\\) are the actual observed values and \\(\\hat{y}_{t+h}\\) are the point forecasts for \\(t+h\\).\n\n\n\nNonconformity Scores \\[\nA_{t+h} = |r_{t+h}|\n\\]\n\n\n\n\n\nQuantile Calculation\n\n\\[\nq_{1-\\alpha}^{(h)} = \\text{Quantile}_{1-\\alpha}(\\{A_{t-n+h}\\}_{t=t-n}^{n})\n\\]\nwhere \\(n\\) is the size of the calibration set and \\(1 - \\alpha\\) is the desired coverage level\n\n\n\nPrediction Intervals\n\n\\[\n\\left[\\hat{y}_{t+h} - q_{1-\\alpha}^{(h)}, \\hat{y}_{t+h} + q_{1-\\alpha}^{(h)}\\right]\n\\]\n\n\n\nDistribution Approximation\n\n\\[\ny_{t+h}^{(i)} = \\hat{y}_{t+h} + r_{t+h}^{(i)}, \\quad r_{t+h}^{(i)} \\in \\{r_{t+h}^{(1)}, r_{t+h}^{(2)}, \\ldots, r_{t+h}^{(n)}\\}\n\\]"
  },
  {
    "objectID": "talks/wgsss/slides.html#forecasting-metrics",
    "href": "talks/wgsss/slides.html#forecasting-metrics",
    "title": "MUSTAFA ASLAN",
    "section": "Forecasting metrics",
    "text": "Forecasting metrics\n\n\nPoint Forecasting Metrics\n\nRoot Mean Squared Error (RMSE) \\[\nRMSE = \\sqrt{\\frac{1}{n} \\sum_{t=1}^{n} (y_t - \\hat{y}_t)^2}\n\\]\nMean Absolute Error (MAE) \\[\nMAE = \\frac{1}{n} \\sum_{t=1}^{n} |y_t - \\hat{y}_t|\n\\]\n\n\nProbabilistic Forecasting Metrics\n\nQunatile Loss (QL) \\[\nQL_{\\alpha} = 2 \\sum_{t=1}^{h} \\left[\\alpha (y_t - \\hat{y}_t^{(\\alpha)}) \\mathbb{1}_{\\{y_t &gt; \\hat{y}_t^{(\\alpha)}\\}} + (1 - \\alpha) (\\hat{y}_t^{(\\alpha)} - y_t) \\mathbb{1}_{\\{y_t \\leq \\hat{y}_t^{(\\alpha)}\\}}\\right]\n\\]"
  },
  {
    "objectID": "talks/wgsss/slides.html#feature-engineering",
    "href": "talks/wgsss/slides.html#feature-engineering",
    "title": "MUSTAFA ASLAN",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nTime-based features\n\n\n\nPublic holidays\n\n\n\nLagged features\n\n\n\nFourier terms\n\n\n\nRolling window features"
  },
  {
    "objectID": "talks/wgsss/slides.html#cross-validation-setup",
    "href": "talks/wgsss/slides.html#cross-validation-setup",
    "title": "MUSTAFA ASLAN",
    "section": "Cross validation setup",
    "text": "Cross validation setup\nRolling-origin cross-validation is used to evaluate model performance over time"
  },
  {
    "objectID": "talks/wgsss/slides.html#point-forecast-results",
    "href": "talks/wgsss/slides.html#point-forecast-results",
    "title": "MUSTAFA ASLAN",
    "section": "Point Forecast results",
    "text": "Point Forecast results\n\n\n\n\n\n\n\n\n\nward_0\nRMSE\nMAE\n\n\n\n\nHMM\n3.208\n2.757\n\n\nlinearRegression\n3.258\n2.804\n\n\nLasso\n3.268\n2.832\n\n\nRandomForest\n3.365\n2.885\n\n\nLightGBM\n3.364\n2.905\n\n\nXGBoost\n3.433\n2.996\n\n\nets\n3.679\n3.168\n\n\n\n\n\n\n\n\n\n\n\n\nward_1\nRMSE\nMAE\n\n\n\n\nets\n3.646\n3.169\n\n\nLasso\n3.701\n3.220\n\n\nRandomForest\n3.707\n3.229\n\n\nlinearRegression\n3.711\n3.233\n\n\nHMM\n3.712\n3.231\n\n\nLightGBM\n3.720\n3.241\n\n\nXGBoost\n3.770\n3.277\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nward_2\nRMSE\nMAE\n\n\n\n\nHMM\n2.843\n2.406\n\n\nlinearRegression\n2.862\n2.426\n\n\nLasso\n2.975\n2.538\n\n\nLightGBM\n2.983\n2.544\n\n\nXGBoost\n3.036\n2.605\n\n\nRandomForest\n3.141\n2.723\n\n\nets\n3.210\n2.800\n\n\n\n\n\n\n\n\n\n\n\n\nward_3\nRMSE\nMAE\n\n\n\n\nLasso\n1.992\n1.664\n\n\nLightGBM\n2.009\n1.700\n\n\nlinearRegression\n2.044\n1.724\n\n\nRandomForest\n2.058\n1.730\n\n\nHMM\n2.078\n1.749\n\n\nXGBoost\n2.098\n1.776\n\n\nets\n2.475\n2.133\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nward_4\nRMSE\nMAE\n\n\n\n\nLasso\n2.320\n1.999\n\n\nlinearRegression\n2.470\n2.135\n\n\nHMM\n2.532\n2.211\n\n\nets\n2.592\n2.244\n\n\nXGBoost\n2.660\n2.314\n\n\nRandomForest\n2.825\n2.486\n\n\nLightGBM\n3.344\n2.946\n\n\n\n\n\n\n\n\n\n\n\n\nward_5\nRMSE\nMAE\n\n\n\n\nHMM\n2.500\n2.122\n\n\nRandomForest\n2.503\n2.129\n\n\nLightGBM\n2.511\n2.143\n\n\nXGBoost\n2.512\n2.144\n\n\nLasso\n2.517\n2.144\n\n\nlinearRegression\n2.532\n2.156\n\n\nets\n2.603\n2.235"
  },
  {
    "objectID": "talks/wgsss/slides.html#forecast-distributions-quantile-scores",
    "href": "talks/wgsss/slides.html#forecast-distributions-quantile-scores",
    "title": "MUSTAFA ASLAN",
    "section": "Forecast Distributions (Quantile scores)",
    "text": "Forecast Distributions (Quantile scores)"
  },
  {
    "objectID": "talks/wgsss/slides.html#forecast-distributions-quantile-scores-1",
    "href": "talks/wgsss/slides.html#forecast-distributions-quantile-scores-1",
    "title": "MUSTAFA ASLAN",
    "section": "Forecast Distributions (Quantile scores)",
    "text": "Forecast Distributions (Quantile scores)"
  },
  {
    "objectID": "talks/wgsss/slides.html#forecast-distributions-quantile-scores-for-45-to-85",
    "href": "talks/wgsss/slides.html#forecast-distributions-quantile-scores-for-45-to-85",
    "title": "MUSTAFA ASLAN",
    "section": "Forecast distributions (Quantile scores for 45% to 85%)",
    "text": "Forecast distributions (Quantile scores for 45% to 85%)"
  },
  {
    "objectID": "talks/wgsss/slides.html#models-explainability",
    "href": "talks/wgsss/slides.html#models-explainability",
    "title": "MUSTAFA ASLAN",
    "section": "Model’s Explainability",
    "text": "Model’s Explainability\n\n\n\n\n\n\n\n\n\n\n\n\nTransition probabilities\n\n\n\n\n\nT. Matrix\nRegime 1\nRegime 2\n\n\n\n\nRegime 1\n0.666\n0.334\n\n\nRegime 2\n0.487\n0.513\n\n\n\n\n\n \nStandard Deviations\n\n\n\n\n\nStd. Dev.\nRegime 1\nRegime 2\n\n\n\n\nsd\n1.393\n0.006"
  },
  {
    "objectID": "talks/wgsss/slides.html#learning-outcomes-from-the-current-experiments",
    "href": "talks/wgsss/slides.html#learning-outcomes-from-the-current-experiments",
    "title": "MUSTAFA ASLAN",
    "section": "Learning outcomes from the current experiments",
    "text": "Learning outcomes from the current experiments\n\nAlthough AR-RHMM may not always outperform all benchmark models in point forecasting metrics, there is a significant improvement in probabilistic forecasting, particularly in quantile loss across 40% and 85% quantiles.\nThe model is more reliable for probabilistic forecasting since it accounts for regime changes and provides a distribution of possible future values, making it suitable for decision-making in hospital resource management.\nThe model performs consistently well interms of probabilistic forecasting since the model is probabilistic in nature and captures regime changes."
  },
  {
    "objectID": "talks/wgsss/slides.html#next-steps-1",
    "href": "talks/wgsss/slides.html#next-steps-1",
    "title": "MUSTAFA ASLAN",
    "section": "Next steps",
    "text": "Next steps\n\nExtend the AR-HMM to a multivariate framework (VAR-HMM) to jointly model multiple wards, capturing interdependencies and shared patterns in bed occupancy across different wards.\nDevelop an optimization framework that utilizes the probabilistic forecasts from the AR-HMM and VAR-HMM models to optimize bed allocation and resource management.\nTest the models in real-world scenarios, collaborating with hospital staff to refine the models based on practical feedback and operational needs."
  },
  {
    "objectID": "talks/presentations/wgsss.html",
    "href": "talks/presentations/wgsss.html",
    "title": "Forecasting Mental Health Hospital Bed Occupancy: A Regime-Switching AutoRegressive Hidden Markov Model Approach",
    "section": "",
    "text": "Slides\nDate: Sep 16 2025 11:00 AM – 13:00 PM\nEvent: 5th Welsh Postgraduate Research Cluster Workshop\nLocation: School of Management, Swansea University, UK"
  },
  {
    "objectID": "talks/presentations/wgsss.html#context",
    "href": "talks/presentations/wgsss.html#context",
    "title": "Forecasting Mental Health Hospital Bed Occupancy: A Regime-Switching AutoRegressive Hidden Markov Model Approach",
    "section": "Context",
    "text": "Context\nAt the 5th Welsh Postgraduate Research Cluster Workshop, we will present our research on forecasting mental health hospital bed occupancy using a Regime Switching Autoregressive Hidden Markov Model (RS-ARHMM). This model is designed to capture the dynamic and often unpredictable nature of hospital bed occupancy, which is influenced by various factors that are even not directly observable."
  },
  {
    "objectID": "talks/presentations/wgsss.html#abstract",
    "href": "talks/presentations/wgsss.html#abstract",
    "title": "Forecasting Mental Health Hospital Bed Occupancy: A Regime-Switching AutoRegressive Hidden Markov Model Approach",
    "section": "Abstract",
    "text": "Abstract\nHospital bed occupancy is a critical metric for healthcare systems, directly impacting patient care quality and operational efficiency. Accurate forecasting of bed occupancy can aid in resource allocation, staffing, and overall hospital management. The presentation introduces a Regime Switching Autoregressive Hidden Markov Model (RS-ARHMM) to forecast hospital bed occupancy, capturing the dynamic nature of healthcare demand. The RS-ARHMM model combines the strengths of autoregressive models in handling time series data with the flexibility of Markov switching to account for regime changes in occupancy patterns. The model is applied to historical bed occupancy data from a mental health hospital, demonstrating its ability to adapt to sudden changes in demand. Results indicate that the RS-ARHMM model outperforms both traditional and modern forecasting methods, providing more accurate and reliable predictions in terms of probabilistic forecasting methods. This approach offers a valuable tool for hospital administrators and policymakers to enhance staffing strategies and resource management, ultimately improving patient outcomes and operational efficiency."
  },
  {
    "objectID": "bins/dist.html",
    "href": "bins/dist.html",
    "title": "my dist",
    "section": "",
    "text": "asdasdasda\n\\[\nf_x = \\int\n\\]\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "CV/CV.html",
    "href": "CV/CV.html",
    "title": "PhD Researcher | Data Scientist & Mathematical Modeller",
    "section": "",
    "text": "PhD in Probabilistic Modeling for Healthcare Management and Operations\nCardiff University, Cardiff, UK Oct 2024 — Present\n\nRecipient of WGSSS-ESRC Studentship Award\nProject focused on enhancing discharge planning process using probabilistic modelling\n\nSupervisors: Prof Bahman Rostami-Tabar, Dr. Jeremy Dixon\n\n\nMSc in Financial Mathematics\nMiddle East Technical University, Ankara, Turkey Oct 2017 — Aug 2021\n\nDissertation: Effects of Exchange Rate Volatility and Firm-Specific Features on the Rates of Returns of the Manufacturing Firms Listed in Borsa İstanbul: A CAPM Approach\n\nStatistical&Machine learning techniques used: Markov Switching GARCH Models, ARIMA, Panel Data Econometrics, Principal Component Analysis\n\n\nBSc in Business Administration\nMiddle East Technical University, Ankara, Turkey Oct 2011 — Aug 2015"
  },
  {
    "objectID": "CV/CV.html#education",
    "href": "CV/CV.html#education",
    "title": "PhD Researcher | Data Scientist & Mathematical Modeller",
    "section": "",
    "text": "PhD in Probabilistic Modeling for Healthcare Management and Operations\nCardiff University, Cardiff, UK Oct 2024 — Present\n\nRecipient of WGSSS-ESRC Studentship Award\nProject focused on enhancing discharge planning process using probabilistic modelling\n\nSupervisors: Prof Bahman Rostami-Tabar, Dr. Jeremy Dixon\n\n\nMSc in Financial Mathematics\nMiddle East Technical University, Ankara, Turkey Oct 2017 — Aug 2021\n\nDissertation: Effects of Exchange Rate Volatility and Firm-Specific Features on the Rates of Returns of the Manufacturing Firms Listed in Borsa İstanbul: A CAPM Approach\n\nStatistical&Machine learning techniques used: Markov Switching GARCH Models, ARIMA, Panel Data Econometrics, Principal Component Analysis\n\n\nBSc in Business Administration\nMiddle East Technical University, Ankara, Turkey Oct 2011 — Aug 2015"
  },
  {
    "objectID": "CV/CV.html#teaching",
    "href": "CV/CV.html#teaching",
    "title": "PhD Researcher | Data Scientist & Mathematical Modeller",
    "section": "TEACHING",
    "text": "TEACHING\nTraining sessions for Data Lab for Social Good Research Group members\n\nIntroduction to Python and Quarto on Visual Studio Code Mar 2025\nIntroduction to Dynamic Programming and Reinforcement Learning May 2025"
  },
  {
    "objectID": "CV/CV.html#work-experiences",
    "href": "CV/CV.html#work-experiences",
    "title": "PhD Researcher | Data Scientist & Mathematical Modeller",
    "section": "WORK EXPERIENCES",
    "text": "WORK EXPERIENCES\nReporting and Data Analytics Executive\nAKBANK (a leading bank in Turkey), Istanbul Jun 2022 — Sep 2024\n\nApplied time-series forecasting and machine learning techniques, including ARIMA, Bayesian Time Series, Prophet, ANN, LSTM, Random Forest, LightGBM and XGBoost, to historical data for making long-horizon forecasts of daily customer call volume. The winning model helped to allocate resources effectively, reducing customer wait times by 6%.\nImplemented machine learning models (e.g., XGBoost, LightGBM, CatBoost) to predict customer behavior with an over recall rate of 70% and an accuracy rate of 85%, enabling fewer customer complaints and increase sales by 14%.\nAnalyzing large amounts of data to identify trends and find patterns, signals and hidden stories within customer calls data.\nApplied machine learning techniques (Z-score, IsolationForest) to detect anomalies.\nApplied unsupervised machine learning techniques (DBSCAN, Gaussian mixture, K-means) to cluster customers.\nHyperparameter tuning for machine learning models using Hyperopt, Optuna and KerasTuner.\n\nResearch Associate\nThe Economic Policy Research Foundation of Turkey (Think Tank), Ankara Jan 2022 — Apr 2024\n\nDetermined areas of research to increase knowledge in the particular field.\nUtilizing inferential statistics such as hypothesis testing (e.g., t-test, ANOVA test, population proportion test), confidence intervals, correlation analysis and regression analysis to make inferences and draw conclusions about data.\nDeveloped statistical models (regression analysis, panel data modeling) for regional development projects to contribute to data-driven decisions.\n\nSenior Process Development Analyst\nETI GIDA Inc (a major FMCG player in Turkey), Eskisehir Jun 2019 — Jan 2022\n\nInteracted with internal customers to understand business needs and translate into requirements and project scope.\nAssessed the impact of current business processes on users and stakeholders and evaluated potential areas for improvement.\nMaintained strong working knowledge of ERP (SAP), CRM and business intelligence tools and operational features.\n\nInternal Auditor\nTurk Telekom Inc. (the telecom giant of Turkey), Ankara Nov 2015 — Jun 2019\n\nPerformed strategic planning, execution and finalization of audits using data analytics and critical thinking skills.\nInvestigated discrepancies discovered during the auditing process.\nRecommended new methods to improve internal controls and operating efficiency."
  },
  {
    "objectID": "CV/CV.html#research-interests",
    "href": "CV/CV.html#research-interests",
    "title": "PhD Researcher | Data Scientist & Mathematical Modeller",
    "section": "RESEARCH INTERESTS",
    "text": "RESEARCH INTERESTS\n\nHealthcare analytics\nData-driven decision making\nStatistical learning and Machine learning\nReinforcement learning and stochastic optimization in decision making\nTime series forecasting\nProbabilistic forecasting\nConformal prediction"
  },
  {
    "objectID": "CV/CV.html#skills-expertise",
    "href": "CV/CV.html#skills-expertise",
    "title": "PhD Researcher | Data Scientist & Mathematical Modeller",
    "section": "SKILLS & EXPERTISE",
    "text": "SKILLS & EXPERTISE\nExpertise: Mathematical and Statistical Modeling, Machine Learning, Time Series Analysis and Forecasting, Stochastic Optimization and Reinforcement Learning, Statistical and Explanatory Data Analysis, web scraping in Python\nProgramming: Python SQL\nReporting: Quarto Advanced Excel QlikView SAS\nLanguages English (Fluent), Kurdish (Native), Turkish (Native)"
  },
  {
    "objectID": "CV/CV.html#professional-development",
    "href": "CV/CV.html#professional-development",
    "title": "PhD Researcher | Data Scientist & Mathematical Modeller",
    "section": "PROFESSIONAL DEVELOPMENT",
    "text": "PROFESSIONAL DEVELOPMENT\n\nCertification:\n\nData Science: Machine Learning, HarvardX (edX)\n\nBooks (some of my go-to books):\n\nForecasting: Principles and Practice (Hyndman et al, 2021)\nIntroduction to Statistical Learning with Applications in R (Tibshirani et al, 2019)\nThe Elements of Statistical Learning (Tibshirani et al, 2008)\nProbabilistic Machine Learning: An Introduction (Murphy. 2022)\nReinforcement Learning: An Introduction (Barto et al, 2018)\nReinforcement Learning and Stochastic Optimization (Powell, 2022)\nA Student’s Guide to Bayesian Statistics (Lambert, 2018)\nDive into deep learning (Zhang, 2022)"
  },
  {
    "objectID": "CV/CV.html#scholarships-grants",
    "href": "CV/CV.html#scholarships-grants",
    "title": "PhD Researcher | Data Scientist & Mathematical Modeller",
    "section": "Scholarships & Grants",
    "text": "Scholarships & Grants\n\nWelsh Graduate School for the Social Sciences (WGSSS) Studentship Award Aug 2024\nNATCOR Bursaries / EURO Funding for Stochastic Modelling Mar 2025"
  },
  {
    "objectID": "CV/CV.html#references",
    "href": "CV/CV.html#references",
    "title": "PhD Researcher | Data Scientist & Mathematical Modeller",
    "section": "References",
    "text": "References\nAvailable upon request"
  }
]