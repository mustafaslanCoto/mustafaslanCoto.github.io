<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.29">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>rl_presentation – Home</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-0815c480559380816a4d1ea211a47e91.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-fe4309c7225341acc1ac1ef082ecc45b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../../styles.css">
<meta property="og:title" content="Home">
<meta property="og:image" content="images_rl/cu.jpg">
<meta property="og:site_name" content="Home">
<meta property="og:image:alt" content="Left Logo">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Home</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../learn/index.html"> 
<span class="menu-text">Learn</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../workshops/index.html"> 
<span class="menu-text">Workshops</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../CV/CV.html"> 
<span class="menu-text">CV</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
    <a href="https://github.com/mustafaslanCoto" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#agenda" id="toc-agenda" class="nav-link active" data-scroll-target="#agenda">Agenda</a></li>
  <li><a href="#what-is-reinforcement-learning" id="toc-what-is-reinforcement-learning" class="nav-link" data-scroll-target="#what-is-reinforcement-learning">What is reinforcement learning</a>
  <ul class="collapse">
  <li><a href="#reinforcement-learning" id="toc-reinforcement-learning" class="nav-link" data-scroll-target="#reinforcement-learning">Reinforcement Learning</a></li>
  </ul></li>
  <li><a href="#elements-of-reinforcement-learning" id="toc-elements-of-reinforcement-learning" class="nav-link" data-scroll-target="#elements-of-reinforcement-learning">Elements of Reinforcement Learning</a>
  <ul class="collapse">
  <li><a href="#elements-of-reinforcement-learning-1" id="toc-elements-of-reinforcement-learning-1" class="nav-link" data-scroll-target="#elements-of-reinforcement-learning-1">Elements of Reinforcement Learning</a></li>
  <li><a href="#elements-of-reinforcement-learning-2" id="toc-elements-of-reinforcement-learning-2" class="nav-link" data-scroll-target="#elements-of-reinforcement-learning-2">Elements of Reinforcement Learning</a></li>
  <li><a href="#goals-and-rewards" id="toc-goals-and-rewards" class="nav-link" data-scroll-target="#goals-and-rewards">Goals and Rewards</a></li>
  <li><a href="#policies-and-value-functions" id="toc-policies-and-value-functions" class="nav-link" data-scroll-target="#policies-and-value-functions">Policies and Value Functions</a></li>
  </ul></li>
  <li><a href="#categories-of-rl-methods" id="toc-categories-of-rl-methods" class="nav-link" data-scroll-target="#categories-of-rl-methods">Categories of RL methods</a>
  <ul class="collapse">
  <li><a href="#categories-of-reinforcement-learning-methods" id="toc-categories-of-reinforcement-learning-methods" class="nav-link" data-scroll-target="#categories-of-reinforcement-learning-methods">Categories of reinforcement learning methods</a></li>
  <li><a href="#model-based-methods-dynamic-programming" id="toc-model-based-methods-dynamic-programming" class="nav-link" data-scroll-target="#model-based-methods-dynamic-programming">Model-based methods (Dynamic Programming)</a></li>
  <li><a href="#model-based-methods-dynamic-programming-1" id="toc-model-based-methods-dynamic-programming-1" class="nav-link" data-scroll-target="#model-based-methods-dynamic-programming-1">Model-based methods (Dynamic Programming)</a></li>
  <li><a href="#model-based-methods-dynamic-programming-2" id="toc-model-based-methods-dynamic-programming-2" class="nav-link" data-scroll-target="#model-based-methods-dynamic-programming-2">Model-based methods (Dynamic Programming)</a></li>
  <li><a href="#policy-evalulation-prediction" id="toc-policy-evalulation-prediction" class="nav-link" data-scroll-target="#policy-evalulation-prediction">Policy Evalulation (Prediction)</a></li>
  <li><a href="#policy-evalulation-prediction-1" id="toc-policy-evalulation-prediction-1" class="nav-link" data-scroll-target="#policy-evalulation-prediction-1">Policy Evalulation (Prediction)</a></li>
  <li><a href="#policy-improvement" id="toc-policy-improvement" class="nav-link" data-scroll-target="#policy-improvement">Policy Improvement</a></li>
  <li><a href="#policy-iteration" id="toc-policy-iteration" class="nav-link" data-scroll-target="#policy-iteration">Policy Iteration</a></li>
  <li><a href="#policy-iteration-1" id="toc-policy-iteration-1" class="nav-link" data-scroll-target="#policy-iteration-1">Policy Iteration</a></li>
  <li><a href="#value-iteration" id="toc-value-iteration" class="nav-link" data-scroll-target="#value-iteration">Value Iteration</a></li>
  <li><a href="#value-iteration-1" id="toc-value-iteration-1" class="nav-link" data-scroll-target="#value-iteration-1">Value Iteration</a></li>
  </ul></li>
  <li><a href="#lets-code" id="toc-lets-code" class="nav-link" data-scroll-target="#lets-code">Let’s code</a>
  <ul class="collapse">
  <li><a href="#generalized-policy-iteration" id="toc-generalized-policy-iteration" class="nav-link" data-scroll-target="#generalized-policy-iteration">Generalized Policy Iteration</a></li>
  </ul></li>
  <li><a href="#model-free-methods" id="toc-model-free-methods" class="nav-link" data-scroll-target="#model-free-methods">Model-free methods</a></li>
  <li><a href="#monte-carlo-methods" id="toc-monte-carlo-methods" class="nav-link" data-scroll-target="#monte-carlo-methods">Monte Carlo Methods</a>
  <ul class="collapse">
  <li><a href="#monte-carlo-mc-methods" id="toc-monte-carlo-mc-methods" class="nav-link" data-scroll-target="#monte-carlo-mc-methods">Monte Carlo (MC) Methods</a></li>
  <li><a href="#monte-carlo-prediction" id="toc-monte-carlo-prediction" class="nav-link" data-scroll-target="#monte-carlo-prediction">Monte Carlo Prediction</a></li>
  <li><a href="#monte-carlo-control" id="toc-monte-carlo-control" class="nav-link" data-scroll-target="#monte-carlo-control">Monte Carlo Control</a></li>
  </ul></li>
  <li><a href="#lets-code-1" id="toc-lets-code-1" class="nav-link" data-scroll-target="#lets-code-1">Let’s code</a>
  <ul class="collapse">
  <li><a href="#monte-carlo-control-without-exploring-starts" id="toc-monte-carlo-control-without-exploring-starts" class="nav-link" data-scroll-target="#monte-carlo-control-without-exploring-starts">Monte Carlo Control without Exploring Starts</a></li>
  <li><a href="#monte-carlo-control-without-exploring-starts-1" id="toc-monte-carlo-control-without-exploring-starts-1" class="nav-link" data-scroll-target="#monte-carlo-control-without-exploring-starts-1">Monte Carlo Control without Exploring Starts</a></li>
  </ul></li>
  <li><a href="#temporal-difference-learning" id="toc-temporal-difference-learning" class="nav-link" data-scroll-target="#temporal-difference-learning">Temporal-Difference Learning</a>
  <ul class="collapse">
  <li><a href="#temporal-difference-learning-1" id="toc-temporal-difference-learning-1" class="nav-link" data-scroll-target="#temporal-difference-learning-1">Temporal-Difference Learning</a></li>
  <li><a href="#td-prediction" id="toc-td-prediction" class="nav-link" data-scroll-target="#td-prediction">TD Prediction</a></li>
  <li><a href="#td-prediction-1" id="toc-td-prediction-1" class="nav-link" data-scroll-target="#td-prediction-1">TD Prediction</a></li>
  <li><a href="#sarsa-on-policy-td-control" id="toc-sarsa-on-policy-td-control" class="nav-link" data-scroll-target="#sarsa-on-policy-td-control">Sarsa: On-policy TD Control</a></li>
  </ul></li>
  <li><a href="#lets-code-2" id="toc-lets-code-2" class="nav-link" data-scroll-target="#lets-code-2">Let’s code</a>
  <ul class="collapse">
  <li><a href="#q-learning-off-policy-td-control" id="toc-q-learning-off-policy-td-control" class="nav-link" data-scroll-target="#q-learning-off-policy-td-control">Q-learning: Off-policy TD Control</a></li>
  </ul></li>
  <li><a href="#lets-code-3" id="toc-lets-code-3" class="nav-link" data-scroll-target="#lets-code-3">Let’s code</a>
  <ul class="collapse">
  <li><a href="#recommended-materials" id="toc-recommended-materials" class="nav-link" data-scroll-target="#recommended-materials">Recommended materials</a>
  <ul class="collapse">
  <li><a href="#readings" id="toc-readings" class="nav-link" data-scroll-target="#readings">Readings:</a></li>
  <li><a href="#tutoriols" id="toc-tutoriols" class="nav-link" data-scroll-target="#tutoriols">Tutoriols:</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#any-questions-or-thoughts" id="toc-any-questions-or-thoughts" class="nav-link" data-scroll-target="#any-questions-or-thoughts">Any questions or thoughts?</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="RL_presentation.html"><i class="bi bi-file-slides"></i>RevealJS</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<!-- logo style -->
<style>
  .logo-left {
    position: absolute;
    top: 1px;
    left: 1px;
  }

  .logo-right {
    position: absolute;
    top: 1px;
    right: 1px;
  }

  .logo-left img,
  .logo-right img {
    height: 140px;
    width: auto;
    
  }
  
/* title sytle */
  h1 {
    text-align: center;
  }

</style>
<div class="logo-left">
<p><img src="images_rl/cu.jpg" alt="Left Logo"></p>
</div>
<div class="logo-right">
<p><img src="images_rl/dlsg.png" alt="Right Logo"></p>
</div>
<br> <br> <br> <br> <br> <br>
<h1>
An Introduction to Reinforcement Learning
</h1>
<div class="title-block">
<br> <br> <br> <br>
<h3 class="anchored">
Mustafa Aslan, PhD Researcher <br> Data Lab for Social Good Research Lab <br> Cardiff University, UK <br> <span style="color: rgba(179, 0, 0, 0.7); font-weight: bold;">Slides:</span> <a href="https://mustafaslancoto.github.io/workshops/" style="color: #000; font-weight: bold; text-decoration: underline dotted;text-underline-offset: 5px"> https://mustafaslancoto.github.io/workshops/ </a>
</h3>
<p>
15 May 2025
</p>
</div>
<section id="agenda" class="level2">
<h2 class="anchored" data-anchor-id="agenda">Agenda</h2>
<ul>
<li>What is Reinforcement Learning</li>
<li>Elements of Reinforcement Learning</li>
<li>Categories of RL methods
<ul>
<li>Model-based methods <span class="math inline">\(\approx\)</span> Dynamic Programming</li>
<li>Model-Free Methods
<ul>
<li>Monte Carlo</li>
<li>Sarsa</li>
<li>Q-learning</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="what-is-reinforcement-learning" class="level1">
<h1>What is reinforcement learning</h1>
<section id="reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="reinforcement-learning">Reinforcement Learning</h2>
<ul>
<li>Reinforcement learning is learning what to do—how to map situations to actions to maximize a numerical reward signal.</li>
<li>The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them.</li>
</ul>
</section>
</section>
<section id="elements-of-reinforcement-learning" class="level1">
<h1>Elements of Reinforcement Learning</h1>
<section id="elements-of-reinforcement-learning-1" class="level2">
<h2 class="anchored" data-anchor-id="elements-of-reinforcement-learning-1">Elements of Reinforcement Learning</h2>
<ul>
<li><p><em>Agent</em> is the learner and decision maker.</p>
<ul>
<li>It interacts with the environment (comprising everything outside the agent) continually to select actions<br>
</li>
<li>The environment responds to these actions and presents new situations to the agent.</li>
</ul></li>
<li><p>A <em>policy</em> defines the learning agent’s way of behaving at a given time.</p>
<ul>
<li>A mapping from states of the environment to actions to be taken when in those states.</li>
</ul></li>
<li><p>The <em>reward</em> signal indicates what is good immediately after each decision time.</p></li>
<li><p>A <em>value function</em> specifies what is good in the long run.</p>
<ul>
<li>The value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state.</li>
</ul></li>
</ul>
</section>
<section id="elements-of-reinforcement-learning-2" class="level2">
<h2 class="anchored" data-anchor-id="elements-of-reinforcement-learning-2">Elements of Reinforcement Learning</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images_rl/agent_environment.png" class="r-stretch img-fluid figure-img"></p>
<figcaption>The agent–environment interaction in a decision process</figcaption>
</figure>
</div>
<p>Agent gives rise to a <code>sequence</code> that begins like this</p>
<p><span class="math display">\[
S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \dots  S_t, A_t, R_{t+1}
\]</span></p>
</section>
<section id="goals-and-rewards" class="level2 smaller">
<h2 class="smaller anchored" data-anchor-id="goals-and-rewards">Goals and Rewards</h2>
<ul>
<li>At each time step, the reward is a simple number, <span class="math inline">\(R_t \in \mathbb{R}\)</span> passing from the environment to the agent.</li>
<li>The agent’s goal is to maximize the total (cumulative) amount of reward it receives in the long run.</li>
<li>The aim is to maximize the <em><code>expected return</code></em>, <span class="math inline">\(G_t\)</span>, which is defined as: <span class="math display">\[
G_t \doteq R_{t+1} + R_{t+2} + R_{t+3}+ \dots+R_T
\]</span></li>
</ul>
<p><em>Discounting</em> technique is used to prioritize immediate rewards over future rewards.</p>
<ul>
<li>The idea is to multiply future rewards by a discount factor <span class="math inline">\(\gamma \in (0,1]\)</span></li>
<li>This makes future rewards worth less than immediate rewards.</li>
<li>The return <span class="math inline">\(G_t\)</span> with discounting is defined as:</li>
</ul>
<p><span class="math display">\[
G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\]</span></p>
</section>
<section id="policies-and-value-functions" class="level2">
<h2 class="anchored" data-anchor-id="policies-and-value-functions">Policies and Value Functions</h2>
<ul>
<li><p>A <strong>policy</strong> is a mapping from states to probabilities of selecting each possible action. If the agent is following policy <span class="math inline">\(\pi\)</span> at time <span class="math inline">\(t\)</span>, then <span class="math inline">\(\pi(a \mid s)\)</span> is the probability that <span class="math inline">\(A_t = a\)</span> if <span class="math inline">\(S_t = s\)</span></p></li>
<li><p><em><code>Value functions—functions of states (or of state–action pairs)</code></em> estimate <em>how good</em> it is for the agent to be in a given state (or how good it is to perform a given action in a given state)</p>
<ul>
<li>The <em>value function</em> of a state <span class="math inline">\(s\)</span> under a policy <span class="math inline">\(\pi\)</span>, denoted <span class="math inline">\(v_\pi(s)\)</span> is: <span class="math display">\[
v_\pi(s) \doteq \mathbb{E}_\pi[G_t | S_t=s] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t=s\right], \text{ for all } s \in \mathbf{S},
\]</span></li>
</ul></li>
</ul>
<p>where <span class="math inline">\(\mathbb{E}[\cdot]\)</span> denotes the expected value of a random variable given that the agent follows policy <span class="math inline">\(\pi\)</span>, and <span class="math inline">\(t\)</span> is any time step.</p>
</section>
</section>
<section id="categories-of-rl-methods" class="level1">
<h1>Categories of RL methods</h1>
<section id="categories-of-reinforcement-learning-methods" class="level2 smaller">
<h2 class="smaller anchored" data-anchor-id="categories-of-reinforcement-learning-methods">Categories of reinforcement learning methods</h2>
<div class="columns">
<div class="column" style="width:50%;">
<section id="model-based" class="level3">
<h3 class="anchored" data-anchor-id="model-based"><code>Model-based</code></h3>
<div class="fragment">
<ul>
<li>The agent knows/learns the model of the environment</li>
<li>They then compute the policy using the ADP methods or the model-free methods on simulated data</li>
</ul>
<p><strong>Pros:</strong></p>
<ul>
<li>Sample efficient</li>
<li>Safer exploration</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Prone to the model errors</li>
<li>Learning a model is challenging</li>
</ul>
</div>
</section>
</div><div class="column" style="width:50%;">
<section id="model-free" class="level3">
<h3 class="anchored" data-anchor-id="model-free"><code>Model-free</code></h3>
<div class="fragment">
<ul>
<li>The agent does not know the model of the environment</li>
<li>They learn the values or policies from trial-and-error interactions with the environment</li>
</ul>
<p><strong>Pros:</strong></p>
<ul>
<li>Do not need a model</li>
<li>Flexible</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Sample inefficient: requires a lot of interactions with the environment</li>
<li>Slow convergence</li>
</ul>
</div>
</section>
</div>
</div>
</section>
<section id="model-based-methods-dynamic-programming" class="level2">
<h2 class="anchored" data-anchor-id="model-based-methods-dynamic-programming">Model-based methods (Dynamic Programming)</h2>
<p>Conditional probability of the next state given the current state and action taken <span class="math display">\[
p(s' \mid s, a) \doteq \Pr \{ S_t = s' \mid S_{t-1} = s, A_{t-1} = a \}, \text{ for all } s',s \in S, \text{and} a \in A(s)
\]</span></p>
<p>The function <span class="math inline">\(p\)</span> defines the <em>dynamics</em> of the MDP</p>
<p><span class="math inline">\(p(s', \mid s, a)\)</span> is a <em>dynamics function</em> and <span class="math inline">\(p\)</span> specifies a probability distribution for each choice of <span class="math inline">\(s\)</span> and <span class="math inline">\(a\)</span>, that is: <span class="math display">\[
\sum_{s'\in S}p(s'\mid s, a) = 1, \text{ for all } s \in S, a \in A(s)
\]</span></p>
</section>
<section id="model-based-methods-dynamic-programming-1" class="level2">
<h2 class="anchored" data-anchor-id="model-based-methods-dynamic-programming-1">Model-based methods (Dynamic Programming)</h2>
<ul>
<li>They are used to derive optimal policies and value functions in Markov Decision Processes (MDPs) and reinforcement learning.</li>
</ul>
<section id="bellman-equation" class="level4">
<h4 class="anchored" data-anchor-id="bellman-equation">Bellman Equation</h4>
<ul>
<li>The Bellman equations express the relationship between the value of a state and the values of its successor states. For the state-value function <span class="math inline">\(v_\pi\)</span>, the Bellman equation is:</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
v_\pi(s) &amp;\doteq \mathbb{E}_\pi \big[G_t \mid S_t = s \big] \\
         &amp;= \mathbb{E}_\pi \big[R_{t+1} + \gamma G_{t+1} \mid S_t = s \big] \\
         &amp;= \mathbb{E}_\pi \big[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s \big] \\
         &amp;= \sum_a \pi(a \mid s) \sum_{s',r} p(s', r \mid s, a) \big[ r + \gamma v_\pi(s') \big].
\end{aligned}
\]</span></p>
</section>
</section>
<section id="model-based-methods-dynamic-programming-2" class="level2 smaller">
<h2 class="smaller anchored" data-anchor-id="model-based-methods-dynamic-programming-2">Model-based methods (Dynamic Programming)</h2>
<ul>
<li><p>An optimal policy is a policy that achieves the maximum expected return from any initial state.</p></li>
<li><p>The optimal state-value function <span class="math inline">\(v_*\)</span> is the maximum value function over all policies: <span class="math display">\[
v_*(s) = \max_\pi v_\pi(s), \text{ for all } s \in S
\]</span></p></li>
<li><p>The optimal action-value function <span class="math inline">\(q_*\)</span> is the maximum action-value function over all policies: <span class="math display">\[
q_*(s, a) = \max_\pi q_\pi(s, a), \text{ for all } s \in S, a \in A(s)
\]</span></p></li>
</ul>
<p>We use Dynamic Programming (DP) to leverage value functions in the search for good policies.</p>
<ul>
<li><p>The Bellman optimality equation for <span class="math inline">\(v_*\)</span> is: <span class="math display">\[
\begin{aligned}
v_*(s) &amp;= \max_a \mathbb{E} \big[R_{t+1} + \gamma v_*(S_{t+1}) \mid S_{t} = s, A_{t} = a  \big] \\
    &amp;= \max_a \sum_{s',r}p(s',r|s,a) \big[r+\gamma v_*(s')],
\end{aligned}
\]</span></p></li>
<li><p>The Bellman optimality equation for <span class="math inline">\(q_*\)</span> is: <span class="math display">\[
q_*(s, a) = \sum_{s', r} p(s', r \mid s, a) \big[ r + \gamma \max_{a'} q_*(s', a') \big]
\]</span></p></li>
</ul>
</section>
<section id="policy-evalulation-prediction" class="level2 smaller">
<h2 class="smaller anchored" data-anchor-id="policy-evalulation-prediction">Policy Evalulation (Prediction)</h2>
<p><em>Policy evaluation</em> is the computation of the state-value function <span class="math inline">\(v_\pi\)</span> for an arbitrary policy <span class="math inline">\(\pi\)</span>. We also refer to it as the *prediction problem. <span class="math display">\[
\begin{aligned}
v_\pi(s) &amp;\doteq \mathbb{E}_\pi \big[G_t \mid S_t = s \big] \\
         &amp;= \mathbb{E}_\pi \big[R_{t+1} + \gamma G_{t+1} \mid S_t = s \big] \\
         &amp;= \mathbb{E}_\pi \big[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s \big] \\
         &amp;= \sum_a \pi(a \mid s) \sum_{s',r} p(s', r \mid s, a) \big[ r + \gamma v_\pi(s') \big].
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\pi(a|s)\)</span> is the probability of taking action <span class="math inline">\(\alpha\)</span> in state <span class="math inline">\(s\)</span> under policy <span class="math inline">\(\pi\)</span>, and the expectations are subscripted by <span class="math inline">\(\pi\)</span> to indicate that they are conditional on <span class="math inline">\(\pi\)</span> being followed.</p>
<p>Consider a sequence of approximate value functions <span class="math inline">\(v_0, v_1, v_2,\dots,\)</span>. Each successive approximation is obtained by using the Bellman equation for <span class="math inline">\(v_\pi\)</span> as an update rule:</p>
<p><span class="math display">\[
\begin{aligned}
v_{k+1}(s) &amp;\doteq \mathbb{E}_\pi \big[R_{t+1} + \gamma v_k(S_{t+1}) \mid S_t = s \big] \\
         &amp;= \sum_a \pi(a \mid s) \sum_{s',r} p(s', r \mid s, a) \big[ r + \gamma v_k(s') \big].
\end{aligned}
\]</span></p>
</section>
<section id="policy-evalulation-prediction-1" class="level2">
<h2 class="anchored" data-anchor-id="policy-evalulation-prediction-1">Policy Evalulation (Prediction)</h2>
<p>The sequence <span class="math inline">\({v_k}\)</span> can be shown in general to converge to <span class="math inline">\(v_\pi\)</span> as <span class="math inline">\(k \rightarrow \infty\)</span> under the same conditions that guarantee the existence of <span class="math inline">\(v_\pi\)</span>. This algorithm is called <em>iterative policy evaluation</em>.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Iterative Policy Evaluation, for estimating <span class="math inline">\(V \approx v_\pi\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Input:</strong><br>
<span class="math inline">\(\pi\)</span>, the policy to be evaluated<br>
Algorithm parameter: a small threshold <span class="math inline">\(\theta &gt; 0\)</span> determining accuracy of estimation<br>
Initialize <span class="math inline">\(V(s)\)</span> arbitrarily, for <span class="math inline">\(s \in S\)</span>, and <span class="math inline">\(V(\text{terminal}) = 0\)</span></p>
<p><strong>Loop:</strong><br>
<span class="math inline">\(\Delta \leftarrow 0\)</span><br>
Loop for each <span class="math inline">\(s \in S\)</span>:<br>
 <span class="math inline">\(v \leftarrow V(s)\)</span><br>
 <span class="math inline">\(V(s) \leftarrow \sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma V(s')]\)</span><br>
 <span class="math inline">\(\Delta \leftarrow \max(\Delta, |v - V(s)|)\)</span><br>
<strong>until</strong> <span class="math inline">\(\Delta &lt; \theta\)</span></p>
</div>
</div>
</section>
<section id="policy-improvement" class="level2">
<h2 class="anchored" data-anchor-id="policy-improvement">Policy Improvement</h2>
<div class="columns">
<div class="column fragment" style="font-size: 70%;">
<ul>
<li>We know how good it is to follow the current policy from <span class="math inline">\(s\)</span>—that is <span class="math inline">\(v_\pi(s)\)</span>—but would it be better or worse to change to the new policy?</li>
</ul>
<p>One way to answer this question is to consider selecting <span class="math inline">\(a\)</span> in <span class="math inline">\(s\)</span> and thereafter following the existing policy <span class="math inline">\(\pi\)</span>.</p>
<p>This leads to the definition of the <em>q-value</em> of a state-action pair:</p>
<p><span class="math display">\[
\begin{aligned}
q_\pi(s, a) &amp;\doteq \mathbb{E} \big[ R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t = a \big] \\
     &amp;= \sum_{s', r} p(s', r \mid s, a) \big[ r + \gamma v_\pi(s') \big].
\end{aligned}
\]</span></p>
</div><div class="column fragment" style="font-size: 70%;">
<p>The policy improvement theorem states that if we improve the policy by acting greedily with respect to <span class="math inline">\(q_\pi\)</span>, the new policy <span class="math inline">\(\pi'\)</span> will be at least as good as <span class="math inline">\(\pi\)</span>.</p>
<p>Formally, if</p>
<p><span class="math display">\[
\begin{aligned}
\pi'(s) &amp;= \arg\max_a q_\pi(s, a) \\
&amp;= \arg \max_a \mathbb{E} \big[R_{t+1}+\gamma v_\pi(S_{t+1}) \mid S_t=s, A_t=a \big] \\
&amp;= \arg \max_a \sum_{s',r}p(s',r \mid s, a) \big[r+\gamma v_\pi(s') \big]
\end{aligned}
\]</span></p>
<p>then</p>
<p><span class="math display">\[
v_{\pi'}(s) \geq v_\pi(s).
\]</span></p>
<p>for all <span class="math inline">\(s \in S\)</span>.</p>
</div>
</div>
</section>
<section id="policy-iteration" class="level2">
<h2 class="anchored" data-anchor-id="policy-iteration">Policy Iteration</h2>
<p>Once a policy, <span class="math inline">\(\pi\)</span>, has been improved using <span class="math inline">\(v_\pi\)</span> to yield a better policy, <span class="math inline">\(\pi^{'}\)</span>, we can then compute <span class="math inline">\(v_{\pi^{'}}\)</span> and improve it again to yield an even better <span class="math inline">\(\pi^{''}\)</span>.</p>
<p><span class="math display">\[
\pi_0 \xrightarrow E v_{\pi_0} \xrightarrow I \pi_1 \xrightarrow E v_{\pi_1}\xrightarrow I \pi_2,\dots \xrightarrow I \pi_* \xrightarrow E v_*
\]</span></p>
<p>where <span class="math inline">\(\xrightarrow E\)</span> denotes a policy <em>evaluation</em> and <span class="math inline">\(\xrightarrow I\)</span> denotes a policy <em>improvement</em>. This way of finding an optimal policy is called policy iteration.</p>
</section>
<section id="policy-iteration-1" class="level2 smaller">
<h2 class="smaller anchored" data-anchor-id="policy-iteration-1">Policy Iteration</h2>
<p>A complete <em>policy iteration</em> algorithm</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Policy Iteration (using iterative policy evaluation) for estimating <span class="math inline">\(\pi \approx \pi_*\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>1. Initialization</strong><br>
<span class="math inline">\(V(s) \in \mathbb{R}\)</span> and <span class="math inline">\(\pi(s) \in A(s)\)</span> arbitrarily for all <span class="math inline">\(s \in S\)</span><br>
<span class="math inline">\(V(\text{terminal}) \doteq 0\)</span></p>
<p><strong>2. Policy Evaluation</strong><br>
Loop:<br>
 <span class="math inline">\(\Delta \leftarrow 0\)</span><br>
 Loop for each <span class="math inline">\(s \in S\)</span>:<br>
  <span class="math inline">\(v \leftarrow V(s)\)</span><br>
  <span class="math inline">\(V(s) \leftarrow \sum_{s', r} p(s', r \mid s, \pi(s)) \left[r + \gamma V(s')\right]\)</span><br>
  <span class="math inline">\(\Delta \leftarrow \max(\Delta, |v - V(s)|)\)</span><br>
Until <span class="math inline">\(\Delta &lt; \theta\)</span> (a small positive number determining the accuracy of estimation)</p>
<p><strong>3. Policy Improvement</strong><br>
<em>policy-stable</em> <span class="math inline">\(\leftarrow\)</span> true<br>
For each <span class="math inline">\(s \in S\)</span>:<br>
 <em>old-action</em> <span class="math inline">\(\leftarrow \pi(s)\)</span><br>
 <span class="math inline">\(\pi(s) \leftarrow \arg\max_a \sum_{s', r} p(s', r \mid s, a)\left[r + \gamma V(s')\right]\)</span><br>
 If <em>old-action</em> <span class="math inline">\(\ne \pi(s)\)</span>, then <em>policy-stable</em> <span class="math inline">\(\leftarrow\)</span> false</p>
<p>If <em>policy-stable</em>, then stop and return <span class="math inline">\(V \approx v_*\)</span> and <span class="math inline">\(\pi \approx \pi_*\)</span>;<br>
Else go to step 2</p>
</div>
</div>
</section>
<section id="value-iteration" class="level2">
<h2 class="anchored" data-anchor-id="value-iteration">Value Iteration</h2>
<ul>
<li>One drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set.</li>
<li>Value iteration is a special case of policy iteration where the policy evaluation step is truncated to just one sweep.</li>
<li>This algorithm combines the policy improvement and truncated policy evaluation steps into a single update operation:</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
v_{k+1}(s) &amp;= \mathbb{E} \big[R_{t+1} + \gamma v_k(S_{t+1}) \mid S_t=s, A_t = a \big] \\
       &amp;=\max_a \sum_{s', r} p(s', r \mid s, a) \big[ r + \gamma v_k(s') \big]
\end{aligned}
\]</span></p>
<p>for all <span class="math inline">\(s \in S\)</span>.</p>
</section>
<section id="value-iteration-1" class="level2 smaller">
<h2 class="smaller anchored" data-anchor-id="value-iteration-1">Value Iteration</h2>
<ul>
<li>Value iteration is obtained simply by turning the Bellman optimality equation into an update rule.</li>
<li>Also note how the value iteration update is identical to the policy evaluation update except that it requires the maximum to be taken over all actions.</li>
</ul>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Value Iteration, for estimating <span class="math inline">\(\pi \approx \pi_*\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Algorithm parameter:</strong><br>
A small threshold <span class="math inline">\(\theta &gt; 0\)</span> determining the accuracy of estimation</p>
<p><strong>Initialization:</strong><br>
Initialize <span class="math inline">\(V(s)\)</span> arbitrarily for all <span class="math inline">\(s \in S^+\)</span>, except that <span class="math inline">\(V(\text{terminal}) = 0\)</span></p>
<p><strong>Loop:</strong><br>
 <span class="math inline">\(\Delta \leftarrow 0\)</span><br>
 Loop for each <span class="math inline">\(s \in S\)</span>:<br>
  <span class="math inline">\(v \leftarrow V(s)\)</span><br>
  <span class="math inline">\(V(s) \leftarrow \max_a \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma V(s') \right]\)</span><br>
  <span class="math inline">\(\Delta \leftarrow \max(\Delta, |v - V(s)|)\)</span><br>
<strong>until</strong> <span class="math inline">\(\Delta &lt; \theta\)</span></p>
<p><strong>Output:</strong><br>
A deterministic policy, <span class="math inline">\(\pi \approx \pi_*\)</span>, such that<br>
 <span class="math inline">\(\pi(s) = \arg\max_a \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma V(s') \right]\)</span></p>
</div>
</div>
</section>
</section>
<section id="lets-code" class="level1">
<h1>Let’s code</h1>
<section id="generalized-policy-iteration" class="level2 smaller">
<h2 class="smaller anchored" data-anchor-id="generalized-policy-iteration">Generalized Policy Iteration</h2>
<div class="columns">
<div class="column fragment" style="font-size: 100%;">
<ul>
<li><p>GPI is a general idea that describes how two processes — <em>evaluating a policy and improving it</em> — work together and influence each other.</p></li>
<li><p>Most reinforcement learning algorithms use a value function to judge how good a policy is, and then update the policy based on that judgment.</p></li>
<li><p>If both value estimation and policy improvement stabilize (i.e., stop changing), then the policy must be the best possible one for that value function—meaning the policy is optimal.</p></li>
</ul>
</div><div class="column fragment" style="font-size: 100%;">
<p><img src="images_rl/eval_greed_diag.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="model-free-methods" class="level1">
<h1>Model-free methods</h1>
<div class="center-text">
<ul>
<li><strong>Monte Carlo Methods</strong></li>
<li><strong>Sarsa</strong></li>
<li><strong>Q-learning</strong></li>
</ul>
</div>
</section>
<section id="monte-carlo-methods" class="level1">
<h1>Monte Carlo Methods</h1>
<section id="monte-carlo-mc-methods" class="level2">
<h2 class="anchored" data-anchor-id="monte-carlo-mc-methods">Monte Carlo (MC) Methods</h2>
<ul>
<li><p>The term “Monte Carlo” is often used more broadly for any estimation method whose operation involves a significant random component.</p></li>
<li><p>MC methods solve reinforcement learning problems by averaging results (returns) from sampled experiences sequences of states, actions, and rewards.</p></li>
<li><p>They do not require knowledge of the environment’s dynamics, making them powerful for learning from real or simulated experiences.</p></li>
</ul>
</section>
<section id="monte-carlo-prediction" class="level2 smaller">
<h2 class="smaller anchored" data-anchor-id="monte-carlo-prediction">Monte Carlo Prediction</h2>
<ul>
<li>Suppose we wish to estimate <span class="math inline">\(v_{\pi}(s)\)</span>, the values of a state <span class="math inline">\(s\)</span> under policy <span class="math inline">\(\pi\)</span>, given a set of episodes obtained by following <span class="math inline">\(\pi\)</span> and passing through <span class="math inline">\(s\)</span>.</li>
</ul>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
First-Visit Monte Carlo Prediction (for estimating <span class="math inline">\(V \approx v_\pi\)</span>)
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Input:</strong><br>
A policy <span class="math inline">\(\pi\)</span> to be evaluated</p>
<p><strong>Initialize:</strong><br>
 <span class="math inline">\(V(s) \in \mathbb{R}\)</span> arbitrarily, for all <span class="math inline">\(s \in S\)</span><br>
 <span class="math inline">\(\text{Returns}(s) \leftarrow\)</span> an empty list, for all <span class="math inline">\(s \in S\)</span></p>
<p><strong>Loop forever</strong> (for each episode):<br>
 Generate an episode following <span class="math inline">\(\pi\)</span>: <span class="math inline">\(S_0, A_0, R_1, S_1, A_1, R_2, \dots, S_{T-1}, A_{T-1}, R_T\)</span><br>
 <span class="math inline">\(G \leftarrow 0\)</span><br>
 Loop for each step of the episode, <span class="math inline">\(t = T-1, T-2, \dots, 0\)</span>:<br>
  <span class="math inline">\(G \leftarrow \gamma G + R_{t+1}\)</span><br>
  Unless <span class="math inline">\(S_t\)</span> appears in <span class="math inline">\(S_0, S_1, \dots, S_{t-1}\)</span>:<br>
   Append <span class="math inline">\(G\)</span> to <span class="math inline">\(\text{Returns}(S_t)\)</span><br>
   <span class="math inline">\(V(S_t) \leftarrow \text{average}(\text{Returns}(S_t))\)</span></p>
</div>
</div>
</section>
<section id="monte-carlo-control" class="level2 smaller">
<h2 class="smaller anchored" data-anchor-id="monte-carlo-control">Monte Carlo Control</h2>
<p>Alternating complete steps of policy evaluation and policy improvement are performed, beginning with an arbitrary policy <span class="math inline">\(\pi_0\)</span> and ending with the optimal policy and optimal action-value function:</p>
<p><span class="math display">\[
\pi_0 \xrightarrow E q_{\pi_0} \xrightarrow I \pi_1 \xrightarrow E q_{\pi_1}\xrightarrow I \pi_2,\dots,\xrightarrow I \pi_* \xrightarrow E q_*
\]</span></p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Monte Carlo ES (Exploring Starts), for estimating <span class="math inline">\(\pi \approx \pi_*\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Initialize:</strong><br>
<span class="math inline">\(\pi(s) \in A(s)\)</span> arbitrarily, for all <span class="math inline">\(s \in S\)</span><br>
<span class="math inline">\(Q(s, a) \in \mathbb{R}\)</span> arbitrarily, for all <span class="math inline">\(s \in S, a \in A(s)\)</span><br>
<span class="math inline">\(\text{Returns}(s, a) \leftarrow\)</span> empty list, for all <span class="math inline">\(s \in S, a \in A(s)\)</span></p>
<p><strong>Loop forever</strong> (for each episode):<br>
 Choose <span class="math inline">\(S_0 \in S, A_0 \in A(S_0)\)</span> randomly, such that all pairs have probability <span class="math inline">\(&gt; 0\)</span><br>
 Generate an episode from <span class="math inline">\(S_0, A_0\)</span>, following <span class="math inline">\(\pi\)</span>:<br>
  <span class="math inline">\(S_0, A_0, R_1, \dots, S_{T-1}, A_{T-1}, R_T\)</span><br>
 <span class="math inline">\(G \leftarrow 0\)</span><br>
 Loop for each step of episode, <span class="math inline">\(t = T-1, T-2, \dots, 0\)</span>:<br>
  <span class="math inline">\(G \leftarrow \gamma G + R_{t+1}\)</span><br>
  Unless the pair <span class="math inline">\((S_t, A_t)\)</span> appears in<br>
   <span class="math inline">\(S_0, A_0, S_1, A_1, \dots, S_{t-1}, A_{t-1}\)</span>:<br>
   Append <span class="math inline">\(G\)</span> to <span class="math inline">\(\text{Returns}(S_t, A_t)\)</span><br>
   <span class="math inline">\(Q(S_t, A_t) \leftarrow \text{average}(\text{Returns}(S_t, A_t))\)</span><br>
   <span class="math inline">\(\pi(S_t) \leftarrow \arg\max_a Q(S_t, a)\)</span></p>
</div>
</div>
</section>
</section>
<section id="lets-code-1" class="level1">
<h1>Let’s code</h1>
<section id="monte-carlo-control-without-exploring-starts" class="level2 smaller">
<h2 class="smaller anchored" data-anchor-id="monte-carlo-control-without-exploring-starts">Monte Carlo Control without Exploring Starts</h2>
<ul>
<li>In on-policy control methods the policy is generally <strong>soft</strong>, meaning that <span class="math inline">\(\pi(a\mid s)&gt;0\)</span> for all <span class="math inline">\(s \in S\)</span> and all <span class="math inline">\(a \in A(s)\)</span>, but gradually shifted closer and closer to a deterministic policy.</li>
<li>The on-policy method we present in this section uses <span class="math inline">\(\epsilon\)</span>-greedy policies, meaning that most of the time they choose an action that has maximal estimated action value, but with probability <span class="math inline">\(\epsilon\)</span> they instead select an action at random. That is, all nongreedy actions are given the minimal probability of selection, <span class="math inline">\(\frac{\epsilon}{|A(s)|}\)</span>, and the remaining bulk of the probability <span class="math inline">\(1-\epsilon+\frac{\epsilon}{|A(s)|}\)</span> is given to the greedy action.</li>
<li><span class="math inline">\(\epsilon\)</span>-greedy policies are examples of <span class="math inline">\(\epsilon-soft\)</span> policies, definied as policies for which <span class="math inline">\(\pi(a \mid s) \geq \frac{\epsilon}{|A(s)|}\)</span> for all states and actions, for some <span class="math inline">\(\epsilon &gt; 0\)</span>. Among <span class="math inline">\(\epsilon\)</span>-soft policies, <span class="math inline">\(\epsilon\)</span>-greedy policies are in some sense those that are closest to greedy.</li>
</ul>
</section>
<section id="monte-carlo-control-without-exploring-starts-1" class="level2 smaller">
<h2 class="smaller anchored" data-anchor-id="monte-carlo-control-without-exploring-starts-1">Monte Carlo Control without Exploring Starts</h2>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
On-policy First-Visit MC Control (for <span class="math inline">\(\varepsilon\)</span>-soft policies), estimates <span class="math inline">\(\pi \approx \pi_*\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Algorithm parameter:</strong><br>
Small <span class="math inline">\(\varepsilon &gt; 0\)</span></p>
<p><strong>Initialize:</strong><br>
<span class="math inline">\(\pi \leftarrow\)</span> an arbitrary <span class="math inline">\(\varepsilon\)</span>-soft policy<br>
<span class="math inline">\(Q(s, a) \in \mathbb{R}\)</span> arbitrarily, for all <span class="math inline">\(s \in S, a \in A(s)\)</span><br>
<span class="math inline">\(\text{Returns}(s, a) \leftarrow\)</span> empty list, for all <span class="math inline">\(s \in S, a \in A(s)\)</span></p>
<p><strong>Repeat forever</strong> (for each episode):<br>
 Generate an episode following <span class="math inline">\(\pi\)</span>: <span class="math inline">\(S_0, A_0, R_1, \dots, S_{T-1}, A_{T-1}, R_T\)</span><br>
 <span class="math inline">\(G \leftarrow 0\)</span><br>
 Loop for each step of episode, <span class="math inline">\(t = T-1, T-2, \dots, 0\)</span>:<br>
  <span class="math inline">\(G \leftarrow \gamma G + R_{t+1}\)</span><br>
  Unless the pair <span class="math inline">\((S_t, A_t)\)</span> appears in <span class="math inline">\(S_0, A_0, \dots, S_{t-1}, A_{t-1}\)</span>:<br>
   Append <span class="math inline">\(G\)</span> to <span class="math inline">\(\text{Returns}(S_t, A_t)\)</span><br>
   <span class="math inline">\(Q(S_t, A_t) \leftarrow \text{average}(\text{Returns}(S_t, A_t))\)</span><br>
   <span class="math inline">\(A^* \leftarrow \arg\max_a Q(S_t, a)\)</span> (ties broken arbitrarily)<br>
   For all <span class="math inline">\(a \in A(S_t)\)</span>:<br>
    <span class="math inline">\(\pi(a \mid S_t) \leftarrow \begin{cases}
1 - \varepsilon + \varepsilon / |A(S_t)| &amp; \text{if } a = A^* \\
\varepsilon / |A(S_t)| &amp; \text{if } a \ne A^*
\end{cases}\)</span></p>
</div>
</div>
<p>where <span class="math inline">\(|A(s)|\)</span> is the number of actions available in state <span class="math inline">\(s\)</span>.</p>
<p>The <span class="math inline">\(\epsilon\)</span>-greedy policy ensures that all actions are tried, but actions with higher value estimates are tried more frequently. This balances exploration (trying new actions) and exploitation (choosing the best-known action).</p>
</section>
</section>
<section id="temporal-difference-learning" class="level1">
<h1>Temporal-Difference Learning</h1>
<section id="temporal-difference-learning-1" class="level2">
<h2 class="anchored" data-anchor-id="temporal-difference-learning-1">Temporal-Difference Learning</h2>
<ul>
<li>TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas.</li>
<li>Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment’s dynamics.</li>
<li>Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap).</li>
</ul>
</section>
<section id="td-prediction" class="level2">
<h2 class="anchored" data-anchor-id="td-prediction">TD Prediction</h2>
<p>Whereas Monte Carlo methods must wait until the end of the episode to determine the increment to <span class="math inline">\(V(St)\)</span> (only then is <span class="math inline">\(G_t\)</span> known), TD methods need to wait only until the next time step. At time t + 1 they immediately form a target and make a useful update using the observed reward <span class="math inline">\(R_{t+1}\)</span> and the estimate <span class="math inline">\(V_{S_{t+1}}\)</span>. The simplest TD method makes the update:</p>
<p><span class="math display">\[
V(S_t) \leftarrow V(S_t) + \alpha \big[ R_{t+1} + \gamma V(S_{t+1})-V(S_t)  \big]
\]</span></p>
<p>immediately on transition to <span class="math inline">\(S_{t+1}\)</span> and receiving <span class="math inline">\(R_{t+1}\)</span>. In effect, the target for the Monte Carlo update is <span class="math inline">\(G_t\)</span>, whereas the target for the TD update is <span class="math inline">\(R_{t+1} + \gamma V(S_{t+1})\)</span>. This TD method is called TD(0), or <em>one-step</em> TD.</p>
</section>
<section id="td-prediction-1" class="level2">
<h2 class="anchored" data-anchor-id="td-prediction-1">TD Prediction</h2>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tabular TD(0) for Estimating <span class="math inline">\(v_\pi\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Input:</strong><br>
The policy <span class="math inline">\(\pi\)</span> to be evaluated</p>
<p><strong>Algorithm parameter:</strong><br>
Step size <span class="math inline">\(\alpha \in (0, 1]\)</span></p>
<p><strong>Initialize:</strong><br>
<span class="math inline">\(V(s)\)</span> arbitrarily for all <span class="math inline">\(s \in S^+\)</span>, except that <span class="math inline">\(V(\text{terminal}) = 0\)</span></p>
<p><strong>Loop for each episode:</strong><br>
 Initialize <span class="math inline">\(S\)</span><br>
 <strong>Loop for each step of episode:</strong><br>
  <span class="math inline">\(A \leftarrow\)</span> action given by <span class="math inline">\(\pi\)</span> for <span class="math inline">\(S\)</span><br>
  Take action <span class="math inline">\(A\)</span>, observe <span class="math inline">\(R\)</span>, <span class="math inline">\(S'\)</span><br>
  <span class="math inline">\(V(S) \leftarrow V(S) + \alpha \left[ R + \gamma V(S') - V(S) \right]\)</span><br>
  <span class="math inline">\(S \leftarrow S'\)</span><br>
 Until <span class="math inline">\(S\)</span> is terminal</p>
</div>
</div>
</section>
<section id="sarsa-on-policy-td-control" class="level2 smaller">
<h2 class="smaller anchored" data-anchor-id="sarsa-on-policy-td-control">Sarsa: On-policy TD Control</h2>
<p>we consider transitions from state–action pair to state–action pair, and learn the values of state–action pairs.</p>
<p><span class="math display">\[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \big[R_{t+1}+\gamma Q(S_{t+1}, A_{t+1})-Q(S_t, A_t) \big]
\]</span></p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
SARSA (On-Policy TD Control), for estimating <span class="math inline">\(Q \approx q_*\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Algorithm parameters:</strong><br>
Step size <span class="math inline">\(\alpha \in (0, 1]\)</span>, small <span class="math inline">\(\varepsilon &gt; 0\)</span></p>
<p><strong>Initialize:</strong><br>
<span class="math inline">\(Q(s, a)\)</span> arbitrarily, for all <span class="math inline">\(s \in S^+\)</span>, <span class="math inline">\(a \in A(s)\)</span><br>
<span class="math inline">\(Q(\text{terminal}, \cdot) = 0\)</span></p>
<p><strong>Loop for each episode:</strong><br>
 Initialize <span class="math inline">\(S\)</span><br>
 Choose <span class="math inline">\(A\)</span> from <span class="math inline">\(S\)</span> using a policy derived from <span class="math inline">\(Q\)</span> (e.g., <span class="math inline">\(\varepsilon\)</span>-greedy)</p>
<p> <strong>Loop for each step of episode:</strong><br>
  Take action <span class="math inline">\(A\)</span>, observe <span class="math inline">\(R\)</span>, <span class="math inline">\(S'\)</span><br>
  Choose <span class="math inline">\(A'\)</span> from <span class="math inline">\(S'\)</span> using a policy derived from <span class="math inline">\(Q\)</span> (e.g., <span class="math inline">\(\varepsilon\)</span>-greedy)<br>
  <span class="math inline">\(Q(S, A) \leftarrow Q(S, A) + \alpha \left[ R + \gamma Q(S', A') - Q(S, A) \right]\)</span><br>
  <span class="math inline">\(S \leftarrow S';\quad A \leftarrow A'\)</span></p>
<p>Until <span class="math inline">\(S\)</span> is terminal</p>
</div>
</div>
</section>
</section>
<section id="lets-code-2" class="level1">
<h1>Let’s code</h1>
<section id="q-learning-off-policy-td-control" class="level2 smaller">
<h2 class="smaller anchored" data-anchor-id="q-learning-off-policy-td-control">Q-learning: Off-policy TD Control</h2>
<p><em>Q-learning</em> is defined by</p>
<p><span class="math display">\[
Q(S_t, A_t) \leftarrow Q(S_t, A_t)+ \alpha \big[R_{t+1}+\gamma \max_aQ(S_{t+1}, a)-Q(S_t, A_t)  \big]
\]</span></p>
<p>The Q-learning algorithm is shown below in procedural form.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Q-learning (Off-Policy TD Control), for estimating <span class="math inline">\(\pi \approx \pi_*\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Algorithm parameters:</strong><br>
Step size <span class="math inline">\(\alpha \in (0, 1]\)</span>, small <span class="math inline">\(\varepsilon &gt; 0\)</span></p>
<p><strong>Initialize:</strong><br>
<span class="math inline">\(Q(s, a)\)</span> arbitrarily for all <span class="math inline">\(s \in S^+\)</span>, <span class="math inline">\(a \in S(s)\)</span>,<br>
except that <span class="math inline">\(Q(\text{terminal}, \cdot) = 0\)</span></p>
<p><strong>Loop for each episode:</strong><br>
 Initialize <span class="math inline">\(S\)</span><br>
 <strong>Loop for each step of episode:</strong><br>
  Choose <span class="math inline">\(A\)</span> from <span class="math inline">\(S\)</span> using a policy derived from <span class="math inline">\(Q\)</span> (e.g., <span class="math inline">\(\varepsilon\)</span>-greedy)<br>
  Take action <span class="math inline">\(A\)</span>, observe <span class="math inline">\(R\)</span>, <span class="math inline">\(S'\)</span><br>
  <span class="math inline">\(Q(S, A) \leftarrow Q(S, A) + \alpha \left[ R + \gamma \max_a Q(S', a) - Q(S, A) \right]\)</span><br>
  <span class="math inline">\(S \leftarrow S'\)</span></p>
<p>Until <span class="math inline">\(S\)</span> is terminal</p>
</div>
</div>
</section>
</section>
<section id="lets-code-3" class="level1">
<h1>Let’s code</h1>
<section id="recommended-materials" class="level2">
<h2 class="anchored" data-anchor-id="recommended-materials">Recommended materials</h2>
<section id="readings" class="level3">
<h3 class="anchored" data-anchor-id="readings">Readings:</h3>
<ul>
<li><a href="http://www.incompleteideas.net/book/the-book-2nd.html">Reinforcement Learning: An Introduction</a> by Richard S. Sutton and Andrew G. Barto</li>
<li><a href="https://castle.princeton.edu/rlso/">Reinforcement Learning and Stochastic Optimization</a> by Warren B. Powell</li>
</ul>
</section>
<section id="tutoriols" class="level3">
<h3 class="anchored" data-anchor-id="tutoriols">Tutoriols:</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0MNVhXEX9to" title="Reinforcement Learning: Machine Learning Meets Control Theory" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/i7q8bISGwMQ" title="Reinforcement Learning Series: Overview of Methods" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/sJIFUTITfBc" title="Model Based Reinforcement Learning: Policy Iteration, Value Iteration, and Dynamic Programming" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0iqz4tcKN58" title="Q-Learning: Model Free Reinforcement Learning and Temporal Difference Learning" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/wDVteayWWvU" title="Overview of Deep Reinforcement Learning Methods" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
</section>
</section>
<section id="any-questions-or-thoughts" class="level1">
<h1>Any questions or thoughts?</h1>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Made with ❤️ and Quarto</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>