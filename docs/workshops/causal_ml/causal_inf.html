<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.27">

  <title>MUSTAFA ASLAN – causal_inf</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-534cd8e3a96973385dffff3f4709048d.css">
  <link rel="stylesheet" href="style.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="MUSTAFA ASLAN">
<meta property="og:site_name" content="MUSTAFA ASLAN">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">


<section class="slide level2">

<!-- logo style -->
<style>
  .logo-left {
    position: absolute;
    top: 10px;
    left: 20px;
  }

  .logo-right {
    position: absolute;
    top: 10px;
    right: 20px;
    display: flex;
    gap: 10px;
    align-items: flex-start;
  }

  .logo-left img,
  .logo-right img {
    height: 180px;
    width: auto;
    
  }
  
/* title sytle */
  h1 {
    text-align: center;
  }

</style>
<div class="logo-left">
<p><img src="images/cu.png" alt="Left Logo"></p>
</div>
<div class="logo-right">
<p><img src="images/dlsg.png" alt="Right Logo"></p>
</div>
<br> <br> <br> <br> <br> <br> <br>
<h1>
Causal Inference with Machine Learning
</h1>
<div class="title-block">
<br> <br>
<h3>
Lecturers:
</h3>
<h5>
Amir Salimi Babamiri, Cardiff University, UK <br> Mustafa Aslan, Cardiff University, UK <br> Harsha Halgamuwe Hewage, Cardiff University, UK <br> Amirhossein Ghadiri, Cardiff University, UK <br> <br> Data Lab for Social Good, Cardiff University, UK <br> <!-- <span style="color: rgba(179, 0, 0, 0.7); font-weight: bold;">Slides:</span>
  <a href="https://mustafaslancoto.github.io/talks/" style="color: #000; font-weight: bold; text-decoration: underline dotted;text-underline-offset: 5px">
    https://mustafaslancoto.github.io/talks/"
  </a> -->
</h5>
<p>
13 Feb 2026
</p>
</div>
</section>
<section id="section" class="slide level2">
<h2></h2>
<p><br> <br></p>
<p><span class="mimic-h2">Outline</span></p>
<div style="font-weight: bold; font-size: 1.1em;">
<ul>
<li>Motivation and Conceptual Foundations</li>
<li>Identification</li>
<li>Randomised Controlled Trials (RCTs)</li>
<li>Introduction to Linear Regression</li>
<li>Regularization Techniques for Linear Regression in High-Dimensional Settings</li>
<li>Statistical Inference on Causal Effects in High Dimensional Linear Regression Models</li>
<li>Statistical Inference on Predictive and Causal Effects in Modern Nonlinear Regression Models</li>
</ul>
</div>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<p><br> <br></p>
<p><span class="mimic-h2">Outline</span></p>
<div style="font-weight: bold; font-size: 1.1em;">
<ul>
<li><span style="color: #1A6872;">Motivation and Conceptual Foundations</span></li>
<li>Identification</li>
<li>Randomised Controlled Trials (RCTs)</li>
<li>Introduction to Linear Regression</li>
<li>Regularization Techniques for Linear Regression in High-Dimensional Settings</li>
<li>Statistical Inference on Causal Effects in High Dimensional Linear Regression Models</li>
<li>Statistical Inference on Predictive and Causal Effects in Modern Nonlinear Regression Models</li>
</ul>
</div>
</section>
<section id="lets-start-thinking-causally" class="slide level2">
<h2>Let’s start thinking causally</h2>
<h3 id="everyday-conversations">Everyday Conversations …</h3>
<p><br></p>
<p><strong>Person 1:</strong> I was late to work today because my alarm didn’t ring.</p>
<p><br></p>
<p><strong>Person 2:</strong> Today, the 7:45 am bus did not come. Perhaps there was a driver shortage.</p>
<p><br></p>
<p><strong>Person 3:</strong> My uncle got diagnosed with lung cancer because he was a chain smoker.</p>
</section>
<section id="can-you-identify-the-cause-and-effect-variables" class="slide level2">
<h2>Can you identify the cause and effect variables?</h2>
<ol type="1">
<li>Functioning alarm ⇒ Work arrival</li>
<li>Driver shortage ⇒ Cancelled buses</li>
<li>Smoking ⇒ Lung cancer</li>
</ol>
<h3 id="so-our-research-questions-would-be">So, our Research Questions would be:</h3>
<ol type="1">
<li>Does having a functioning alarm affect work arrival time?</li>
<li>Does driver shortage impact bus disruptions?</li>
<li>Does smoking lead to lung cancer?</li>
</ol>
</section>
<section id="can-we-make-the-research-questions-causal" class="slide level2">
<h2>Can we make the Research Questions causal?</h2>
<ol type="1">
<li>To answer the questions, we need evidence.</li>
<li>Evidence comes from real-life data.</li>
<li>Data needs to be analysed using the correct methodology.</li>
<li><em>Correct methodology is developed when we know our end goal is for our research question to give a causal answer!</em></li>
</ol>
<h3 id="reframing-the-research-questions">Reframing the Research Questions:</h3>
<ol type="1">
<li>Does having a functioning alarm <span style="color: #BF505C;">causally</span> affect the work arrival time?</li>
<li>Does driver shortage <span style="color: #BF505C;">cause</span> bus cancellations?</li>
<li>Does smoking <span style="color: #BF505C;">cause</span> lung cancer?</li>
</ol>
</section>
<section id="conceptual-foundation-correlation-vs-causation" class="slide level2">
<h2>Conceptual Foundation: Correlation vs Causation</h2>
<p><br> <br></p>
<table class="caption-top">
<colgroup>
<col style="width: 18%">
<col style="width: 81%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Association</strong></td>
<td>The connection between two variables that is mostly based on our presumptions and general observations.</td>
</tr>
<tr class="even">
<td><strong>Correlation</strong></td>
<td>A statistical term that indicates the degree to which two variables move in coordination with one another.</td>
</tr>
<tr class="odd">
<td><strong>Causation</strong></td>
<td>It is a change in one variable that directly causes a change in another variable; it is a cause-and-effect relationship.</td>
</tr>
</tbody>
</table>
</section>
<section id="counterfactual-framework-rubins-causal-model" class="slide level2">
<h2>Counterfactual framework (Rubin’s Causal Model)</h2>
<p>For every individual <span class="math inline">\(i\)</span> there are two Potential Outcomes:</p>
<p><span class="math display">\[
Y_{1i} = \text{outcome if treated}
\]</span></p>
<p><span class="math display">\[
Y_{0i} = \text{outcome if not treated}
\]</span></p>
<p>where <span class="math inline">\(d\)</span> is the Treatment indicator (1 = <em>treated</em>, 0 = <em>control</em>).</p>
<p>But we only ever observe one of them:</p>
<p><span class="math display">\[
Y_i = d_i Y_{1i} + (1 - d_i) Y_{0i}
\]</span></p>
<p><strong>Fundamental Problem of Causal Inference:</strong></p>
<div style="padding: 10px; margin: 10px 0; border-left: 1px solid #1A6872; text-align: left; color: #1A6872;">
<p>We never observe both <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_0\)</span> for the same person at the same time.</p>
</div>
</section>
<section id="using-the-example-data" class="slide level2">
<h2>Using the Example Data</h2>
<div class="columns">
<div class="column" style="width:50%;">
<table class="caption-top">
<thead>
<tr class="header">
<th>Name</th>
<th><span class="math inline">\(d\)</span></th>
<th><span class="math inline">\(Y_0\)</span></th>
<th><span class="math inline">\(Y_1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Andy</td>
<td>1</td>
<td>.</td>
<td>10</td>
</tr>
<tr class="even">
<td>Ben</td>
<td>1</td>
<td>.</td>
<td>5</td>
</tr>
<tr class="odd">
<td>Chad</td>
<td>1</td>
<td>.</td>
<td>16</td>
</tr>
<tr class="even">
<td>Daniel</td>
<td>1</td>
<td>.</td>
<td>3</td>
</tr>
<tr class="odd">
<td>Edith</td>
<td>0</td>
<td>5</td>
<td>.</td>
</tr>
<tr class="even">
<td>Frank</td>
<td>0</td>
<td>7</td>
<td>.</td>
</tr>
<tr class="odd">
<td>George</td>
<td>0</td>
<td>8</td>
<td>.</td>
</tr>
<tr class="even">
<td>Hank</td>
<td>0</td>
<td>10</td>
<td>.</td>
</tr>
</tbody>
</table>
<p><br> <br></p>
<p>Source: Cunningham (2021), <em>Causal Inference: The Mixtape</em></p>
</div><div class="column" style="width:50%;">
<p><br> <br></p>
<ul>
<li>If <span class="math inline">\(d = 1\)</span>: we observe <span class="math inline">\(Y_1\)</span>; <span class="math inline">\(Y_0\)</span> is missing (counterfactual).</li>
<li>If <span class="math inline">\(d = 0\)</span>: we observe <span class="math inline">\(Y_0\)</span>; <span class="math inline">\(Y_1\)</span> is missing.</li>
</ul>
</div></div>
</section>
<section id="causal-effect-and-the-goal" class="slide level2">
<h2>Causal Effect and the Goal</h2>
<p>The individual treatment effect is:</p>
<p><span class="math display">\[
\tau_i = Y_{1i} - Y_{0i}
\]</span></p>
<p>But it is never observable for any single individual.</p>
<p><strong>Therefore:</strong> causal inference focuses on averages:</p>
<p><span class="math display">\[
ATE = E[Y_1 - Y_0]
\]</span></p>
<p>In data we estimate it using:</p>
<p><span class="math display">\[
ATE = E[Y | d = 1] - E[Y | d = 0]
\]</span></p>
<h3 id="key-idea">Key Idea:</h3>
<p>Causal inference compares the outcome we observe with an outcome we cannot observe, and <strong>statistical methods try to approximate the missing counterfactual</strong>.</p>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<p><br> <br></p>
<p><span class="mimic-h2">Outline</span></p>
<div style="font-weight: bold; font-size: 1.1em;">
<ul>
<li>Motivation and Conceptual Foundations</li>
<li><span style="color: #1A6872;">Identification</span></li>
<li>Randomised Controlled Trials (RCTs)</li>
<li>Instrumental Variables (IV) and Difference-in-Differences (DiD)</li>
<li>Introduction to Linear Regression</li>
<li>Regularization Techniques for Linear Regression in High-Dimensional Settings</li>
<li>Statistical Inference on Causal Effects in High Dimensional Linear Regression Models</li>
<li>Statistical Inference on Predictive and Causal Effects in Modern Nonlinear Regression Models</li>
</ul>
</div>
</section>
<section id="identification" class="slide level2">
<h2>Identification</h2>
<h3 id="goal">Goal:</h3>
<p>How to find the causal effect of treatment on the outcome?</p>
<ul>
<li><strong>What gives the causal effect?</strong> <em>Comparison with the Counterfactual</em></li>
<li><strong>Why can’t we see it directly?</strong> <em>Potential outcomes problem</em></li>
<li><strong>What design assumption makes it recoverable?</strong> <em>Identification</em></li>
</ul>
<div style="padding: 10px; margin: 10px 0; border-left: 1px solid #1A6872; text-align: left; color: #1A6872;">
<p>“The issue of identification stemmed from the quest to know the attainability of economically meaningful relationships from statistical analysis of economic data” Duo 1993.</p>
</div>
</section>
<section id="identification-issues" class="slide level2">
<h2>Identification Issues</h2>
<p>Can we just compare two individuals (one treated and one non-treated)? <strong>No!</strong></p>
<h3 id="potential-identification-issues">Potential Identification Issues</h3>
<ol type="1">
<li><p><strong>Confounding (Omitted Variable Bias):</strong> When variables affect both treatment and outcome, we can’t separate causation from correlation.</p></li>
<li><p><strong>Selection Bias:</strong> The sample or treatment groups are non-randomly selected.</p></li>
<li><p><strong>Reverse Causality:</strong> Outcome affects treatment rather than treatment affecting outcome.</p></li>
<li><p><strong>Measurement Error:</strong> When treatment, outcome, or confounders are measured incorrectly.</p></li>
<li><p><strong>Simultaneity:</strong> It occurs when treatment and outcome influence each other at the same time.</p></li>
</ol>
</section>
<section id="identification-issues-1" class="slide level2">
<h2>Identification Issues</h2>
<ol start="6" type="1">
<li><p><strong>Model Misspecification:</strong> Even if the causal structure is right, the statistical model can fail due to Wrong functional form, Omitted interactions and/or Nonlinearity ignored.</p></li>
<li><p><strong>Sorting and Endogeneity:</strong> Sorting happens when individuals self-select into treatment in a way that is related to their potential outcomes. We say treatment assignment is endogenous because it is not independent of potential outcomes.</p></li>
<li><p><strong>Unobservable Heterogeneity:</strong> When unit-specific unobserved characteristics drive both treatment and outcome.</p></li>
<li><p><strong>Violation of SUTVA (Stable Unit Treatment Value Assumption):</strong> SUTVA needs two assumptions-(A) <em>No Spillover</em>-Your result should not change because someone else got (or didn’t get) the treatment, and (B) <em>Consistency</em>- The treatment we define is exactly the treatment people actually received.</p></li>
</ol>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<p><br> <br></p>
<p><span class="mimic-h2">Outline</span></p>
<div style="font-weight: bold; font-size: 1.1em;">
<ul>
<li>Motivation and Conceptual Foundations</li>
<li>Identification</li>
<li><span style="color: #1A6872;">Randomised Controlled Trials (RCTs)</span></li>
<li>Instrumental Variables (IV) and Difference-in-Differences (DiD)</li>
<li>Introduction to Linear Regression</li>
<li>Regularization Techniques for Linear Regression in High-Dimensional Settings</li>
<li>Statistical Inference on Causal Effects in High Dimensional Linear Regression Models</li>
<li>Statistical Inference on Predictive and Causal Effects in Modern Nonlinear Regression Models</li>
</ul>
</div>
</section>
<section id="randomized-controlled-trials-rct-basic-setup" class="slide level2">
<h2>Randomized Controlled Trials (RCT): Basic Setup</h2>
<p><strong>Potential outcomes framework:</strong> <span class="math display">\[Y_i(1), Y_i(0)\]</span></p>
<p><strong>Treatment assignment:</strong> <span class="math display">\[
T_i = \begin{cases}
1 &amp; \text{if unit } i \text{ receives treatment} \\
0 &amp; \text{if unit } i \text{ is control}
\end{cases}
\]</span></p>
<p><strong>Observed outcome:</strong> <span class="math display">\[
Y_i = T_i Y_i(1) + (1 - T_i) Y_i(0)
\]</span></p>
<p><strong>RCT key feature: Random assignment</strong> <span class="math display">\[
T_i \perp (Y_i(1), Y_i(0))
\]</span></p>
<p><strong>Identification:</strong> <span class="math display">\[
ATE = E[Y_i(1) - Y_i(0)] = E[Y_i | T_i = 1] - E[Y_i | T_i = 0]
\]</span></p>
</section>
<section id="rct-linear-regression-model" class="slide level2">
<h2>RCT: Linear Regression Model</h2>
<p>In a randomized controlled trial, treatment <span class="math inline">\(T_i\)</span> is randomly assigned.</p>
<h3 id="linear-model-for-estimating-the-treatment-effect">Linear model for estimating the treatment effect:</h3>
<p><span class="math display">\[
Y_i = \alpha + \beta T_i + \gamma X_i + \varepsilon_i
\]</span></p>
<h3 id="key-feature-of-an-rct">Key feature of an RCT:</h3>
<p><span class="math display">\[
\text{Cov}(T_i, \varepsilon_i) = 0
\]</span></p>
<p>because randomization ensures <span class="math inline">\(T_i\)</span> is independent of unobserved factors.</p>
<h3 id="interpretation">Interpretation:</h3>
<ul>
<li><span class="math inline">\(\beta\)</span> measures the causal effect of treatment.</li>
<li><span class="math inline">\(X_i\)</span> are optional controls to improve precision (not identification).</li>
</ul>
<p><strong>Key idea:</strong> random assignment makes the treated and control groups comparable, so the coefficient on <span class="math inline">\(T_i\)</span> can be interpreted causally.</p>
</section>
<section id="rct-validity-issues" class="slide level2">
<h2>RCT Validity Issues</h2>
<h3 id="internal-validity">Internal validity:</h3>
<p>Can we estimate a causal effect for the experimental sample?</p>
<p><strong>Randomization ⇒ solves:</strong></p>
<ul>
<li>Confounding</li>
<li>Selection bias</li>
<li>Reverse causality</li>
</ul>
<h3 id="external-validity">External validity:</h3>
<p>Does the effect generalize?</p>
<ul>
<li>Population differences</li>
<li>Institutional, geographic, or behavioral effects</li>
<li>Experimental setting vs real world</li>
</ul>
</section>
<section id="when-rcts-are-not-perfect-non-compliance" class="slide level2">
<h2>When RCTs Are Not Perfect: Non-compliance</h2>
<p><strong>Non-compliance:</strong> people do not follow their assignment.</p>
<h3 id="examples">Examples:</h3>
<ul>
<li>Someone is assigned to the treatment but does not take it.</li>
<li>Someone in the control group somehow gets the treatment.</li>
</ul>
<h3 id="intention-to-treat-itt">Intention-to-Treat (ITT):</h3>
<ul>
<li>Compare groups based on assignment, not on what they actually did.</li>
<li>This keeps the randomisation intact.</li>
</ul>
</section>
<section id="when-rcts-are-not-perfect-attrition" class="slide level2">
<h2>When RCTs Are Not Perfect: Attrition</h2>
<p><strong>Attrition:</strong> Some outcomes are missing.</p>
<h3 id="examples-1">Examples:</h3>
<ul>
<li>People drop out of the study.</li>
<li>Some participants cannot be reached later.</li>
</ul>
<h3 id="why-this-is-a-problem">Why this is a problem:</h3>
<ul>
<li>The missing data may be related to the outcome.</li>
<li>This can break the original randomisation.</li>
</ul>
<h3 id="what-researchers-do">What researchers do:</h3>
<ul>
<li>Use the original assignment for estimation.</li>
<li>Check whether dropout looks random or systematic.</li>
</ul>
</section>
<section id="section-4" class="slide level2">
<h2></h2>
<p><br> <br></p>
<p><span class="mimic-h2">Outline</span></p>
<div style="font-weight: bold; font-size: 1.1em;">
<ul>
<li>Motivation and Conceptual Foundations</li>
<li>Identification</li>
<li>Randomised Controlled Trials (RCTs)</li>
<li><span style="color: #1A6872;">Instrumental Variables (IV) and Difference-in-Differences (DiD)</span></li>
<li>Introduction to Linear Regression</li>
<li>Regularization Techniques for Linear Regression in High-Dimensional Settings</li>
<li>Statistical Inference on Causal Effects in High Dimensional Linear Regression Models</li>
<li>Statistical Inference on Predictive and Causal Effects in Modern Nonlinear Regression Models</li>
</ul>
</div>
</section>
<section id="natural-experiments-idea-and-motivation" class="slide level2">
<h2>Natural Experiments: Idea and Motivation</h2>
<h3 id="key-idea-1">Key idea:</h3>
<ul>
<li>Use naturally occurring variation that mimics random assignment.</li>
<li>Exploit policy changes, institutional rules, shocks, geography.</li>
</ul>
<h3 id="goal-1">Goal:</h3>
<p>Identify causal effects when RCTs are infeasible.</p>
<h3 id="assumption-as-if-random-assignment">Assumption: As-if random assignment</h3>
<p><span class="math display">\[
T_i \perp (Y_i(1), Y_i(0)) \text{ within some subpopulation}
\]</span></p>
<h3 id="examples-2">Examples:</h3>
<ul>
<li>Education reforms</li>
<li>Weather or natural shocks</li>
<li>Eligibility rules (cutoffs, policy thresholds)</li>
</ul>
<p><strong>Natural experiments motivate econometric tools:</strong> IV, DiD, Matching, Regression discontinuity.</p>
</section>
<section id="instrumental-variables-iv" class="slide level2">
<h2>Instrumental Variables (IV)</h2>
<h3 id="when-we-use-iv">When we use IV:</h3>
<ul>
<li>Treatment is not randomly assigned.</li>
<li>There is selection, omitted variables, or reverse causality.</li>
<li>We have something that pushes people into treatment.</li>
</ul>
<h3 id="idea">Idea:</h3>
<p>Find a variable that affects treatment, but does not directly affect the outcome.</p>
<h3 id="example-intuition">Example intuition:</h3>
<p>A policy, a rule, or an eligibility threshold that changes who receives the treatment, but does not change outcomes except through that treatment.</p>
<h3 id="what-iv-does">What IV does:</h3>
<ul>
<li>Creates as-if random variation in treatment.</li>
<li>Lets us focus on the group whose behavior is changed by the instrument.</li>
<li>Recovers a causal effect even when we can’t control for everything.</li>
</ul>
<h3 id="key-message">Key message:</h3>
<p>IV solves endogeneity when we cannot observe or adjust for all confounders.</p>
</section>
<section id="iv-with-continuous-instrument-setup" class="slide level2">
<h2>IV with Continuous Instrument: Setup</h2>
<h3 id="structural-causal-model">Structural causal model:</h3>
<p><span class="math display">\[
Y_i = \alpha + \beta T_i + \gamma X_i + \varepsilon_i
\]</span></p>
<h3 id="problem">Problem:</h3>
<p><span class="math display">\[
\text{Cov}(T_i, \varepsilon_i) \neq 0
\]</span></p>
<h3 id="example-sources-of-endogeneity">Example sources of endogeneity:</h3>
<ul>
<li>omitted ability</li>
<li>selection into treatment</li>
<li>reverse causality</li>
</ul>
<p>We introduce an instrument <span class="math inline">\(Z_i\)</span> (continuous or multi-valued).</p>
</section>
<section id="iv-with-continuous-instrument-1st-and-2nd-stage" class="slide level2">
<h2>IV with Continuous Instrument: 1st and 2nd Stage</h2>
<h3 id="first-stage-predict-treatment">First stage (predict treatment):</h3>
<p><span class="math display">\[
T_i = \pi_0 + \pi_1 Z_i + \pi_2 X_i + u_i
\]</span></p>
<h3 id="requirement">Requirement:</h3>
<p><span class="math display">\[
\pi_1 \neq 0 \text{ (instrument relevance)}
\]</span></p>
<h3 id="reduced-form-effect-of-instrument-on-outcome">Reduced form (effect of instrument on outcome):</h3>
<p><span class="math display">\[
Y_i = \rho_0 + \rho_1 Z_i + \rho_2 X_i + v_i
\]</span></p>
<h3 id="second-stage-2sls">Second stage (2SLS):</h3>
<p><span class="math display">\[
Y_i = \alpha + \beta \hat{T}_i + \gamma X_i + \varepsilon_i
\]</span></p>
<p>where <span class="math inline">\(\hat{T}_i\)</span> is the fitted value from the first stage.</p>
</section>
<section id="iv-identification-for-continuous-z" class="slide level2">
<h2>IV Identification for Continuous Z</h2>
<h3 id="two-core-assumptions">Two core assumptions:</h3>
<ol type="1">
<li><p><strong>Relevance</strong> <span class="math display">\[
\text{Cov}(Z_i, T_i) \neq 0
\]</span></p></li>
<li><p><strong>Exogeneity</strong> <span class="math display">\[
Z_i \perp \varepsilon_i
\]</span></p></li>
</ol>
<p>(no direct effect of <span class="math inline">\(Z\)</span> on <span class="math inline">\(Y\)</span>)</p>
<h3 id="what-iv-estimates">What IV estimates:</h3>
<p><span class="math display">\[
\beta = \frac{\text{Cov}(Z_i, Y_i)}{\text{Cov}(Z_i, T_i)}
\]</span></p>
<p>(the continuous IV analog of Wald)</p>
<h3 id="interpretation-1">Interpretation:</h3>
<p>causal effect of <span class="math inline">\(T\)</span> on <span class="math inline">\(Y\)</span> for compliers.</p>
</section>
<section id="example-continuous-iv" class="slide level2">
<h2>Example: Continuous IV</h2>
<h3 id="question">Question:</h3>
<p>Does education increase wages?</p>
<h3 id="endogenous-regressor">Endogenous regressor:</h3>
<p><span class="math display">\[
T = \text{years of education}
\]</span></p>
<h3 id="continuous-instrument">Continuous instrument:</h3>
<p><span class="math display">\[
Z = \text{distance to nearest college}
\]</span></p>
<ul>
<li><strong>1st stage:</strong> college proximity predicts schooling</li>
<li><strong>2nd stage:</strong> predicted schooling explains wages</li>
</ul>
<h3 id="why-it-works">Why it works:</h3>
<p>proximity affects education but not wages directly.</p>
</section>
<section id="difference-in-differences-did" class="slide level2">
<h2>Difference-in-Differences (DiD)</h2>
<p><strong>When we use DiD</strong></p>
<ul>
<li>A policy or shock affects one group but not another.</li>
<li>We observe data before and after the change.</li>
</ul>
<p><strong>Idea</strong></p>
<p>Compare how outcomes evolve over time in a treated group versus a similar control group.</p>
<p><strong>Why it works</strong></p>
<p>If both groups were on similar trends before the intervention, the change in the treatment group relative to the control group reflects the causal effect.</p>
<p><strong>What DiD does well</strong> - Handles unobserved factors that do not change over time. - Captures the impact of large reforms, laws, or shocks. - Works with repeated or panel data.</p>
<p><strong>Key message</strong></p>
<p>DiD isolates the effect of a treatment by comparing changes over time across groups.</p>
</section>
<section id="did-linear-regression-setup" class="slide level2">
<h2>DiD: Linear Regression Setup</h2>
<p>Two groups (Treated/Control) and two periods (Before/After).</p>
<p><span class="math display">\[
Y_{it} = \alpha + \beta(T_{treat_i} \times P_{post_t}) + \lambda T_{treat_i} + \delta P_{post_t} + \varepsilon_{it}
\]</span></p>
<h3 id="did-estimator">DiD estimator:</h3>
<p>coefficient on the interaction term.</p>
<p><span class="math display">\[
\beta = \text{DiD}
\]</span></p>
<p>This captures the causal effect of treatment.</p>
</section>
<section id="did-identification-conditions" class="slide level2">
<h2>DiD: Identification Conditions</h2>
<h3 id="main-assumption">Main assumption:</h3>
<p><strong>Parallel trends:</strong> <span class="math inline">\((Y_{T, pre} - Y_{C, pre}) = (Y_{T, post} - Y_{C, post})\)</span> in absence of treatment</p>
<h3 id="controls-in-regression-allow">Controls in regression allow:</h3>
<p><span class="math display">\[
Y_{it} = \alpha + \beta(T_{treat_i} \times P_{post_t}) + \gamma X_{it} + \varepsilon_{it}
\]</span></p>
<h3 id="why-it-works-1">Why it works:</h3>
<ul>
<li>removes time-invariant unobservables</li>
<li>controls for common shocks</li>
</ul>
</section>
<section id="did-example" class="slide level2">
<h2>DiD Example</h2>
<h3 id="policy">Policy:</h3>
<p>Minimum wage increases only in Region A.</p>
<h3 id="model">Model:</h3>
<p><span class="math display">\[
Y_{it} = \alpha + \beta(\text{RegionA}_i \times \text{After}_t) + \varepsilon_{it}
\]</span></p>
<h3 id="suppose">Suppose:</h3>
<ul>
<li>Region A employment +5pp</li>
<li>Region B employment +1pp</li>
</ul>
<p><span class="math display">\[
\text{DiD} = 5 - 1 = 4\text{pp}
\]</span></p>
<h3 id="interpretation-2">Interpretation:</h3>
<p>The reform increased employment by 4pp.</p>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<p><br> <br></p>
<p><span class="mimic-h2">Outline</span></p>
<div style="font-weight: bold; font-size: 1.1em;">
<ul>
<li>Motivation and Conceptual Foundations</li>
<li>Identification</li>
<li>Randomised Controlled Trials (RCTs)</li>
<li>Instrumental Variables (IV) and Difference-in-Differences (DiD)</li>
<li><span style="color: #1A6872;">Introduction to Linear Regression</span></li>
<li>Regularization Techniques for Linear Regression in High-Dimensional Settings</li>
<li>Statistical Inference on Causal Effects in High Dimensional Linear Regression Models</li>
<li>Statistical Inference on Predictive and Causal Effects in Modern Nonlinear Regression Models</li>
</ul>
</div>
</section>
<section id="foundations-of-linear-regression" class="slide level2">
<h2>Foundations of Linear Regression</h2>
<ul>
<li>A linear regression model is given by: <span class="math display">\[
Y = \beta^{'} X + \epsilon
\]</span></li>
</ul>
<p>&nbsp;&nbsp;&nbsp; where <span class="math inline">\(\mathbb{E}[\epsilon] = 0\)</span>.</p>
<ul>
<li><p>We consider <span class="math inline">\(Y\)</span> as the outcome variable, <span class="math inline">\(X = (X_1, X_2, \ldots, X_p)'\)</span> as the vector of covariates, and <span class="math inline">\(\beta\)</span> as the vector of parameters.</p></li>
<li><p>Our goal is to construct the best linear predictor of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, which is the linear function of <span class="math inline">\(X\)</span> that minimizes the mean squared error (MSE):</p></li>
</ul>
<p><span class="math display">\[
\beta = \arg\min_{\beta \in \mathbb{R}^p} E[(Y - \beta^{'} X)^2]
\]</span></p>
</section>
<section id="goodness-of-fit" class="slide level2">
<h2>Goodness of Fit</h2>
<ul>
<li><p>The mean squared error (MSE) of the best linear predictor is given by: <span class="math display">\[
\text{MSE} = E[(Y - \beta^{'} X)^2]
\]</span></p></li>
<li><p>The <span class="math inline">\(R^2\)</span> of the best linear predictor is defined as: <span class="math display">\[
R^2 = \frac{E[(\hat{\beta}^{'} X)^2]}{E[Y^2]} = 1 - \frac{E_n \epsilon^2}{E_n Y^2} \in [0, 1]
\]</span></p></li>
</ul>
<p><strong>Interpretation:</strong></p>
<div style="padding: 10px; margin: 10px 0; border-left: 2px solid #1A6872; text-align: left; color: #1A6872;">
<ul>
<li><span class="math inline">\(R^2\)</span> measures the proportion of variance in <span class="math inline">\(Y\)</span> that is explained by the linear predictor <span class="math inline">\(\hat{\beta}^{'} X\)</span>.</li>
</ul>
</div>
</section>
<section id="overfitting-what-happens-when-pn-is-large" class="slide level2">
<h2>Overfitting: What happens when <span class="math inline">\(p/n\)</span> is large?</h2>
<div class="columns">
<div class="column fragment" style="width:60%;">
<div class="fragment">
<ul>
<li><p>When the number of predictors <span class="math inline">\(p\)</span> is large relative to the number of observations <span class="math inline">\(n\)</span>, models can become overly complex and fit the noise in the training data rather than the underlying signal.</p></li>
<li><p>Consider an example where <span class="math inline">\(p = n\)</span> and all <span class="math inline">\(X\)</span> variables are independent standard normal random variables. In this case, we have</p></li>
</ul>
<p><span class="math display">\[
\text{MSE}_{sample} = 0 \quad \text{and} \quad R^2_{sample} = 1
\]</span></p>
</div>
<div class="fragment">
<div style="padding: 10px; margin: 10px 0; border-left: 2px solid #1A6872; text-align: center; color: #1A6872;">
<p><strong>WHY?</strong></p>
</div>
</div>
<div class="fragment">
<ul>
<li>Here we have extreme <strong>overfitting</strong>: the model perfectly fits the training data but fails to generalize.</li>
</ul>
</div>
</div><div class="column fragment" style="width:40%;">
<div style="border-left: 1px solid #ccc; padding-left: 20px;">
<p><span class="cust_title">Overfitting Example</span></p>
</div>
<div style="border-left: 1px solid #ccc; padding-left: 20px;">
<div id="fed606e5" class="cell" data-fig-height="50%" data-fig-width="100%" data-execution_count="1">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="causal_inf_files/figure-revealjs/cell-2-output-1.png" class="quarto-figure quarto-figure-center" width="690" height="396"></p>
</figure>
</div>
</div>
</div>
<div id="4b03a40d" class="cell" data-fig-height="50%" data-fig-width="100%" data-execution_count="2">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="causal_inf_files/figure-revealjs/cell-3-output-1.png" class="quarto-figure quarto-figure-center" width="696" height="396"></p>
</figure>
</div>
</div>
</div>
</div>
</div></div>
</section>
<section id="inference-about-predictive-effects-or-association" class="slide level2">
<h2>Inference about Predictive Effects or Association</h2>
<ul>
<li><p>Predictive effects describe how our (population best linear) predictions change when a value of <span style="color: #B2582D;">target regressor</span> changes, holding all other regressors constant.</p></li>
<li><p>Specifically, we partition the vector of regressors <span class="math inline">\(X\)</span> into two parts: the target regressor of interest <span class="math inline">\(D\)</span> and the remaining regressors <span class="math inline">\(W\)</span> (also called control variables or covariates).</p></li>
</ul>
<p><span class="math display">\[
X = (D, W'),
\]</span></p>
<ul>
<li>We can then write the best linear predictor of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> as</li>
</ul>
<p><span class="math display">\[
Y =  \beta_1 D + \beta^{'}_2 W + \epsilon
\]</span></p>
<div style="padding: 10px; margin: 10px 0; border-left: 2px solid #1A6872; text-align: left; color: #1A6872;">
<p><span style="color: #1A6872;">How does the predicted value of change if increases by a unit while remains unchanged?</span></p>
</div>
</section>
<section id="section-6" class="slide level2">
<h2></h2>
<ul>
<li>The predictive value of <span class="math inline">\(Y\)</span> changes by <span class="math inline">\(\beta_1\)</span> units when <span class="math inline">\(D\)</span> increases by one unit.</li>
</ul>
<p><br></p>
<ul>
<li>Note that this interpretation is purely predictive and does not imply causality.</li>
</ul>
<p><br></p>
<div style="padding: 10px; margin: 10px 0; border-left: 2px solid #1A6872; text-align: center; color: #1A6872;">
<p><strong>WHY?</strong></p>
</div>
<p><br></p>
<ul>
<li>This is because we are holding all other regressors <span class="math inline">\(W\)</span> constant, which may not be realistic in practice.</li>
</ul>
<p><br></p>
<ul>
<li>For example: In a wage regression, <span class="math inline">\(D\)</span> could be years of education, and <span class="math inline">\(W\)</span> could include experience, gender, and location. The coefficient on education (<span class="math inline">\(\beta_1\)</span>) tells us how much more (or less) we predict someone will earn for each additional year of education, with the same experience, gender, and location.</li>
</ul>
</section>
<section id="understanding-beta_1-via-partialling-out" class="slide level2">
<h2>Understanding <span class="math inline">\(\beta_1\)</span> via <span style="color: #1A6872;">“Partialling-Out”</span></h2>
<ul>
<li><p>“Partialling-out” is the process of isolating the effect of the target regressor <span class="math inline">\(D\)</span> on the outcome variable <span class="math inline">\(Y\)</span> by removing the influence of the control variables <span class="math inline">\(W\)</span>.</p></li>
<li><p>Partialling-out operation is define as a procedure with three steps:</p>
<ol type="1">
<li>Regress <span class="math inline">\(Y\)</span> on <span class="math inline">\(W\)</span> and obtain the residuals <span class="math inline">\(\tilde{Y}\)</span>.</li>
</ol>
<p><span class="math display">\[
Y = \gamma^{'}_{YW} W + u \quad \Rightarrow \quad \tilde{Y} = Y - \hat{Y} = Y - \gamma^{'}_{YW} W
\]</span></p>
<ol start="2" type="1">
<li>Regress <span class="math inline">\(D\)</span> on <span class="math inline">\(W\)</span> and obtain the residuals <span class="math inline">\(\tilde{D}\)</span>.</li>
</ol>
<p><span class="math display">\[
D = \gamma^{'}_{DW} W + v \quad \Rightarrow \quad \tilde{D} = D - \hat{D} = D - \gamma^{'}_{DW} W
\]</span></p>
<ol start="3" type="1">
<li>Regress the residuals <span class="math inline">\(\tilde{Y}\)</span> on the residuals <span class="math inline">\(\tilde{D}\)</span> to estimate <span class="math inline">\(\beta_1\)</span>.</li>
</ol>
<p><span class="math display">\[
\tilde{Y} = \beta_1 \tilde{D} + \tilde{\epsilon}
\]</span></p></li>
</ul>
</section>
<section id="section-7" class="slide level2">
<h2></h2>
<p>We can also show partialling-out procedure by partialling-out operation to both sides of our regression equation</p>
<p><span class="math display">\[
Y =  \beta_1 D + \beta^{'}_2 W + \epsilon
\]</span></p>
<p>to get</p>
<p><span class="math display">\[
\tilde{Y} = \beta_1 \tilde{D} + \beta^{'}_2 \tilde{W} + \tilde{\epsilon}
\]</span></p>
<p>Which simplifies to</p>
<p><span class="math display">\[
\tilde{Y} = \beta_1 \tilde{D} + \epsilon
\]</span></p>
<div style="padding: 10px; margin: 10px 0; border-left: 1px solid #ccc; text-align: left;">
<p><span style="color: #1A6872;"><strong>Why</strong> does <span class="math inline">\(\tilde{W}\)</span> disappear in the partialled-out regression?</span><br>
<span style="color: #1A6872;"><strong>Why</strong> <span class="math inline">\(\tilde{\epsilon} = \epsilon\)</span>?</span></p>
</div>
<p><strong>Interpretation of <span class="math inline">\(\beta_1\)</span> in Partialling-Out</strong></p>
<ul>
<li><p><span class="math inline">\(\beta_1\)</span> can be interpreted as the effect of <span class="math inline">\(D\)</span> on <span class="math inline">\(Y\)</span> after removing the influence of <span class="math inline">\(W\)</span> as univariate linear regression of <em>residualized</em> <span class="math inline">\(Y\)</span> on <em>residualized</em> <span class="math inline">\(D\)</span>.</p></li>
<li><p>Residuals are defined by partialling-out the linear effects of <span class="math inline">\(W\)</span> from both <span class="math inline">\(Y\)</span> and <span class="math inline">\(D\)</span>.</p></li>
</ul>
</section>
<section id="section-8" class="slide level2">
<h2></h2>
<div style="padding: 10px; margin: 10px 0; text-align: left; font-size: 1.4em; font-weight: bold;">
<p><br> <br></p>
<div class="fragment">
<p><span style="color: #BF505C;">When <span class="math inline">\(p/n\)</span> is large, using linear regression for partialling-out can lead to overfitting issues, resulting in biased estimates of <span class="math inline">\(\beta_1\)</span>.</span></p>
</div>
<p><br></p>
<div class="fragment">
<p><span style="color: #707C36;">To address this, we can use dimension reduction or regularization techniques, such as Lasso or Ridge regression, during the partialling-out steps.</span></p>
</div>
</div>
</section>
<section id="section-9" class="slide level2">
<h2></h2>
<p><br> <br></p>
<p><span class="mimic-h2">Outline</span></p>
<div style="font-weight: bold; font-size: 1.1em;">
<ul>
<li>Motivation and Conceptual Foundations</li>
<li>Identification</li>
<li>Randomised Controlled Trials (RCTs)</li>
<li>Instrumental Variables (IV) and Difference-in-Differences (DiD)</li>
<li>Introduction to Linear Regression</li>
<li><span style="color: #1A6872;">Regularization Techniques for Linear Regression in High-Dimensional Settings</span></li>
<li>Statistical Inference on Causal Effects in High Dimensional Linear Regression Models</li>
<li>Statistical Inference on Predictive and Causal Effects in Modern Nonlinear Regression Models</li>
</ul>
</div>
</section>
<section id="linear-regression-with-high-dimensional-covariates" class="slide level2">
<h2>Linear Regression with High-Dimensional <br> Covariates</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>A regression model</li>
</ul>
<p><span class="math display">\[
Y = \beta^{'} X + \epsilon, \quad \epsilon \perp X,
\]</span></p>
<div style="margin-left: 1.5em;">
<p>where <span class="math inline">\(\beta^{'} X\)</span> is the population best linear predictor of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>.</p>
</div>
<ul>
<li>The vector <span class="math inline">\(X = (X_1, X_2, \ldots, X_p)\)</span> represents the <span class="math inline">\(p\)</span> covariates (<span class="math inline">\(p\)</span> regressors).</li>
</ul>
<div style="padding: 10px; margin: 10px 0; text-align: center; font-size: 1.2em; font-weight: bold;">
<p><span style="color: #1A6872;"><span class="math inline">\(p\)</span> is large, possibly larger than <span class="math inline">\(n\)</span></span></p>
</div>
<ul>
<li>This case where is very large is what we call a <span style="color: #1A6872;">high-dimensional</span> setting.</li>
</ul>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images\high_dimens.png" class="quarto-figure quarto-figure-center" style="margin-top: -291px; margin-right: -251px;" width="1920" height="1250"></p>
</figure>
</div>
</div></div>
</section>
<section id="constructed-regressors" class="slide level2">
<h2>Constructed Regressors</h2>
<ul>
<li><p>In high-dimensional settings, we often create new features from the original covariates to capture complex relationships.</p></li>
<li><p>if <span class="math inline">\(W\)</span> are raw covariates, we can create <em>technical (constructed) regressors</em> <span class="math inline">\(X\)</span> by including polynomial terms, interaction terms, or other transformations of <span class="math inline">\(W\)</span>.</p></li>
</ul>
<p><span class="math display">\[
X = P(W) = (P_1(W), P_2(W), \ldots, P_p(W))'
\]</span></p>
<p><span style="margin-left: 50px;">where the set of transformations <span class="math inline">\(P(W)\)</span> can be very large, leading to a high-dimensional feature space.</span></p>
<ul>
<li>Example transformations include squared terms (<span class="math inline">\(W_i^2\)</span>), interaction terms (<span class="math inline">\(W_i W_j\)</span>), and higher-order polynomials (<span class="math inline">\(W_i^3\)</span>, etc.).</li>
</ul>
</section>
<section id="section-10" class="slide level2">
<h2></h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><br></p>
<p><span class="mimic-h2">Why Do We Need Constructed Regressors?</span></p>
<ul>
<li><p>The main motivation for using constructed regressors is to built more flexible models that can capture non-linear relationships and interactions among the original covariates.</p></li>
<li><p>By expanding the feature space in prediction rules, <span class="math inline">\(\beta^{'} X = \beta^{'} P(W)\)</span>, we can approximate complex functions of the original covariates <span class="math inline">\(W\)</span> more accurately.</p></li>
</ul>
<p><br></p>
<div style="padding: 10px; margin: 10px 0; border-left: 2px solid #1A6872; text-align: left; color: #1A6872; font-weight: bold;">
<p><span class="math inline">\(\beta^{'} P(W)\)</span> are nonlinear in <span class="math inline">\(W\)</span> but still linear in parameters <span class="math inline">\(\beta\)</span>.</p>
</div>
</div><div class="column" style="width:50%;">
<div style="border-left: 1px solid #ccc; padding-left: 20px;">
<div id="75357d68" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="causal_inf_files/figure-revealjs/cell-4-output-1.png" width="975" height="544"></p>
</figure>
</div>
</div>
</div>
<div id="cb2a5202" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="causal_inf_files/figure-revealjs/cell-5-output-1.png" width="975" height="543"></p>
</figure>
</div>
</div>
</div>
</div>
</div></div>
</section>
<section id="best-predictor" class="slide level2">
<h2>Best Predictor</h2>
<ul>
<li>In the population, the best predictor of <span class="math inline">\(Y\)</span> given <span class="math inline">\(W\)</span> is</li>
</ul>
<p><span class="math display">\[
g(W) = E[Y|W]
\]</span></p>
<p><span style="margin-left: 50px;">the conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(W\)</span>. The function <span class="math inline">\(g(W)\)</span> is called the regression function of <span class="math inline">\(Y\)</span> on <span class="math inline">\(W\)</span>.</span></p>
<ul>
<li>The conditional expectation funnction <span class="math inline">\(g(W)\)</span> solves the best prediction problem</li>
</ul>
<p><span class="math display">\[
\min_{m(W)} E[(Y - m(W))^2].
\]</span></p>
<ul>
<li><p>Here we minimize the mean squared prediction error over all prediction rules <span class="math inline">\(m(W)\)</span>.</p></li>
<li><p><span class="math inline">\(\beta^{'} P(W)\)</span> is an approximation to best predictor <span class="math inline">\(g(W)\)</span></p></li>
<li><p>Using richer and more complex constructed regressors <span class="math inline">\(P(W)\)</span> allows us to better approximate the true regression function <span class="math inline">\(g(W)\)</span>.</p></li>
</ul>
</section>
<section id="motivation-for-regularization" class="slide level2">
<h2>Motivation for Regularization</h2>
<p><br></p>
<ul>
<li>Classical linear regression can perform poorly in high-dimensional settings due to <span style="color: #BF505C;">overfitting</span>.</li>
</ul>
<p><br></p>
<ul>
<li>This is especially apparent when <span class="math inline">\(p \geq n\)</span>.</li>
</ul>
<p><br></p>
<ul>
<li>Regularization techniques, such as Lasso and Ridge regression, help mitigate overfitting by adding a penalty term to the loss function.</li>
</ul>
</section>
<section id="lasso-regression" class="slide level2">
<h2>Lasso Regression</h2>
<ul>
<li><p>Lasso (Least Absolute Shrinkage and Selection Operator) regression adds a penalty to the loss function</p></li>
<li><p>Lasso constructs the estimator <span class="math inline">\(\hat{\beta}\)</span> by solving the following penalized least squares problem:</p></li>
</ul>
<p><span class="math display">\[
\min_{b \in \mathbb{R}^p} \left\{ \frac{1}{n} \sum_{i=1}^{n} (Y_i - b^{'} X_i)^2 + \lambda \sum_{j=1}^{p} |b_j| \right\}
\]</span></p>
<ul>
<li><p>The first term is the usual mean squared error, while the second term is called a <span style="color: #BF505C;">penalty term</span>.</p></li>
<li><p>The tuning parameter <span class="math inline">\(\lambda \geq 0\)</span> controls the strength of the penalty.</p></li>
<li><p>Lasso performs both <span style="color: #1A6872;">variable selection and regularization</span>, shrinking some coefficients to exactly zero, effectively selecting a simpler model.</p></li>
<li><p>As long as <span class="math inline">\(\lambda &gt; 0\)</span>, the introduction of the penalty term leads to a prediction rule that is less complex and less prone to overfitting compared to ordinary least squares.</p></li>
</ul>
</section>
<section id="how-to-choose-the-tuning-parameter-lambda" class="slide level2">
<h2>How to Choose the Tuning Parameter <span class="math inline">\(\lambda\)</span>?</h2>
<ul>
<li><p>The tuning parameter <span class="math inline">\(\lambda\)</span> in Lasso regression controls the trade-off between fitting the training data well and keeping the model simple.</p>
<ul>
<li><p>A larger <span class="math inline">\(\lambda\)</span> increases the penalty for large coefficients, leading to a sparser model with more coefficients set to zero.</p></li>
<li><p>A smaller <span class="math inline">\(\lambda\)</span> allows the model to fit the training data more closely, potentially leading to overfitting if <span class="math inline">\(p\)</span> is large relative to <span class="math inline">\(n\)</span>.</p></li>
</ul></li>
<li><p>Common methods for selecting <span class="math inline">\(\lambda\)</span> include:</p>
<ul>
<li>A theoretically valid choice is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\lambda = 2 c \hat{\sigma} \sqrt{n} \Phi^{-1}(1 - \alpha / (2p))
\]</span></p>
<div style="margin-left: 60px;">
<p>where <span class="math inline">\(\hat{\sigma}\)</span> is an estimate of the standard deviation of the error term, <span class="math inline">\(\Phi^{-1}\)</span> is the inverse CDF of the standard normal distribution, and <span class="math inline">\(c &gt; 1\)</span> is a constant, and <span class="math inline">\(\alpha\)</span> is a small significance level (e.g., 0.05).</p>
</div>
<ul>
<li>Cross-validation: Split the data into training and validation sets multiple times, fit the model for different values of <span class="math inline">\(\lambda\)</span>, and choose the one that minimizes the average validation error.</li>
</ul>
</section>
<section id="section-11" class="slide level2">
<h2></h2>
<p><br> <br></p>
<div class="fragment" style="padding: 10px; margin: 10px 0; text-align: left; font-size: 1.4em; border-left: 4px solid #BF505C; font-weight: bold;">
<p><span style="color: #BF505C;">Lasso shrinks relevant regressors towards zero and <em>“underestimates”</em> the absolute value of the coefficients.</span></p>
</div>
<p><br></p>
<div class="fragment" style="padding: 10px; margin: 10px 0; text-align: left; font-size: 1.4em; border-left: 4px solid #BF505C; font-weight: bold;">
<p><span style="color: #BF505C;">Therefore, Lasso may not be ideal for inference about predictive effects or causal effects.</span></p>
</div>
</section>
<section id="example-bias-in-lasso-coefficients" class="slide level2">
<h2>Example: Bias in Lasso Coefficients</h2>
<div id="52b318da" class="cell" data-fig-height="100%" data-fig-width="100%" data-execution_count="5">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="causal_inf_files/figure-revealjs/cell-6-output-1.png" class="quarto-figure quarto-figure-center" width="1573" height="813"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="post-lasso-estimation" class="slide level2">
<h2>Post-Lasso Estimation</h2>
<ul>
<li><p>To mitigate the bias introduced by Lasso, we can use a two-step procedure called Post-Lasso estimation.</p></li>
<li><p>The Post-Lasso estimator is obtained by:</p>
<ol type="1">
<li><p>First, use Lasso regression to select a subset of relevant regressors (those with non-zero coefficients).</p></li>
<li><p>Then, fit an ordinary least squares regression using only the selected regressors from the first step.</p></li>
</ol>
<div style="padding: 10px; margin: 10px 0; text-align: left;">
<p><span style="color: #1A6872;"><strong>Does Post-Lasso, <span class="math inline">\(\hat{\beta^{'}} X\)</span>, provide a good approximation to best linear prediction rule, <span class="math inline">\(\beta^{'} X\)</span> ?</strong></span></p>
</div></li>
<li><p>We will have <span class="math inline">\(s\)</span> selected regressors after Lasso, also called <em>effective dimension</em></p></li>
<li><p>To estimate the Post-Lasso estimator, we need <span class="math inline">\(n/s\)</span> to be sufficiently large to avoid overfitting in the second step.</p></li>
</ul>
</section>
<section id="ridge-regression" class="slide level2">
<h2>Ridge Regression</h2>
<ul>
<li>Ridge regression is another regularization technique that adds a penalty to the loss function based on the squared magnitude of the coefficients.</li>
<li>Ridge regression constructs the estimator <span class="math inline">\(\hat{\beta}\)</span> by solving the following penalized least squares problem:</li>
</ul>
<p><span class="math display">\[
\min_{b \in \mathbb{R}^p} \left\{ \frac{1}{n} \sum_{i=1}^{n} (Y_i - b^{'} X_i)^2 + \lambda \sum_{j=1}^{p} b_j^2 \right\}
\]</span></p>
<ul>
<li>In contrast to Lasso, Ridge regression does not perform variable selection; instead, it shrinks all coefficients towards zero but none are set exactly to zero.</li>
</ul>
</section>
<section id="elastic-net-regression" class="slide level2">
<h2>Elastic Net Regression</h2>
<ul>
<li>Elastic Net regression combines the penalties of both Lasso and Ridge regression.</li>
</ul>
<p><span class="math display">\[
\min_{b \in \mathbb{R}^p} \left\{ \frac{1}{n} \sum_{i=1}^{n} (Y_i - b^{'} X_i)^2 + \lambda_1 \sum_{j=1}^{p} |b_j| + \lambda_2 \sum_{j=1}^{p} b_j^2 \right\}
\]</span></p>
<ul>
<li><p><span class="math inline">\(\lambda_1\)</span> controls the Lasso penalty, while <span class="math inline">\(\lambda_2\)</span> controls the Ridge penalty.</p></li>
<li><p>Two tuning parameters could be selected via cross-validation or other hyperparameter optimization methods in machine learning, such as grid search or Bayesian optimization.</p></li>
</ul>
</section>
<section id="section-12" class="slide level2">
<h2></h2>
<p><br> <br> <br></p>
<p><span class="mimic-h2">Choice of Regression Methods in Practice</span></p>
<ul>
<li><p>The choice between Lasso, Ridge, and Elastic Net regression depends on the specific characteristics of the data and the goals of the analysis.</p></li>
<li><p>If we are interested in building the best prediction, we can tune each method via cross-validation and select the one with the lowest prediction error on test data.</p></li>
</ul>
</section>
<section id="section-13" class="slide level2">
<h2></h2>
<p><br> <br></p>
<p><span class="mimic-h2">Outline</span></p>
<div style="font-weight: bold; font-size: 1.1em;">
<ul>
<li>Motivation and Conceptual Foundations</li>
<li>Identification</li>
<li>Randomised Controlled Trials (RCTs)</li>
<li>Instrumental Variables (IV) and Difference-in-Differences (DiD)</li>
<li>Introduction to Linear Regression</li>
<li>Regularization Techniques for Linear Regression in High-Dimensional Settings</li>
<li><span style="color: #1A6872;">Statistical Inference on Causal Effects in High Dimensional Linear Regression Models</span></li>
<li>Statistical Inference on Predictive and Causal Effects in Modern Nonlinear Regression Models</li>
</ul>
</div>
</section>
<section id="causal-interpretation-of-predictive-effects" class="slide level2">
<h2>Causal interpretation of predictive effects</h2>
<ul>
<li>Remember our regression model with target regressor <span class="math inline">\(D\)</span> and control variables <span class="math inline">\(W\)</span>:</li>
</ul>
<p><span class="math display">\[
Y =  \beta_1 D + \beta^{'}_2 W + \epsilon
\]</span></p>
<ul>
<li><p>If conditioning on <span class="math inline">\(W\)</span> is sufficient to control for confounding between <span class="math inline">\(D\)</span> and <span class="math inline">\(Y\)</span>, then <span class="math inline">\(\beta_1\)</span> can be interpreted as the average causal effect of <span class="math inline">\(D\)</span> on <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Then, predictive effect of <span class="math inline">\(D\)</span> on <span class="math inline">\(Y\)</span> can answer the causal question:</p></li>
</ul>
<div style="padding: 10px; margin: 10px 0; border-left: 2px solid #1A6872; text-align: left;">
<p><span style="color: #1A6872;">What is the average change in <span class="math inline">\(Y\)</span> when we intervene to increase <span class="math inline">\(D\)</span> by one unit, holding <span class="math inline">\(W\)</span> constant?</span></p>
</div>
</section>
<section id="double-lasso" class="slide level2 smaller">
<h2>Double Lasso</h2>
<p>The key step is application of Lasso regression for partialling-out in the presence of high-dimensional covariates. Consider the following regression model:</p>
<p><span class="math display">\[
Y = \alpha D + \beta^{'} W + \epsilon,
\]</span></p>
<p>where <span class="math inline">\(D\)</span> is the target regressor of interest and <span class="math inline">\(W\)</span> is a vector of <span class="math inline">\(p\)</span> control variables. After partialling-out <span class="math inline">\(W\)</span> from both <span class="math inline">\(Y\)</span> and <span class="math inline">\(D\)</span>, we get</p>
<ol type="1">
<li>We run Lasso regressions of <span class="math inline">\(Y\)</span> on <span class="math inline">\(W\)</span> and <span class="math inline">\(D\)</span> on <span class="math inline">\(W\)</span>:</li>
</ol>
<p><span class="math display">\[
\hat{\gamma}_{YW} = \arg\min_{\gamma \in \mathbb{R}^p} \left\{ \frac{1}{n} \sum_{i=1}^{n} (Y_i - \gamma^{'} W_i)^2 + \lambda \sum_{j=1}^{p} |\gamma_j| \right\},
\]</span></p>
<p><span class="math display">\[
\hat{\gamma}_{DW} = \arg\min_{\gamma \in \mathbb{R}^p} \left\{ \frac{1}{n} \sum_{i=1}^{n} (D_i - \gamma^{'} W_i)^2 + \lambda \sum_{j=1}^{p} |\gamma_j| \right\}.
\]</span></p>
<ol start="2" type="1">
<li>Compute the residuals <span class="math inline">\(\tilde{Y}\)</span> and <span class="math inline">\(\tilde{D}\)</span>:</li>
</ol>
<p><span class="math display">\[
\tilde{Y} = Y - \hat{\gamma}_{YW}^{'} W, \quad \tilde{D} = D - \hat{\gamma}_{DW}^{'} W.
\]</span></p>
<ol start="3" type="1">
<li>Finally, we run an ordinary least squares regression of <span class="math inline">\(\tilde{Y}\)</span> on <span class="math inline">\(\tilde{D}\)</span> to estimate <span class="math inline">\(\alpha\)</span>:</li>
</ol>
<p><span class="math display">\[
\begin{align}
\hat{\alpha} &amp;= \arg\min_{\alpha \in \mathbb{R}} \frac{1}{n} \sum_{i=1}^{n} ({\tilde{Y}}_i - \alpha {\tilde{D}}_i)^2  \\
&amp;= (E_n \tilde{D}^2)^{-1} E_n\tilde{D} \tilde{Y}.
\end{align}
\]</span></p>
</section>
<section id="uncertainty-in-hatalpha" class="slide level2">
<h2>Uncertainty in <span class="math inline">\(\hat{\alpha}\)</span></h2>
<ul>
<li>To quantify the uncertainty of the Double Lasso estimator <span class="math inline">\(\hat{\alpha}\)</span>, we can use the following formula for the standard deviation of <span class="math inline">\(\hat{\alpha}\)</span>:</li>
</ul>
<p><span class="math display">\[
V = (E_n \tilde{D}^2)^{-1} E_n (\tilde{D}^2 \hat{\epsilon}^2) (E_n \tilde{D}^2)^{-1},
\]</span></p>
<ul>
<li>The standard error of <span class="math inline">\(\hat{\alpha}\)</span> is then:</li>
</ul>
<p><span class="math display">\[
\text{SE}(\hat{\alpha}) = \sqrt{V/n}.
\]</span></p>
<ul>
<li>This allows us to construct confidence intervals and perform hypothesis tests for the estimated causal effect <span class="math inline">\(\hat{\alpha}\)</span>. For example, a 95% confidence interval for <span class="math inline">\(\alpha\)</span> can be constructed as:</li>
</ul>
<p><span class="math display">\[
\left[\hat{\alpha} \pm 2 \times \sqrt{V/n} \right].
\]</span></p>
</section>
<section id="section-14" class="slide level2">
<h2></h2>
<p><br> <br> <br></p>
<div style="padding: 10px; margin: 10px 0; border-left: 2px solid #2CA0AB; text-align: left; color: #1A6872; font-weight: bold; font-size: 1.4em;">
<p>Practical Example: A comparison of OLS and Double Lasso</p>
<p><em>For the relevant code and data, go to <a href="https://mustafaslancoto.github.io/talks/">here</a></em></p>
</div>
</section>
<section id="inference-on-many-coefficients" class="slide level2">
<h2>Inference on Many Coefficients</h2>
<ul>
<li><p>If we are interested in more than one coefficient, we can repeat the Double Lasso procedure for each target regressor of interest.</p></li>
<li><p>Here we consider the model: <span class="math display">\[
Y = \sum_{j=1}^{p_1} \alpha_j D_j + \sum_{k=1}^{p_2} \beta_k W_k + \epsilon,
\]</span></p></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp; where the number of target regressors <span class="math inline">\(p_1\)</span> can also be large, and the number of control variables <span class="math inline">\(p_2\)</span> can be large as well.</p>
</section>
<section id="motivation-for-many-coefficients-inference" class="slide level2">
<h2>Motivation for Many Coefficients Inference</h2>
<ul>
<li><p>There can be multiple policy variables of interest that we want to analyze simultaneously, such as the effects of different education policies on student outcomes.</p></li>
<li><p>We can be interested in heterogeneous treatment effects across different subgroups, which requires estimating multiple coefficients for each subgroup.</p></li>
<li><p>We can be interested in nonlinear effects of policies</p></li>
</ul>
</section>
<section id="one-by-one-double-lasso" class="slide level2">
<h2>One by One Double Lasso</h2>
<ul>
<li>For each <span class="math inline">\(j = 1, \ldots, p_1\)</span>, we can apply the one-by-one Double Lasso procedure to estimate <span class="math inline">\(\alpha_j\)</span> while treating the other <span class="math inline">\(D_{-j}\)</span> as part of the control variables.</li>
</ul>
<p><span class="math display">\[
Y = \alpha_j D_j + \gamma_j{'} W_j + \epsilon, \quad W_j = ((D_{-j})^{'}, W^{'})^{'}
\]</span></p>
<ul>
<li><p>The double lasso provides a high quality estimate of <span class="math inline">\(\hat{\alpha} = (\alpha_j)_{j=1}^{p_1}\)</span> for each <span class="math inline">\(j\)</span> and we can construct confidence intervals for each <span class="math inline">\(\alpha_j\)</span>.</p></li>
<li><p>This allows us to make inference on multiple coefficients simultaneously, even in high-dimensional settings where <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> are large relative to <span class="math inline">\(n\)</span>.</p></li>
</ul>
</section>
<section id="section-15" class="slide level2">
<h2></h2>
<p><br> <br></p>
<p><span class="mimic-h2">Outline</span></p>
<div style="font-weight: bold; font-size: 1.1em;">
<ul>
<li>Motivation and Conceptual Foundations</li>
<li>Identification</li>
<li>Randomised Controlled Trials (RCTs)</li>
<li>Instrumental Variables (IV) and Difference-in-Differences (DiD)</li>
<li>Introduction to Linear Regression</li>
<li>Regularization Techniques for Linear Regression in High-Dimensional Settings</li>
<li>Statistical Inference on Causal Effects in High Dimensional Linear Regression Models</li>
<li>Statistical Inference on Predictive and Causal Effects in Modern Nonlinear Regression Models]{style=“color: #1A6872;”}</li>
</ul>
</div>
</section>
<section id="introduction" class="slide level2">
<h2>Introduction</h2>
<ul>
<li>We recall our regression model with target regressor <span class="math inline">\(D\)</span> and control variables <span class="math inline">\(W\)</span>, where we ask predictive effect question:</li>
</ul>
<p>How does the predicted value of outcome <span class="math inline">\(Y\)</span>,</p>
<p><span class="math display">\[
E[Y|D, W],
\]</span></p>
<p>change if a regressor value <span class="math inline">\(D\)</span> increases by one init, while regressor values, <span class="math inline">\(W\)</span>, remain unchanged</p>
</section>
<section id="doubledebiased-machine-learning-dml-inference-in-the-partially-linear-regression-model-plm" class="slide level2">
<h2>Double/debiased machine learning (DML) Inference in the Partially Linear Regression Model (PLM)</h2>
<ul>
<li>A partially linear regression model:</li>
</ul>
<p><span id="eq-gx"><span class="math display">\[
Y = \alpha D + {\color{#707C36}{g(W)}} + \epsilon, \quad E[\epsilon|D,W] = 0,
\tag{1}\]</span></span></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp; where <span class="math inline">\(Y\)</span> is the outcome variable, <span class="math inline">\(D\)</span> is the treatment variable, <span class="math inline">\(W\)</span> is a vector of control variables.</p>
<ul>
<li><p>The model allows a part of the regression function, <span class="math inline">\(\color{#707C36}{g(W)}\)</span>, to be fully nonlinear</p></li>
<li><p>However, the model is not fully general, because it imposes additivity in <span class="math inline">\(\color{#707C36}{g(W)}\)</span> and <span class="math inline">\(D\)</span></p></li>
</ul>
</section>
<section id="partialling-out" class="slide level2">
<h2>Partialling-Out</h2>
<p>Applying partialling-out to <a href="#/eq-gx" class="quarto-xref">Equation&nbsp;1</a>, we obtain:</p>
<p><span class="math display">\[
\tilde{Y} = \alpha \tilde{D} + \epsilon,
\]</span></p>
<p>where <span class="math inline">\(\tilde{Y}\)</span> and <span class="math inline">\(\tilde{D}\)</span> are the residuals left after predicting <span class="math inline">\(Y\)</span> and <span class="math inline">\(D\)</span> using <span class="math inline">\(W\)</span>.</p>
<p><span class="math display">\[
\tilde{Y} := Y - \ell(W) \quad \text{and} \quad \tilde{D} := D - m(W),
\]</span> = E[Y|W]</p>
<p>where <span class="math inline">\(\ell(W)\)</span> and <span class="math inline">\(m(W)\)</span> are conditional expectation functions of <span class="math inline">\(Y\)</span> and <span class="math inline">\(D\)</span> given <span class="math inline">\(W\)</span>, respectively.</p>
<p><span class="math display">\[
\ell(W) = E[Y|W], \quad m(W) = E[D|W].
\]</span></p>
</section>
<section id="dml-estimation-procedure" class="slide level2">
<h2>DML Estimation Procedure</h2>
<ol type="1">
<li>Split the data into random folds: <span class="math inline">\(\{1, \ldots, n\} = \cup_{k=1}^K I_k\)</span>. Compute ML estimators <span class="math inline">\(\hat{\ell}_{k}\)</span> and <span class="math inline">\(\hat{m}_k\)</span>, leaving out the <span class="math inline">\(k\)</span>-th fold of the data. Obtain the cross-fitted residuals for each fold <span class="math inline">\(i \in I_k\)</span>:</li>
</ol>
<p><span class="math display">\[
\tilde{Y}_i = Y_i - \hat{\ell}_k(W_i), \quad \tilde{D}_i = D_i - \hat{m}_k(W_i).
\]</span></p>
<ol start="2" type="1">
<li>Apply the ordinary least squares to <span class="math inline">\(\tilde{Y}_i\)</span> on <span class="math inline">\(\tilde{D}_i\)</span> to obtain the DML estimator <span class="math inline">\(\hat{\alpha}\)</span>:</li>
</ol>
<p><span class="math display">\[
\hat{\alpha} = E_n[(\tilde{Y}_i-\alpha \tilde{D}_i) \tilde{D}_i] =0.
\]</span></p>
<ol start="3" type="1">
<li>Construct confidence intervals for <span class="math inline">\(\alpha\)</span>:</li>
</ol>
<p><span class="math display">\[
\left[\hat{\alpha} \pm 2 \times \sqrt{V/n} \right],
\]</span></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp; covers <span class="math inline">\(\alpha\)</span> in approximately 95% of repeated samples, where <span class="math inline">\(V\)</span> is the variance of the DML estimator <span class="math inline">\(\hat{\alpha}\)</span>.</p>
</section>
<section id="selecting-the-best-ml-learners-of-hatell-and-hatm" class="slide level2">
<h2>Selecting the Best ML Learners of <span class="math inline">\(\hat{\ell}\)</span> and <span class="math inline">\(\hat{m}\)</span></h2>
<ul>
<li>We can use cross-validation to select the best ML learners for estimating <span class="math inline">\(\hat{\ell}\)</span> and <span class="math inline">\(\hat{m}\)</span>.</li>
</ul>
<p><span class="math display">\[
\frac{1}{K} \sum_{k=1}^K ||\hat{\ell_k}- \ell||_{L^2}^2 \quad \text{and} \quad \frac{1}{K} \sum_{k=1}^K ||\hat{m_k}- m||_{L^2}^2
\]</span></p>
<ul>
<li>Rather than selecting a single best learner, we can also use residuals from multiple learners to form linear combinations of the residuals, which can potentially improve the estimation of <span class="math inline">\(\alpha\)</span>.</li>
</ul>
</section>
<section id="dml-inference-in-the-interactive-regression-model-irm" class="slide level2">
<h2>DML Inference in the Interactive Regression Model (IRM)</h2>
<ul>
<li><p>Here, we relax the additivity assumption in the partially linear regression model.</p></li>
<li><p>We consider estimation of average treatment effects when treatment effects are fully heterogeneous and the treatment variable is binary.</p></li>
<li><p>We consider vectors, <span class="math inline">\(W = (Y, D, X)\)</span>, where <span class="math inline">\(Y\)</span> is the outcome variable, <span class="math inline">\(D \in \{0,1\}\)</span> is a binary treatment variable or policy, and <span class="math inline">\(X\)</span> is a vector of control variables.</p></li>
</ul>
<p><span id="eq-irm"><span class="math display">\[
Y = g(D, X) + \epsilon, \quad E[\epsilon|D,X] = 0,
\tag{2}\]</span></span></p>
<p><span id="eq-irm-d"><span class="math display">\[
D = m(X) + \tilde{D}, \quad E[\tilde{D}|X] = 0,
\tag{3}\]</span></span></p>
<div style="padding: 10px; margin: 10px 0; border-left: 2px solid #1A6872; text-align: left;  color: #1A6872;">
<p>Since <span class="math inline">\(D\)</span> is not additively separable in the <a href="#/eq-irm" class="quarto-xref">Equation&nbsp;2</a>, this model is more general than the partially linear regression model.</p>
</div>
</section>
<section id="average-predictive-effect-ape" class="slide level2">
<h2>Average Predictive Effect (APE)</h2>
<ul>
<li>The average predictive effect (APE) of the binary treatment <span class="math inline">\(D\)</span> on the outcome <span class="math inline">\(Y\)</span> is defined as:</li>
</ul>
<p><span class="math display">\[
\theta_0 = E[g(1, X) - g(0, X)]
\]</span></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp; which represents the average predictive effect of switching the treatment from 0 to 1, averaging over the distribution of covariates <span class="math inline">\(X\)</span>.</p>
<ul>
<li>The confounding factors, <span class="math inline">\(X\)</span>, affect the policy variable via the propensity score <span class="math inline">\(m(X) = E[D|X]\)</span> and affect the outcome variable via the regression function <span class="math inline">\(g(D, X)\)</span>.</li>
</ul>
</section>
<section id="ape-estimation" class="slide level2">
<h2>APE Estimation</h2>
<p>ATE will be based on the relation:</p>
<p><span id="eq-irm-ate"><span class="math display">\[
\theta_{0} = \mathbb{E}[\varphi_{0}(W)],
\tag{4}\]</span></span></p>
<p>where</p>
<p><span class="math display">\[
\varphi_{0}(W) = g_{0}(1, X) - g_{0}(0, X) + \bigl(Y - g_{0}(D, X)\bigr) H_{0}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
H_{0} = \frac{\mathbf{1}(D = 1)}{m_{0}(X)} - \frac{\mathbf{1}(D = 0)}{1 - m_{0}(X)}
\]</span></p>
<p>is the Horvitz-Thompson transformation.</p>
</section>
<section id="dml-estimation-procedure-for-apesates-in-irm" class="slide level2">
<h2>DML Estimation Procedure for APEs/ATEs in IRM</h2>
<ol type="1">
<li>Split the data into random folds: <span class="math inline">\(\{1, \ldots, n\} = \cup_{k=1}^K I_k\)</span>. Compute ML estimators <span class="math inline">\(\hat{g}_{k}\)</span> and <span class="math inline">\(\hat{m}_k\)</span>, leaving out the <span class="math inline">\(k\)</span>-th fold of the data, such that <span class="math inline">\(\epsilon \le \hat{m}_k \le 1-\epsilon\)</span> for some small <span class="math inline">\(\epsilon &gt; 0\)</span>. For each fold <span class="math inline">\(i \in I_k\)</span>, compute:</li>
</ol>
<p><span class="math display">\[
\hat{\varphi}(W_i) = \hat{g}_{k}(1, X_i) - \hat{g}_{k}(0, X_i) + \bigl(Y_i - \hat{g}_{k}(D_i, X_i)\bigr) \hat{H}_i,
\]</span></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp; where <span class="math inline">\(\hat{H}_i = \frac{\mathbf{1}(D_i = 1)}{\hat{m}_k(X_i)} - \frac{\mathbf{1}(D_i = 0)}{1 - \hat{m}_k(X_i)}\)</span>.</p>
<ol start="2" type="1">
<li><p>Compute the estimator <span class="math inline">\(\theta_0 = E_n[\hat{\varphi}(W_i)]\)</span>.</p></li>
<li><p>Construct confidence intervals for <span class="math inline">\(\theta_0\)</span>:</p></li>
</ol>
<p><span class="math display">\[
\left[\hat{\theta} \pm 2 \times \sqrt{\hat{V}/n} \right],
\]</span></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp; where <span class="math inline">\(\hat{V} = E_n[(\hat{\varphi}(W_i) - \hat{\theta})^2]\)</span>.</p>
</section>
<section id="the-local-average-treatment-effect-model-late" class="slide level2">
<h2>The Local Average Treatment Effect Model (LATE)</h2>
<p>Consider a structural Equation Model (SEM) where:</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;:= f_{Y}(D, X, A, \varepsilon_{Y}) \\
D &amp;:= f_{D}(Z, X, A, \varepsilon_{D}) \in \{0,1\}, \\
Z &amp;:= f_{Z}(X, \varepsilon_{Z}) \in \{0,1\}, \\
X &amp;:= \varepsilon_{X}, \quad A := \varepsilon_{A},
\end{aligned}
\]</span></p>
<p>where all <span class="math inline">\(\epsilon\)</span> are all independent.</p>
<ul>
<li>Suppose the instrument <span class="math inline">\(Z\)</span> is is an offer to participate in a training program and that the treatment <span class="math inline">\(D\)</span> is actual endogenous participation in the training program. Participation in the program may depend on unobservables, <span class="math inline">\(A\)</span>, such as motivation, which also affects the outcome <span class="math inline">\(Y\)</span>. The variable <span class="math inline">\(X\)</span> captures observed covariates such as age, education, and work experience.</li>
</ul>
<p>The model allows us to identify the local average treatment effect (LATE), defined as:</p>
<p><span class="math display">\[
\theta = E[Y(1) - Y(0)|D(1) &gt; D(0)],
\]</span></p>
<p>where <span class="math inline">\({D(1) &gt; D(0)}\)</span> defines the compliance event, where switching the instrument, <span class="math inline">\(Z\)</span>, from 0 to 1.</p>
</section>
<section id="section-16" class="slide level2">
<h2></h2>
<p>In the LATE model, <span class="math inline">\(\theta\)</span> can be identified by the ratio of two statistical parameters,</p>
<p><span id="eq-late"><span class="math display">\[
\theta_0 = \theta_1 / \theta_2,
\tag{5}\]</span></span></p>
<p>where <span class="math display">\[
\theta_1 = E[E[Y|Z=1, X] - E[Y|Z=0, X]],
\]</span></p>
<p>and <span class="math display">\[
\theta_2 = E[E[D|Z=1, X] - E[D|Z=0, X]].
\]</span></p>
<p><a href="#/eq-late" class="quarto-xref">Equation&nbsp;5</a> is equivalent to the below expression:</p>
<p><span class="math display">\[
\theta_0 = \frac{E[E[Y|Z=1, X] - E[Y|Z=0, X]]}{E[E[D|Z=1, X] - E[D|Z=0, X]]}.
\]</span></p>
<p>This parameter is the ratio of the average predictive effect of <span class="math inline">\(Z\)</span> on <span class="math inline">\(Y\)</span> to the average predictive effect of <span class="math inline">\(Z\)</span> on <span class="math inline">\(D\)</span>.</p>
</section>
<section id="dml-estimation-for-late" class="slide level2">
<h2>DML Estimation for LATE</h2>
<p>Define regression functions:</p>
<p><span class="math display">\[
\begin{aligned}
\mu_0(Z, X) &amp;= E[Y|Z, X] \\
m_0(Z, X) &amp;= E[D|Z, X] \\
p_0(X) &amp;= E[Z|X].
\end{aligned}
\]</span></p>
<p>Therefore, nuance parameters are <span class="math inline">\(\eta = (\mu, m, p)\)</span>.</p>
<p>The DML estimator of <span class="math inline">\(\theta\)</span> is given by:</p>
<p><span class="math display">\[
\psi(W; \theta, \eta) := \mu(1, X) - \mu(0, X)
    + H(p)\bigl(Y - \mu(Z, X)\bigr)
    - \bigl(m(1, X) - m(0, X) + H(p)\bigl(D - m(Z, X)\bigr)\bigr)\theta,
\]</span></p>
<p>for <span class="math inline">\(W = (Y, D, X, Z)\)</span> and</p>
<p><span class="math display">\[
H(p) := \frac{Z}{p(X)} - \frac{1 - Z}{1 - p(X)} .
\]</span></p>
</section>
<section id="section-17" class="slide level2">
<h2></h2>
<p><br> <br> <br></p>
<div style="padding: 10px; margin: 10px 0; border-left: 2px solid #2CA0AB; text-align: left; color: #1A6872; font-weight: bold; font-size: 1.4em;">
<p>Practical Example: Application of PLM, IRM and LATE to the 401(K) data</p>
<p><em>For the relevant code and data, go to <a href="https://mustafaslancoto.github.io/talks/">here</a></em></p>
</div>
</section>
<section>
<section id="appendix" class="title-slide slide level1 center">
<h1>Appendix</h1>

</section>
<section id="why-partialling-out-works-neyman-orthogonality" class="slide level2">
<h2>Why Partialling-out works: Neyman Orthogonality</h2>
<ul>
<li><p>The key to the success of the Double Lasso procedure is the property of <span style="color: #1A6872;">Neyman orthogonality</span>, which ensures that the estimation error from the first step (Lasso) does not bias the estimation of <span class="math inline">\(\alpha\)</span> in the second step.</p></li>
<li><p>Neyman orthogonality means that the moment condition used to estimate <span class="math inline">\(\alpha\)</span> is insensitive to small errors in the estimation of the nuisance parameters <span class="math inline">\(\gamma_{YW}\)</span> and <span class="math inline">\(\gamma_{DW}\)</span>.</p></li>
</ul>
<p><span class="math display">\[
\eta^{\circ} = (\gamma_{DW}^{'}, \gamma_{YW}^{'})^{'}.
\]</span></p>
<ul>
<li>The local insensitivity of target parameters to nuisance parameters Neyman orthogonality.</li>
</ul>
<p><span class="math display">\[
\partial_{\eta} \alpha(\eta^{\circ}) = 0.
\]</span></p>
<ul>
<li>This property allows us to achieve <span class="math inline">\(\sqrt{n}\)</span>-consistency and asymptotic normality of the Double Lasso estimator <span class="math inline">\(\hat{\alpha}\)</span>, even when the nuisance parameters are estimated at slower rates due to high dimensionality.</li>
</ul>
</section>
<section id="simulation-example-neyman-orthogonality" class="slide level2">
<h2>Simulation Example: Neyman Orthogonality</h2>
<ul>
<li>We compare the performance of the naive and orthogonal methods in a computational experiment where we <span class="math inline">\(p=n=100\)</span> and we have <span class="math inline">\(\beta_j = 1/j^2, (\gamma_{DW})_j = 1/j^2\)</span></li>
</ul>
<p><span class="math display">\[
Y = 1 D + \beta^{'} W + \epsilon_Y, \quad W \sim N(0, I), \quad \epsilon_Y \sim N(0, 1)
\]</span></p>
<p><span class="math display">\[
D = \gamma_{DW}^{'} W + \tilde{D}, \quad \tilde{D} \sim N(0, 1)/4
\]</span></p>
</section>
<section id="results" class="slide level2">
<h2>Results</h2>
<p>We run 1000 simulations and compute the bias and standard deviation of the naive and orthogonal estimators. The results are as follows:</p>
</section>
<section id="section-18" class="slide level2">
<h2></h2>
<ul>
<li><p>The reason that the naive estimator does not perform well is that it only selects controls, <span class="math inline">\(W_j\)</span>, that are strongly predictive of <span class="math inline">\(Y\)</span> and omitting weak predictors of <span class="math inline">\(Y\)</span> that could be strongly predictive of <span class="math inline">\(D\)</span> leads to omitted variable bias in the estimation of <span class="math inline">\(\alpha\)</span>. This is called <span style="color: #BF505C;">ommitted variable bias</span>.</p></li>
<li><p>In contrast, the orthogonal estimator is designed to be robust to such selection mistakes, which is why it performs much better in terms of bias and standard deviation.</p></li>
</ul>
</section>
<section id="double-selection" class="slide level2">
<h2>Double Selection</h2>
<ul>
<li><p>An alternative to the Double Lasso procedure is the Double Selection method, which also uses Lasso for variable selection but in a slightly different way.</p></li>
<li><ol type="1">
<li>Find controls that predict <span class="math inline">\(Y\)</span> by running a Lasso regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(W\)</span> and selecting the non-zero coefficients.</li>
</ol></li>
<li><ol start="2" type="1">
<li>Find controls that predict <span class="math inline">\(D\)</span> by running a Lasso regression of <span class="math inline">\(D\)</span> on <span class="math inline">\(W\)</span> and selecting the non-zero coefficients.</li>
</ol></li>
<li><ol start="3" type="1">
<li>Take the union of the selected controls from both steps and run an ordinary least squares regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(D\)</span> and the union of selected controls to estimate $</li>
</ol></li>
<li><p>This procedure is approximately equivalent to the partialling out approach and also relies on the principle of Neyman orthogonality to achieve valid inference on <span class="math inline">\(\alpha\)</span>.</p></li>
</ul>
</section>
<section id="group-average-treatment-effects-gates" class="slide level2">
<h2>Group Average Treatment Effects (GATEs)</h2>
<p>We may also be interested in estimating group average treatment effects (GATEs) for a subgroup:</p>
<p><span class="math display">\[
\theta_0(x) = E[g(1, X) - g(0, X)|G = 1],
\]</span></p>
<p>where <span class="math inline">\(G\)</span> is is a group indicator defined in terms of covariates <span class="math inline">\(X\)</span>.</p>
<p>For example, we might be interested in the impact of a vaccine on teenagers, so <span class="math inline">\(G\)</span> would be an indicator for <span class="math inline">\(13 \leq \text{age} \leq 19\)</span>.</p>
<p>DML estimation of GATEs can be done by modifying the estimation procedure for APEs/ATEs to focus on the subgroup defined by <span class="math inline">\(G=1\)</span>.</p>
<p><span class="math display">\[
\hat{\theta}(x) = \mathbb{E}[\varphi_{0}(W) | G=1) =  \frac{E[\hat{\varphi}(W) G]}{\Pr(G=1)}.
\]</span></p>
</section>
<section id="causal-dags-for-the-401k-example" class="slide level2 smaller">
<h2>Causal DAGs for the 401(K) Example</h2>
<div class="quarto-layout-panel" data-layout="[[1,1], [1]]">
<div class="quarto-layout-row">
<div class="cell quarto-layout-cell" data-fig-height="4" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<svg width="960" height="384" viewbox="0.00 0.00 260.06 90.02" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none; display: block; margin: auto auto auto auto">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 86.02)">
<title>G</title>
<polygon fill="#fbfaf4" stroke="transparent" points="-4,4 -4,-86.02 256.06,-86.02 256.06,4 -4,4"></polygon>
<!-- D -->
<g id="node1" class="node">
<title>D</title>
<ellipse fill="none" stroke="#707c36" cx="162.03" cy="-64.01" rx="18.02" ry="18.02"></ellipse>
<text text-anchor="middle" x="162.03" y="-59.81" font-family="Times,serif" font-size="14.00">D</text>
</g>
<!-- Y -->
<g id="node2" class="node">
<title>Y</title>
<ellipse fill="none" stroke="#2ca0ab" cx="234.05" cy="-41.01" rx="18.02" ry="18.02"></ellipse>
<text text-anchor="middle" x="234.05" y="-36.81" font-family="Times,serif" font-size="14.00">Y</text>
</g>
<!-- D&#45;&gt;Y -->
<g id="edge1" class="edge">
<title>D-&gt;Y</title>
<path fill="none" stroke="black" d="M179.27,-58.68C187.46,-55.99 197.59,-52.66 206.85,-49.62"></path>
<polygon fill="black" stroke="black" points="208.01,-52.92 216.41,-46.48 205.82,-46.27 208.01,-52.92"></polygon>
</g>
<!-- X -->
<g id="node3" class="node">
<title>X</title>
<ellipse fill="none" stroke="black" cx="90.01" cy="-18.01" rx="18.02" ry="18.02"></ellipse>
<text text-anchor="middle" x="90.01" y="-13.81" font-family="Times,serif" font-size="14.00">X</text>
</g>
<!-- X&#45;&gt;D -->
<g id="edge3" class="edge">
<title>X-&gt;D</title>
<path fill="none" stroke="black" d="M105.52,-27.54C114.91,-33.71 127.33,-41.87 138.05,-48.91"></path>
<polygon fill="black" stroke="black" points="136.26,-51.93 146.54,-54.49 140.11,-46.08 136.26,-51.93"></polygon>
</g>
<!-- X&#45;&gt;Y -->
<g id="edge2" class="edge">
<title>X-&gt;Y</title>
<path fill="none" stroke="black" d="M107.86,-20.74C132.1,-24.66 177.07,-31.95 206,-36.63"></path>
<polygon fill="black" stroke="black" points="205.79,-40.14 216.22,-38.29 206.91,-33.23 205.79,-40.14"></polygon>
</g>
<!-- F -->
<g id="node4" class="node">
<title>F</title>
<ellipse fill="none" stroke="black" stroke-dasharray="5,2" cx="18" cy="-41.01" rx="18" ry="18"></ellipse>
<text text-anchor="middle" x="18" y="-36.81" font-family="Times,serif" font-size="14.00">F</text>
</g>
<!-- F&#45;&gt;D -->
<g id="edge4" class="edge">
<title>F-&gt;D</title>
<path fill="none" stroke="black" d="M35.85,-43.74C60.09,-47.66 105.05,-54.95 133.98,-59.63"></path>
<polygon fill="black" stroke="black" points="133.77,-63.14 144.2,-61.29 134.89,-56.23 133.77,-63.14"></polygon>
</g>
<!-- F&#45;&gt;X -->
<g id="edge5" class="edge">
<title>F-&gt;X</title>
<path fill="none" stroke="black" d="M35.24,-35.68C43.43,-32.99 53.56,-29.66 62.81,-26.62"></path>
<polygon fill="black" stroke="black" points="63.97,-29.92 72.37,-23.48 61.78,-23.27 63.97,-29.92"></polygon>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<div class="cell quarto-layout-cell" data-fig-height="4" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<svg width="960" height="384" viewbox="0.00 0.00 260.06 90.02" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none; display: block; margin: auto auto auto auto">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 86.02)">
<title>G</title>
<polygon fill="#fbfaf4" stroke="transparent" points="-4,4 -4,-86.02 256.06,-86.02 256.06,4 -4,4"></polygon>
<!-- D -->
<g id="node1" class="node">
<title>D</title>
<ellipse fill="none" stroke="#707c36" cx="162.03" cy="-64.01" rx="18.02" ry="18.02"></ellipse>
<text text-anchor="middle" x="162.03" y="-59.81" font-family="Times,serif" font-size="14.00">D</text>
</g>
<!-- Y -->
<g id="node2" class="node">
<title>Y</title>
<ellipse fill="none" stroke="#2ca0ab" cx="234.05" cy="-41.01" rx="18.02" ry="18.02"></ellipse>
<text text-anchor="middle" x="234.05" y="-36.81" font-family="Times,serif" font-size="14.00">Y</text>
</g>
<!-- D&#45;&gt;Y -->
<g id="edge1" class="edge">
<title>D-&gt;Y</title>
<path fill="none" stroke="black" d="M179.27,-58.68C187.46,-55.99 197.59,-52.66 206.85,-49.62"></path>
<polygon fill="black" stroke="black" points="208.01,-52.92 216.41,-46.48 205.82,-46.27 208.01,-52.92"></polygon>
</g>
<!-- X -->
<g id="node3" class="node">
<title>X</title>
<ellipse fill="none" stroke="black" cx="90.01" cy="-18.01" rx="18.02" ry="18.02"></ellipse>
<text text-anchor="middle" x="90.01" y="-13.81" font-family="Times,serif" font-size="14.00">X</text>
</g>
<!-- X&#45;&gt;D -->
<g id="edge3" class="edge">
<title>X-&gt;D</title>
<path fill="none" stroke="black" d="M105.52,-27.54C114.91,-33.71 127.33,-41.87 138.05,-48.91"></path>
<polygon fill="black" stroke="black" points="136.26,-51.93 146.54,-54.49 140.11,-46.08 136.26,-51.93"></polygon>
</g>
<!-- X&#45;&gt;Y -->
<g id="edge2" class="edge">
<title>X-&gt;Y</title>
<path fill="none" stroke="black" d="M107.86,-20.74C132.1,-24.66 177.07,-31.95 206,-36.63"></path>
<polygon fill="black" stroke="black" points="205.79,-40.14 216.22,-38.29 206.91,-33.23 205.79,-40.14"></polygon>
</g>
<!-- F -->
<g id="node4" class="node">
<title>F</title>
<ellipse fill="none" stroke="black" stroke-dasharray="5,2" cx="18" cy="-41.01" rx="18" ry="18"></ellipse>
<text text-anchor="middle" x="18" y="-36.81" font-family="Times,serif" font-size="14.00">F</text>
</g>
<!-- F&#45;&gt;D -->
<g id="edge4" class="edge">
<title>F-&gt;D</title>
<path fill="none" stroke="black" d="M35.85,-43.74C60.09,-47.66 105.05,-54.95 133.98,-59.63"></path>
<polygon fill="black" stroke="black" points="133.77,-63.14 144.2,-61.29 134.89,-56.23 133.77,-63.14"></polygon>
</g>
<!-- F&#45;&gt;X -->
<g id="edge5" class="edge">
<title>F-&gt;X</title>
<path fill="none" stroke="black" d="M44.75,-32.55C53.98,-29.52 64.13,-26.19 72.37,-23.48"></path>
<polygon fill="black" stroke="black" points="43.65,-29.23 35.24,-35.68 45.83,-35.88 43.65,-29.23"></polygon>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="cell quarto-layout-cell" data-fig-height="4" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<svg width="960" height="384" viewbox="0.00 0.00 260.08 98.01" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none; display: block; margin: auto auto auto auto">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 94.01)">
<title>G</title>
<polygon fill="#fbfaf4" stroke="transparent" points="-4,4 -4,-94.01 256.08,-94.01 256.08,4 -4,4"></polygon>
<!-- D -->
<g id="node1" class="node">
<title>D</title>
<ellipse fill="none" stroke="#707c36" cx="162.05" cy="-64.01" rx="18.02" ry="18.02"></ellipse>
<text text-anchor="middle" x="162.05" y="-59.81" font-family="Times,serif" font-size="14.00">D</text>
</g>
<!-- Y -->
<g id="node2" class="node">
<title>Y</title>
<ellipse fill="none" stroke="#2ca0ab" cx="234.07" cy="-41.01" rx="18.02" ry="18.02"></ellipse>
<text text-anchor="middle" x="234.07" y="-36.81" font-family="Times,serif" font-size="14.00">Y</text>
</g>
<!-- D&#45;&gt;Y -->
<g id="edge1" class="edge">
<title>D-&gt;Y</title>
<path fill="none" stroke="black" d="M179.29,-58.68C187.48,-55.99 197.61,-52.66 206.87,-49.62"></path>
<polygon fill="black" stroke="black" points="208.03,-52.92 216.43,-46.48 205.84,-46.27 208.03,-52.92"></polygon>
</g>
<!-- X -->
<g id="node3" class="node">
<title>X</title>
<ellipse fill="none" stroke="black" cx="90.03" cy="-18.01" rx="18.02" ry="18.02"></ellipse>
<text text-anchor="middle" x="90.03" y="-13.81" font-family="Times,serif" font-size="14.00">X</text>
</g>
<!-- X&#45;&gt;D -->
<g id="edge3" class="edge">
<title>X-&gt;D</title>
<path fill="none" stroke="black" d="M105.54,-27.54C114.93,-33.71 127.35,-41.87 138.07,-48.91"></path>
<polygon fill="black" stroke="black" points="136.28,-51.93 146.56,-54.49 140.13,-46.08 136.28,-51.93"></polygon>
</g>
<!-- X&#45;&gt;Y -->
<g id="edge2" class="edge">
<title>X-&gt;Y</title>
<path fill="none" stroke="black" d="M107.88,-20.74C132.12,-24.66 177.09,-31.95 206.02,-36.63"></path>
<polygon fill="black" stroke="black" points="205.81,-40.14 216.24,-38.29 206.93,-33.23 205.81,-40.14"></polygon>
</g>
<!-- F -->
<g id="node4" class="node">
<title>F</title>
<ellipse fill="none" stroke="black" stroke-dasharray="5,2" cx="90.03" cy="-72.01" rx="18" ry="18"></ellipse>
<text text-anchor="middle" x="90.03" y="-67.81" font-family="Times,serif" font-size="14.00">F</text>
</g>
<!-- F&#45;&gt;D -->
<g id="edge4" class="edge">
<title>F-&gt;D</title>
<path fill="none" stroke="black" d="M107.98,-70.07C115.69,-69.19 125.01,-68.13 133.68,-67.14"></path>
<polygon fill="black" stroke="black" points="134.23,-70.6 143.77,-65.98 133.44,-63.64 134.23,-70.6"></polygon>
</g>
<!-- U -->
<g id="node5" class="node">
<title>U</title>
<ellipse fill="none" stroke="black" stroke-dasharray="5,2" cx="18.01" cy="-45.01" rx="18.02" ry="18.02"></ellipse>
<text text-anchor="middle" x="18.01" y="-40.81" font-family="Times,serif" font-size="14.00">U</text>
</g>
<!-- U&#45;&gt;X -->
<g id="edge6" class="edge">
<title>U-&gt;X</title>
<path fill="none" stroke="black" d="M35.25,-38.75C43.65,-35.51 54.09,-31.49 63.53,-27.84"></path>
<polygon fill="black" stroke="black" points="64.81,-31.1 72.88,-24.24 62.29,-24.57 64.81,-31.1"></polygon>
</g>
<!-- U&#45;&gt;F -->
<g id="edge5" class="edge">
<title>U-&gt;F</title>
<path fill="none" stroke="black" d="M35.25,-51.27C43.65,-54.51 54.09,-58.54 63.53,-62.18"></path>
<polygon fill="black" stroke="black" points="62.29,-65.45 72.88,-65.78 64.81,-58.92 62.29,-65.45"></polygon>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
</div>
<div style="margin-top: 10px; font-size: 0.8em; color">
<p>Three Causal DAGs for analysis of the 401(K) example in which adjusting for covariates <span class="math inline">\(X\)</span> is sufficient to control for confounding between <span class="math inline">\(D\)</span> and <span class="math inline">\(Y\)</span>.</p>
</div>


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p><a href="https://mustafaslancoto.github.io/talks/">Causal Inference with Machine Learning</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,

        height: 1080,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>